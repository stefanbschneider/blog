{
  "cells": [
    {
      "cell_type": "raw",
      "id": "3c4a832b",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "draft: true\n",
        "categories:\n",
        "- transformer\n",
        "- llm\n",
        "- machine learning\n",
        "- HuggingFace\n",
        "date: '2025-01-11'\n",
        "date-modified: '2025-01-11'\n",
        "title: Generative Document Question Answering with HuggingFace\n",
        "description: Generating answers about a given document/article using pre-trained models on HuggingFace.\n",
        "image: images/hf-logo-with-title.png\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294d36c7",
      "metadata": {},
      "source": [
        "# Extractive Question Answering\n",
        "\n",
        "In a [previous blog post](https://stefanbschneider.github.io/blog/posts/question-answering-huggingface/), I showed how answer document-related questions with [HuggingFace](https://huggingface.co/) LLMs in just a few lines of Python code and visualize them as simple [Gradio App](https://www.gradio.app/).\n",
        "\n",
        "In that blog post, I used the standard question-answering pipeline from HuggingFace.\n",
        "This pipeline defaults to a DistilBERT model fine-tuned on the Stanford Question Answering Dataset (SQuAD).\n",
        "[This model](https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad) does *extractive* question answering as illustrated in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c066d39f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "pip install -U pypdf torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "85f89229",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.4559027850627899,\n",
              " 'start': 287,\n",
              " 'end': 302,\n",
              " 'answer': 'the Transformer'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "extractive_qa = pipeline(task=\"question-answering\")\n",
        "\n",
        "# Abstract from \"Attention is all you need\" by Vaswani et al.: https://arxiv.org/abs/1706.03762\n",
        "abstract = \"\"\"The dominant sequence transduction models are based on complex recurrent or\n",
        "convolutional neural networks that include an encoder and a decoder. The best\n",
        "performing models also connect the encoder and decoder through an attention\n",
        "mechanism. We propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
        "entirely. Experiments on two machine translation tasks show these models to\n",
        "be superior in quality while being more parallelizable and requiring significantly\n",
        "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...\n",
        "\"\"\"\n",
        "\n",
        "extractive_qa(\"What's a transformer'?\", abstract)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "120449bb",
      "metadata": {},
      "source": [
        "The pipeline is given a text as input, here parts of the \"Attention is all you need\" abstract (see [arxiv](https://arxiv.org/abs/1706.03762)),\n",
        "and a question that should be answered based on the given text/context.\n",
        "\n",
        "Rather than an answer in natural language, the model outputs an excerpt that is extraced from the original context, given by a start- and end-index within.\n",
        "While this allows concise answers with clear reference to the original source, the answers are not very natural or accurate.\n",
        "The model has no way of combining and merging information from different places of the original text.\n",
        "\n",
        "In the example above, I asked what a transformer is and the model simply answered \"the Transformer\". Not very helpful!\n",
        "\n",
        "\n",
        "# Extractive vs. Generative/Abstractive Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a4089c9",
      "metadata": {},
      "source": [
        "# Question Answering with HuggingFace\n",
        "\n",
        "We can read the text of PDF document with `pypdf`. As an example, I'm using the author version of a [paper](https://ieeexplore.ieee.org/document/9789886) I wrote on [`mobile-env`](https://github.com/stefanbschneider/mobile-env). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8662ed07",
      "metadata": {},
      "source": [
        "Now we can create a question answering pipeline using HuggingFace, loading a pre-trained model. Then we can ask some questions, providing the PDF text as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2325a098",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(task=\"question-answering\", model=\"deepset/tinyroberta-squad2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "871b3b95",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.9887111186981201,\n",
              " 'start': 16488,\n",
              " 'end': 16505,\n",
              " 'answer': 'GitHub repository'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_answerer(\"What is mobile-env?\", pdf_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "89f9bbc6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.9665615558624268, 'start': 3552, 'end': 3558, 'answer': 'Python'}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_answerer(\"What programming language is mobile-env written in?\", pdf_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ced00482",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.6506955027580261,\n",
              " 'start': 12539,\n",
              " 'end': 12570,\n",
              " 'answer': 'more ï¬‚exible, better documented'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_answerer(\"What is the main difference between mobile-env and other simulators?\", pdf_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0df359f1",
      "metadata": {},
      "source": [
        "The pipeline returns a dict, where the answer is a quote from the given context, here the PDF document. This is called *extractive* question answering.\n",
        "\n",
        "It also provides a score indicating the model's confindence in the answer and the start/end index from where the answer is quoted. \n",
        "\n",
        "That's it! Let's see how we can build a simple app on top of this.\n",
        "\n",
        "# Building an App with Gradio\n",
        "\n",
        "[Gradio](https://www.gradio.app/) allows building simple apps tailored for machine learning use cases.\n",
        "You can define the inputs, a function to where to pass these inputs, and how to display the functions outputs.\n",
        "\n",
        "Here, our inputs are the PDF document and the question.\n",
        "The function loads the document and passes the question and text to the pre-trained model.\n",
        "It then outputs the models answer to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f5a8daee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7862\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def answer_doc_question(pdf_file, question):\n",
        "    pdf_text = get_text_from_pdf(pdf_file)\n",
        "    answer = question_answerer(question, pdf_text)\n",
        "    return answer[\"answer\"]\n",
        "\n",
        "# Add default a file and question, so it's easy to try out the app.\n",
        "pdf_input = gr.File(\n",
        "    value=\"https://ris.uni-paderborn.de/download/30236/30237/author_version.pdf\",\n",
        "    file_types=[\".pdf\"],\n",
        "    label=\"Upload a PDF document and ask a question about it.\",\n",
        ")\n",
        "question = gr.Textbox(\n",
        "    value=\"What is mobile-env?\",\n",
        "    label=\"Type a question regarding the uploaded document here.\",\n",
        ")\n",
        "gr.Interface(\n",
        "    fn=answer_doc_question, inputs=[pdf_input, question], outputs=\"text\"\n",
        ").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a903f86",
      "metadata": {},
      "source": [
        "If you run this locally, you should see a rendered app based on the question answering pipeline we built above!\n",
        "\n",
        "# Deploying the app in HuggingFace Spaces\n",
        "\n",
        "You can easily host the app on [HuggingFace Spaces](https://huggingface.co/spaces), which provide free (and slow) hosting (or fast paid hosting).\n",
        "\n",
        "You simply create a new space under your account and add an `app.py`, which contains all code above. The requirements go into a `requirements.txt`. That's it!\n",
        "\n",
        "This is the app we built here: [https://huggingface.co/spaces/stefanbschneider/pdf-question-answering](https://huggingface.co/spaces/stefanbschneider/pdf-question-answering)\n",
        "\n",
        "<script\n",
        "\ttype=\"module\"\n",
        "\tsrc=\"https://gradio.s3-us-west-2.amazonaws.com/4.17.0/gradio.js\"\n",
        "></script>\n",
        "\n",
        "<gradio-app src=\"https://stefanbschneider-pdf-question-answering.hf.space\"></gradio-app>\n",
        "\n",
        "\n",
        "# What's Next?\n",
        "\n",
        "* [Read about the underlying transformer architecture powering most LLMs](https://stefanbschneider.github.io/blog/posts/understanding-transformers-attention/)\n",
        "* Improve the quality of the question answering app. Some ideas:\n",
        "    * Fine-tune the pre-trained model on a domain dataset, eg, [Arxiv Q&A](https://huggingface.co/datasets/taesiri/arxiv_qa)\n",
        "    * [Domain adaptation by fine-tuning a masked model directly on the document](https://huggingface.co/learn/nlp-course/en/chapter7/3)\n",
        "    * Using the [document-question-answering pipeline on HuggingFace](https://huggingface.co/tasks/document-question-answering)\n",
        "    * Trying a model that supports *generative* question answering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f513d0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
