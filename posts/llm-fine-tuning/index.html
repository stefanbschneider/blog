<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stefan Schneider">
<meta name="dcterms.date" content="2025-02-09">
<meta name="description" content="Fine-tuning a pre-trained language model on a custom dataset for long-form question answering using HuggingFace.">

<title>Stefan’s Blog - Fine-Tuning a Pre-Trained LLM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-RDWKTYTDBL"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-RDWKTYTDBL', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  });
});
</script> 
  
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Stefan’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/stefanbschneider/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Fine-Tuning a Pre-Trained LLM</h1>
                  <div>
        <div class="description">
          Fine-tuning a pre-trained language model on a custom dataset for long-form question answering using HuggingFace.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">transformer</div>
                <div class="quarto-category">llm</div>
                <div class="quarto-category">machine learning</div>
                <div class="quarto-category">HuggingFace</div>
                <div class="quarto-category">Vast.ai</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Stefan Schneider </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 9, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">February 9, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#longformer-encoder-decoder-led-base-model" id="toc-longformer-encoder-decoder-led-base-model" class="nav-link active" data-scroll-target="#longformer-encoder-decoder-led-base-model">Longformer Encoder-Decoder (LED) Base Model</a></li>
  <li><a href="#task-specific-dataset-long-form-question-answering" id="toc-task-specific-dataset-long-form-question-answering" class="nav-link" data-scroll-target="#task-specific-dataset-long-form-question-answering">Task-Specific Dataset (Long-Form Question Answering)</a>
  <ul class="collapse">
  <li><a href="#finding-a-suitable-dataset" id="toc-finding-a-suitable-dataset" class="nav-link" data-scroll-target="#finding-a-suitable-dataset">Finding a Suitable Dataset</a></li>
  <li><a href="#analyzing-and-adjusting-the-dataset" id="toc-analyzing-and-adjusting-the-dataset" class="nav-link" data-scroll-target="#analyzing-and-adjusting-the-dataset">Analyzing and Adjusting the Dataset</a></li>
  </ul></li>
  <li><a href="#fine-tuning" id="toc-fine-tuning" class="nav-link" data-scroll-target="#fine-tuning">Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#preparing-the-data" id="toc-preparing-the-data" class="nav-link" data-scroll-target="#preparing-the-data">Preparing the Data</a></li>
  <li><a href="#configuring-the-model-and-training" id="toc-configuring-the-model-and-training" class="nav-link" data-scroll-target="#configuring-the-model-and-training">Configuring the Model and Training</a></li>
  <li><a href="#training-and-monitoring" id="toc-training-and-monitoring" class="nav-link" data-scroll-target="#training-and-monitoring">Training and Monitoring</a></li>
  <li><a href="#full-training-run-on-vast.ai" id="toc-full-training-run-on-vast.ai" class="nav-link" data-scroll-target="#full-training-run-on-vast.ai">Full Training Run on Vast.ai*</a></li>
  </ul></li>
  <li><a href="#testing-the-fine-tuned-model" id="toc-testing-the-fine-tuned-model" class="nav-link" data-scroll-target="#testing-the-fine-tuned-model">Testing the Fine-Tuned Model</a></li>
  <li><a href="#whats-next" id="toc-whats-next" class="nav-link" data-scroll-target="#whats-next">What’s Next?</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/stefanbschneider/blog/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Large language models (LLMs) can perform all kinds of tasks ranging from translation over summarization to text generation or even multi-modal tasks involving sound, images, or videos. Usually, there are LLMs readily available for any kind of task, which can be easily found on <a href="https://huggingface.co/models">HuggingFace</a>. However, if there is no available model doing just what you want, then fine-tuning is the way to go. During fine-tuning, a pre-trained base or foundation model is further trained on a comparably small, task-specific dataset. Fine-tuning is much faster and cheaper than pre-training a new model from scratch.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/fine-tuning.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Illustration of the fine-tuning process.</figcaption>
</figure>
</div>
<p>In my case, I was looking for a model to answer questions about long documents in natural language. Most models I could find were limited to short context lengths, i.e., could not handle entire documents as input, or were not trained to output generated natural answers (see my post <a href="https://stefanbschneider.github.io/blog/posts/generative-qa/">here</a>).</p>
<p>Hence, in this blog post, I fine-tune a pre-trained Longformer Encoder-Decoder (LED) base model for generative question answering.</p>
<section id="longformer-encoder-decoder-led-base-model" class="level1">
<h1>Longformer Encoder-Decoder (LED) Base Model</h1>
<p>As base model, I use the Longformer Encoder-Decoder (LED) from AllenAI: <a href="https://huggingface.co/allenai/led-base-16384">allenai/led-base-16384</a> This base model supports very long contexts as input but, as I understand, is not yet trained for any specific downstream tasks. Note, there is a larger version of the LED base model with even more trainable weights/parameters: <a href="https://huggingface.co/allenai/led-large-16384">allenai/led-large-16384</a> Fine-tuning this larger model could lead to better results but also needs more resources and time to train.</p>
<p>There is a fine-tuned LED model for <a href="https://huggingface.co/allenai/led-large-16384-arxiv">text summarization</a>, but I did not see any for question answering. Taking the (smaller) base model directly for answering questions does not work at all:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture <span class="op">--</span>no<span class="op">-</span>display</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">-</span>U datasets evaluate transformers accelerate rouge_score wandb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>qa_pipeline <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"text2text-generation"</span>, model<span class="op">=</span><span class="st">"allenai/led-base-16384"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Abstract from "Attention is all you need" by Vaswani et al.: https://arxiv.org/abs/1706.03762</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>abstract <span class="op">=</span> <span class="st">"""The dominant sequence transduction models are based on complex recurrent or</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="st">convolutional neural networks that include an encoder and a decoder. The best</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="st">performing models also connect the encoder and decoder through an attention</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="st">mechanism. We propose a new simple network architecture, the Transformer,</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="st">based solely on attention mechanisms, dispensing with recurrence and convolutions</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="st">entirely. Experiments on two machine translation tasks show these models to</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="st">be superior in quality while being more parallelizable and requiring significantly</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="st">less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What's a transformer'?"</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="ss">f"question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss"> context: </span><span class="sc">{</span>abstract<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>qa_pipeline(input_text, max_length<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>][<span class="st">'generated_text'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use mps:0
Input ids are automatically padded from 144 to 1024 to be a multiple of `config.attention_window`: 1024</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>"question: What's a transformer? In this context: The dominant sequence transduction models are based on complex recurrent or non-convolutional neural networks that include an encoder and a decoder. The best-performing models also connect the encoder and decoder through an attention-mechanism. We propose a new simple network architecture, the Transformer, that is based on a network architecturebased solely on attention mechanisms, dispensing with recurrence and convolutions, and integrating them"</code></pre>
</div>
</div>
<p>The model’s “answer” is basically just a repetition of the provided context, including the question.</p>
<p>Let’s see if fine-tuning can improve the answers. But first, we need a suitable dataset.</p>
</section>
<section id="task-specific-dataset-long-form-question-answering" class="level1">
<h1>Task-Specific Dataset (Long-Form Question Answering)</h1>
<section id="finding-a-suitable-dataset" class="level2">
<h2 class="anchored" data-anchor-id="finding-a-suitable-dataset">Finding a Suitable Dataset</h2>
<p>My task of interest is answering questions in natural language given a (potentially long) context. Since I do not have the means of collecting and creating my own dataset, I was looking for a suitable dataset online.</p>
<p>The well-known <a href="https://huggingface.co/datasets/rajpurkar/squad_v2">SQuAD dataset</a> is only suitable for extractive question answering, where the answer is a span text from the provided context. The <a href="https://huggingface.co/datasets/ibm-research/duorc">DuoRC dataset</a> with questions and answers about a given movie plot can be used for both extractive and generative/abstracitve Q&amp;A. However, I found the answers to be overly short, often just a few words, and not always very natural.</p>
<p>Finally, I found a <a href="https://huggingface.co/datasets/LLukas22/lfqa_preprocessed">suitable dataset for long-form question answering (LFQA)</a> with natural, multi-sentence answers to questions based on provided contexts (details on this dataset). The dataset is a successor of <a href="https://facebookresearch.github.io/ELI5/index.html">facebook’s ELI5 dataset</a> (explain like I’m five), which is <a href="https://huggingface.co/datasets/defunct-datasets/eli5">no longer available</a>. Details are in <a href="https://towardsdatascience.com/long-form-qa-beyond-eli5-an-updated-dataset-and-approach-319cb841aabb/">this blog post by the dataset’s authors</a>.</p>
</section>
<section id="analyzing-and-adjusting-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="analyzing-and-adjusting-the-dataset">Analyzing and Adjusting the Dataset</h2>
<p>To get familiar with the dataset and understand what kind of inputs and outputs the fine-tuned model has to handle, I visualized the length of contexts (model input, together with the questions) as well as the length of the expected answers (model output) in the dataset. Since I am interested in the length in terms of number of tokens (relevant for fine-tuning later), I first tokenized the contexts and answers with the LED base model’s tokenizer.</p>
<p>The context lengths are quite normally distributed with a few quite long contexts, but most rather short:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/context-lengths-tokens-original.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Distribution of context lengths (in number of tokens).</figcaption>
</figure>
</div>
<p>Since I am interested in long contexts, it is fine to have a few contexts that are much longer than the others.</p>
<p>The answer lengths have an even stronger long-tail distribution with some few answers that were overly long (up to ~6000 tokens, even longer than the context!).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/answer-lengths-tokens-original.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Distribution of answer lengths in the original dataset (in number of tokens).</figcaption>
</figure>
</div>
<p>Since I did not want my fine-tuned model to create overly long answers, I filtered these examples out of the dataset and made my own version of the dataset with answers only up 512 tokens. This means the maximum answer length is roughly 12x shorter at the cost of 10% less training data.</p>
<p>My filtered dataset is available here: <a href="https://huggingface.co/datasets/stefanbschneider/lfqa-max-answer-length-512">stefanbschneider/lfqa-max-answer-length-512</a> The notebook I used for creating the filtered dataset as well as the plots is also in the repository: <a href="https://huggingface.co/datasets/stefanbschneider/lfqa-max-answer-length-512/blob/main/process-lfqa-dataset.ipynb"><code>process-lfqa-dataset.ipynb</code></a></p>
<p>An example in the dataset looks like this:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"question"</span><span class="fu">:</span> <span class="st">"what's the difference between a forest and a wood?"</span><span class="fu">,</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"answer"</span><span class="fu">:</span> <span class="st">"They're used interchangeably a lot. You'll get different answers from different resources, but the ..."</span><span class="fu">,</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"context"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Wood is divided, according to its botanical origin, into two kinds: softwoods, ..."</span><span class="ot">,</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Processing and products differs especially with regard to the distinction between softwood and hardwood ..."</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="ot">]</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="fine-tuning" class="level1">
<h1>Fine-Tuning</h1>
<p>Now that the dataset is ready, the data has to be prepared and the model has to be loaded and configured for fine-tuning.</p>
<p>Let’s start by importing the necessary libraries and loading the LED base model and tokenizer.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    Seq2SeqTrainer,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    Seq2SeqTrainingArguments,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    AutoTokenizer,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    AutoModelForSeq2SeqLM,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    GenerationConfig,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># load model and tokenizer</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"allenai/led-base-16384"</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model and enable gradient checkpointing to reduce memory during training (at the cost of speed)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSeq2SeqLM.from_pretrained(model_name)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="preparing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="preparing-the-data">Preparing the Data</h2>
<p>Next, I create two functions for processing the data for training and validation. These functions prepare the data in batches of size 2 (larger batches do not fit onto my GPU). I found that batch size 1 did not work at all; the loss quickly dropped to zero and the model stopped learning.</p>
<p>Since each question is paired with a list of multiple contexts, these contexts are concatenated to the corresponding question into one single string, which is given as input to the model. Note that the expected output length is also set to 512 tokens here.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_data_to_model_inputs(batch):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># combine context strings and questions to one input</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> [</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">, context: </span><span class="sc">{</span><span class="st">' '</span><span class="sc">.</span>join(context)<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> question, context <span class="kw">in</span> <span class="bu">zip</span>(batch[<span class="st">"question"</span>], batch[<span class="st">"context"</span>])</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># tokenize the inputs and labels</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span>,</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Max supported article/context length + question.</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="dv">8192</span>,</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> tokenizer(</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        batch[<span class="st">"answer"</span>],</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Since I limit the answers to 512 tokens in the dataset, I can also limit the max_length here</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The following settings are copied from the fine-tuning notebook provided by AllenAI:</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">"input_ids"</span>] <span class="op">=</span> inputs.input_ids</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">"attention_mask"</span>] <span class="op">=</span> inputs.attention_mask</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create 0 global_attention_mask lists</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">"global_attention_mask"</span>] <span class="op">=</span> <span class="bu">len</span>(batch[<span class="st">"input_ids"</span>]) <span class="op">*</span> [</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        [<span class="dv">0</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(batch[<span class="st">"input_ids"</span>][<span class="dv">0</span>]))]</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># since above lists are references, the following line changes the 0 index for all samples</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">"global_attention_mask"</span>][<span class="dv">0</span>][<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">"labels"</span>] <span class="op">=</span> outputs.input_ids</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We have to make sure that the PAD token is ignored</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">"labels"</span>] <span class="op">=</span> [</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        [<span class="op">-</span><span class="dv">100</span> <span class="cf">if</span> token <span class="op">==</span> tokenizer.pad_token_id <span class="cf">else</span> token <span class="cf">for</span> token <span class="kw">in</span> labels]</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> labels <span class="kw">in</span> batch[<span class="st">"labels"</span>]</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batch</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_and_process_dataset(split: <span class="bu">str</span>, dataset_limit: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Load and process the dataset for training or validation. Optionally limit the number of samples."""</span></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> load_dataset(<span class="st">"stefanbschneider/lfqa-max-answer-length-512"</span>, split<span class="op">=</span>split)</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># optionally reduce the data sets to a small fraction</span></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> dataset_limit <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>        dataset <span class="op">=</span> dataset.select(<span class="bu">range</span>(dataset_limit))</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process the dataset with the function above. Afterwards, remove the original columns.</span></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.<span class="bu">map</span>(</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        process_data_to_model_inputs,</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>        batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>        remove_columns<span class="op">=</span>[<span class="st">"context"</span>, <span class="st">"question"</span>, <span class="st">"answer"</span>],</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Format the dataset to torch</span></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>    dataset.set_format(</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>        <span class="bu">type</span><span class="op">=</span><span class="st">"torch"</span>,</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>        columns<span class="op">=</span>[<span class="st">"input_ids"</span>, <span class="st">"attention_mask"</span>, <span class="st">"global_attention_mask"</span>, <span class="st">"labels"</span>],</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For development and experimentation, it is useful to only load a small fraction of the dataset, using the <code>dataset_limit</code> argument I introduced above:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and process datasets; limit to small size for experimentation</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> load_and_process_dataset(<span class="st">"train"</span>, dataset_limit<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>val_data <span class="op">=</span> load_and_process_dataset(<span class="st">"validation"</span>, dataset_limit<span class="op">=</span><span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"df511791d7854e3994ac7fbf8f18d6c1","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
<section id="configuring-the-model-and-training" class="level2">
<h2 class="anchored" data-anchor-id="configuring-the-model-and-training">Configuring the Model and Training</h2>
<p>With the data loaded and ready for training, both the model and the training have to be configured. The generation config defines how the model generates new answers. Here, I set answers to be 100 to 512 tokens long.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and set the model's generation config</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>generation_config <span class="op">=</span> GenerationConfig(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The generated answer should be 100-512 tokens long</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    min_length<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    early_stopping<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    num_beams<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    length_penalty<span class="op">=</span><span class="fl">2.0</span>,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Don't repeat n=3-grams (same words in same order) in the generated text --&gt; more natural</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    no_repeat_ngram_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    decoder_start_token_id<span class="op">=</span>tokenizer.cls_token_id,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    bos_token_id<span class="op">=</span>tokenizer.bos_token_id,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>model.generation_config <span class="op">=</span> generation_config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The training arguments control the training procedure as well as how (often) the model is saved, evaluated, and how training is monitored.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set training arguments</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> Seq2SeqTrainingArguments(</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    predict_with_generate<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fp16 only works on GPU, not on M1 mps. mps is used by default if it's available</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#fp16=True,</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"models"</span>,</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,    <span class="co"># for proper training: 100,</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    eval_steps<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#save_steps=500,</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save to HF hub &amp; log to wandb</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#push_to_hub=True,</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">#hub_model_id="stefanbschneider/led-base-16384-lfqa-ans-len-512",</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    log_level<span class="op">=</span><span class="st">"info"</span>,</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"wandb"</span>,</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    run_name<span class="op">=</span><span class="st">"test-run"</span>,</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I use batch size 2 (set above) and save the model locally to “models”.</p>
<p>For logging, I use <a href="https://wandb.ai/home">Weights &amp; Biases</a>, which is also the default. For that to work, you need a free W&amp;B account. <code>logging_steps</code> determines the frequency (in number of optimization steps) in which the training results are logged locally and to W&amp;B. Similarly, <code>eval_steps</code> controls how often the evaluation is run on the <code>val_data</code> prepared above. During training, a model checkpoint is saved locally every <code>save_steps</code>, maintaining at most <code>save_total_limit</code> checkpoints locally (then overwriting old checkpoints). I do not use <a href="https://huggingface.co/docs/transformers/en/perf_train_gpu_one#gradient-accumulation">gradient accumulation</a>, which can mimic higher batch sizes even with less memory. Finally, <code>num_train_epochs</code> sets the number of training epochs, i.e., how often training is repeated on the training set.</p>
<p>These paramters together with the size of the training set (above limited to 128 for debugging) determine the overall number of optimization steps during training:</p>
<p><span class="math display">\[\text{num\_steps} = \frac{\text{num\_examples\_in\_data}}{\text{batch\_size} * \text{gradient\_accumulation}} * \text{epochs}\]</span></p>
<p>If <code>push_to_hub=True</code>, new checkpoints are automatically pushed to the HuggingFace hub. This is definitely recommended when running the full fine-tuning, so that the model is safely stored online and can easily be used later on. This needs a HuggingFace account. Once the account is created, log in locally with the CLI: <code>huggingface-cli login</code> and paste an <a href="https://huggingface.co/settings/tokens">access token</a> to connect with your account.</p>
</section>
<section id="training-and-monitoring" class="level2">
<h2 class="anchored" data-anchor-id="training-and-monitoring">Training and Monitoring</h2>
<p>To evaluate the model on the validation set, we need to define a validation function that computes some metric of interest. It seems like long-form question answering is generally hard to evaluate automatically (see <a href="https://arxiv.org/abs/2305.18201">paper</a>), so I just resort to the <a href="https://huggingface.co/spaces/evaluate-metric/rouge">ROUGE score</a>, which is often used in summarization. It measures the similarity between the predicted answer by the model and the expected answer from the dataset.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> evaluate</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>rouge <span class="op">=</span> evaluate.load(<span class="st">"rouge"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(pred) <span class="op">-&gt;</span> <span class="bu">dict</span>[<span class="bu">str</span>, <span class="bu">float</span>]:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute rouge score during validation/evaluation"""</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    labels_ids <span class="op">=</span> pred.label_ids</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    pred_ids <span class="op">=</span> pred.predictions</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    pred_str <span class="op">=</span> tokenizer.batch_decode(pred_ids, skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    labels_ids[labels_ids <span class="op">==</span> <span class="op">-</span><span class="dv">100</span>] <span class="op">=</span> tokenizer.pad_token_id</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    label_str <span class="op">=</span> tokenizer.batch_decode(labels_ids, skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    rouge_output <span class="op">=</span> rouge.compute(</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        predictions<span class="op">=</span>pred_str, references<span class="op">=</span>label_str, rouge_types<span class="op">=</span>[<span class="st">"rouge2"</span>]</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    )[<span class="st">"rouge2"</span>]</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return rouge2 F1 score</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"rouge2"</span>: <span class="bu">round</span>(rouge_output, <span class="dv">4</span>)}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, putting all this together, we can start training:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Seq2SeqTrainer(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    processing_class<span class="op">=</span>tokenizer,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_data,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>val_data,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Optionally, push the final trained model to the HuggingFace hub</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co"># trainer.push_to_hub()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
***** Running training *****
  Num examples = 128
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed &amp; accumulation) = 2
  Gradient Accumulation steps = 1
  Total optimization steps = 64
  Number of trainable parameters = 161,844,480
Automatic Weights &amp; Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: stefanbschneider to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Saving model checkpoint to models/checkpoint-64
Configuration saved in models/checkpoint-64/config.json
Configuration saved in models/checkpoint-64/generation_config.json
Model weights saved in models/checkpoint-64/model.safetensors
tokenizer config file saved in models/checkpoint-64/tokenizer_config.json
Special tokens file saved in models/checkpoint-64/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)

</code></pre>
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.19.5
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/Users/stefanshschneider/Projects/private/blog/posts/llm-fine-tuning/wandb/run-20250209_132357-haehdkmt</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/stefanbschneider/huggingface/runs/haehdkmt" target="_blank">test-run</a></strong> to <a href="https://wandb.ai/stefanbschneider/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/developer-guide" target="_blank">docs</a>)<br>
</div>
<div class="cell-output cell-output-display">
 View project at <a href="https://wandb.ai/stefanbschneider/huggingface" target="_blank">https://wandb.ai/stefanbschneider/huggingface</a>
</div>
<div class="cell-output cell-output-display">
 View run at <a href="https://wandb.ai/stefanbschneider/huggingface/runs/haehdkmt" target="_blank">https://wandb.ai/stefanbschneider/huggingface/runs/haehdkmt</a>
</div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="64" max="64" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [64/64 16:11, Epoch 1/1]
    </div>
    
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Step</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>TrainOutput(global_step=64, training_loss=3.9099715799093246, metrics={'train_runtime': 989.3982, 'train_samples_per_second': 0.129, 'train_steps_per_second': 0.065, 'total_flos': 691252437712896.0, 'train_loss': 3.9099715799093246, 'epoch': 1.0})</code></pre>
</div>
</div>
<p>Based on the calculation above, we get <span class="math inline">\(\frac{128}{2} = 64\)</span> optimization steps for this small example training run. Training progress is logged every 10 steps to Weights &amp; Biases. There, you can see, among other metrics, that the training loss is gradually decreasing, indicating that the model is learning something:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/train-loss-wandb.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Training loss decreasing during training in Weights &amp; Biases</figcaption>
</figure>
</div>
</section>
<section id="full-training-run-on-vast.ai" class="level2">
<h2 class="anchored" data-anchor-id="full-training-run-on-vast.ai">Full Training Run on <a href="https://cloud.vast.ai/?ref_id=202191">Vast.ai*</a></h2>
<p>In the example above, I limited the size of the dataset to a very small fraction. For proper fine-tuning, the full dataset should be used and training should be repeated for multiple epochs. Similarly, logging, eval, and save frequency should be adjusted and the trained model should be pushed to the HuggingFace hub.</p>
<p>As I do not have a GPU locally, I rented a cheap GPU from <a href="https://cloud.vast.ai/?ref_id=202191">Vast.ai*</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/vastai.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Cheap RTX 4070s Ti rented from <a href="https://cloud.vast.ai/?ref_id=202191">Vast.ai*</a></figcaption>
</figure>
</div>
<p>Training on 50% of the full dataset (a bit over 100k samples) for one epoch took roughly 24 hours on the rented RTX 4070s Ti.</p>
<p>Monitoring the training and evaluation metrics on Weights &amp; Biases, shows the loss slowly going down, both in training and validation, as well as the ROUGE score gradually increasing. If the loss would only decrease on the training set but not the validation set, this would indicate overfitting to the training set.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/wandb.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Monitoring training and validation loss and ROUGE score on Weights &amp; Biases</figcaption>
</figure>
</div>
<p><strong>I pushed the final fine-tuned model to HuggingFace: <a href="https://huggingface.co/stefanbschneider/led-base-16384-lfqa-ans-len-512">stefanbschneider/led-base-16384-lfqa-ans-len-512</a></strong></p>
</section>
</section>
<section id="testing-the-fine-tuned-model" class="level1">
<h1>Testing the Fine-Tuned Model</h1>
<p>To see if the fine-tuned model learned answering questions, I load it from the HuggingFace hub and ask it the same question as the base model above: What is a transformer?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture <span class="op">--</span>no<span class="op">-</span>display</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>tuned_pipeline <span class="op">=</span> pipeline(</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    task<span class="op">=</span><span class="st">"text2text-generation"</span>,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"stefanbschneider/led-base-16384-lfqa-ans-len-512"</span>,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Abstract from "Attention is all you need" by Vaswani et al.: https://arxiv.org/abs/1706.03762</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>abstract <span class="op">=</span> <span class="st">"""The dominant sequence transduction models are based on complex recurrent or</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="st">convolutional neural networks that include an encoder and a decoder. The best</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="st">performing models also connect the encoder and decoder through an attention</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="st">mechanism. We propose a new simple network architecture, the Transformer,</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="st">based solely on attention mechanisms, dispensing with recurrence and convolutions</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="st">entirely. Experiments on two machine translation tasks show these models to</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="st">be superior in quality while being more parallelizable and requiring significantly</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="st">less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What's a transformer'?"</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="ss">f"question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss"> context: </span><span class="sc">{</span>abstract<span class="sc">}</span><span class="ss">"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>tuned_pipeline(input_text)[<span class="dv">0</span>][<span class="st">'generated_text'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>'A transformer is a transformer that is a transformer that is a transformer that is a transformer that is'</code></pre>
</div>
</div>
<p>While the model does not just repeat the input like the base model before, i.e., the format looks better, the answer itself is rubbish, unfortunately :/</p>
<p>I am not sure why the answer is so bad. Possibly because the fine-tuning did not even complete a single epoch on the entire training dataset (only half of it). Maybe, results get better with more training; at least that’s what the decreasing loss is suggesting.</p>
<p>I am somewhat disappointed by the outcome, but hope that the generall appraoch of fine-tuning still makes sense and is useful to others. If you have ideas of how I could improve the performance of the fine-tuned model, <strong>I would love to hear your suggestions</strong> (contact info is on <a href="https://stefanbschneider.github.io/">my website</a>)!</p>
</section>
<section id="whats-next" class="level1">
<h1>What’s Next?</h1>
<p>Try it yourself:</p>
<ul>
<li>Load and use my fine-tuned model directly: <a href="https://huggingface.co/stefanbschneider/led-base-16384-lfqa-ans-len-512">stefanbschneider/led-base-16384-lfqa-ans-len-512</a></li>
<li>Or load the dataset (<a href="https://huggingface.co/datasets/stefanbschneider/lfqa-max-answer-length-512">stefanbschneider/lfqa-max-answer-length-512</a>) and follow the script for fine-tuning it yourself: <a href="https://huggingface.co/stefanbschneider/led-base-16384-lfqa-ans-len-512/blob/main/led-finetune-lfqa-train.py"><code>led-finetune-lfqa-train.py</code></a></li>
<li><a href="https://cloud.vast.ai/?ref_id=202191">Rent a GPU at Vast.ai*</a></li>
</ul>
<p>Related blog posts:</p>
<ul>
<li><a href="https://stefanbschneider.github.io/blog/posts/generative-qa/">Generative Document Question Answering with HuggingFace</a></li>
<li><a href="https://stefanbschneider.github.io/blog/posts/question-answering-huggingface/">Building a Simple Q&amp;A App with HuggingFace and Gradio</a></li>
</ul>
<p>Additional Resources:</p>
<ul>
<li><a href="https://huggingface.co/docs/transformers/en/training">HuggingFace Fine-Tuning Tutorial</a></li>
<li><a href="https://keras.io/guides/transfer_learning/">Keras Transfer Learning and Fine-Tuning Guide</a></li>
</ul>
<p>*: Links to Vast.ai are affiliate links.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center"><div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>