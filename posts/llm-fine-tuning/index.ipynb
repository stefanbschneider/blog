{
  "cells": [
    {
      "cell_type": "raw",
      "id": "3c4a832b",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "categories:\n",
        "- transformer\n",
        "- llm\n",
        "- machine learning\n",
        "- HuggingFace\n",
        "- Vast.ai\n",
        "draft: true\n",
        "date: '2025-02-09'\n",
        "date-modified: '2025-02-09'\n",
        "title: Fine-Tuning a Pre-Trained LLM\n",
        "description: Fine-tuning a pre-trained language model on a custom dataset for long-form question answering using HuggingFace.\n",
        "image: images/hf-logo-with-title.png\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294d36c7",
      "metadata": {},
      "source": [
        "Large language models (LLMs) can perform all kinds of tasks ranging from translation over summarization to text generation or even multi-modal tasks involving sound, images, or videos.\n",
        "Usually, there are LLMs readily available for any kind of task, which can be easily found on [HuggingFace](https://huggingface.co/models).\n",
        "However, if there is no available model doing just what you want, then fine-tuning is the way to go.\n",
        "During fine-tuning, a pre-trained base or foundation model is further trained on a comparably small, task-specific dataset.\n",
        "Fine-tuning is much faster and cheaper than pre-training a new model from scratch.\n",
        "\n",
        "In my case, I was looking for a model to answer questions about long documents in natural language.\n",
        "Most models I could find were limited to short context lengths, i.e., could not handle entire documents as input, or were not trained to output generated natural answers (see my post [here](https://stefanbschneider.github.io/blog/posts/generative-qa/)). \n",
        "\n",
        "Hence, in this blog post, I fine-tune a pre-trained Longformer Encoder-Decoder (LED) base model for generative question answering.\n",
        "\n",
        "\n",
        "\n",
        "# Longformer Encoder-Decoder (LED) Base Model\n",
        "\n",
        "As base model, I use the Longformer Encoder-Decoder (LED) from AllenAI: [allenai/led-base-16384](https://huggingface.co/allenai/led-base-16384)\n",
        "This base model supports very long contexts as input but, as I understand, is not yet trained for any specific downstream tasks.\n",
        "There is a fine-tuned LED model for [text summarization](https://huggingface.co/allenai/led-large-16384-arxiv), but I did not see any for question answering.\n",
        "\n",
        "Taking the base model directly for answering questions does not work at all:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "272edc2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "%pip install -U datasets evaluate transformers accelerate rouge_score wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "18436498",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"question: What's a transformer? In this context: The dominant sequence transduction models are based on complex recurrent or non-convolutional neural networks that include an encoder and a decoder. The best-performing models also connect the encoder and decoder through an attention-mechanism. We propose a new simple network architecture, the Transformer, that is based on a network architecturebased solely on attention mechanisms, dispensing with recurrence and convolutions, and integrating them\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(task=\"text2text-generation\", model=\"allenai/led-base-16384\")\n",
        "\n",
        "# Abstract from \"Attention is all you need\" by Vaswani et al.: https://arxiv.org/abs/1706.03762\n",
        "abstract = \"\"\"The dominant sequence transduction models are based on complex recurrent or\n",
        "convolutional neural networks that include an encoder and a decoder. The best\n",
        "performing models also connect the encoder and decoder through an attention\n",
        "mechanism. We propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
        "entirely. Experiments on two machine translation tasks show these models to\n",
        "be superior in quality while being more parallelizable and requiring significantly\n",
        "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...\n",
        "\"\"\"\n",
        "question = \"What's a transformer'?\"\n",
        "input_text = f\"question: {question} context: {abstract}\"\n",
        "\n",
        "qa_pipeline(input_text, max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3228adff",
      "metadata": {},
      "source": [
        "The model's \"answer\" is basically just a repetition of the provided context, including the question.\n",
        "\n",
        "Let's see if fine-tuning can improve the answers.\n",
        "But first, we need a suitable dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "939dfd73",
      "metadata": {},
      "source": [
        "# Task-Specific Dataset (Long-Form Question Answering)\n",
        "\n",
        "## Finding a Suitable Dataset\n",
        "\n",
        "My task of interest is answering questions in natural language given a (potentially long) context.\n",
        "Since I do not have the means of collecting and creating my own dataset, I was looking for a suitable dataset online.\n",
        "\n",
        "The well-known [SQuAD dataset](https://huggingface.co/datasets/rajpurkar/squad_v2) is only suitable for extractive question answering, where the answer is a span text from the provided context.\n",
        "The [DuoRC dataset](https://huggingface.co/datasets/ibm-research/duorc) with questions and answers about a given movie plot can be used for both extractive and generative/abstracitve Q&A.\n",
        "However, I found the answers to be overly short, often just a few words, and not always very natural.\n",
        "\n",
        "Finally, I found a [suitable dataset for long-form question answering (LFQA)](https://huggingface.co/datasets/LLukas22/lfqa_preprocessed) with natural, multi-sentence answers to questions based on provided contexts (details on this dataset).\n",
        "The dataset is a successor of [facebook's ELI5 dataset](https://facebookresearch.github.io/ELI5/index.html) (explain like I'm five), which is [no longer available](https://huggingface.co/datasets/defunct-datasets/eli5).\n",
        "Details are in [this blog post by the dataset's authors](https://towardsdatascience.com/long-form-qa-beyond-eli5-an-updated-dataset-and-approach-319cb841aabb/).\n",
        "\n",
        "\n",
        "\n",
        "## Analyzing and Adjusting the Dataset\n",
        "\n",
        "For my task of interest, answering questions about long documents, long input contexts are \n",
        "\n",
        "Looking at the long-tail distribution of answer lengths in my selected dataset, I found that a few answers were overly long (up to ~6000 tokens).\n",
        "\n",
        "![Distribution of answer lengths in the original dataset (in number of tokens).](images/answer-lengths-tokens-original.png)\n",
        "\n",
        "Since I did not want my fine-tuned model to create overly long answers, I filtered these examples out of the dataset and made my own version of the dataset with answers only up 512 tokens.\n",
        "This means the maximum answer length is roughly 12x shorter at the cost of 10% less training data.\n",
        "\n",
        "My filtered dataset is available here: [stefanbschneider/lfqa-max-answer-length-512]\n",
        "\n",
        "An example in the dataset looks like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"question\": \"what's the difference between a forest and a wood?\",\n",
        "    \"answer\": \"They're used interchangeably a lot. You'll get different answers from different resources, but the ...\",\n",
        "    \"context\": [\n",
        "        \"Wood is divided, according to its botanical origin, into two kinds: softwoods, ...\",\n",
        "        \"Processing and products differs especially with regard to the distinction between softwood and hardwood ...\"\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "# Fine-Tuning\n",
        "\n",
        "Now that the dataset is ready, the data has to be prepared and the model has to be loaded and configured for fine-tuning.\n",
        "\n",
        "First, let's install all dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b531e18",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "id": "92d96637",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "id": "165d82ba",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c066d39f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "%pip install -U pypdf torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "85f89229",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.4559027850627899,\n",
              " 'start': 287,\n",
              " 'end': 302,\n",
              " 'answer': 'the Transformer'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "extractive_qa = pipeline(task=\"question-answering\")\n",
        "\n",
        "# Abstract from \"Attention is all you need\" by Vaswani et al.: https://arxiv.org/abs/1706.03762\n",
        "abstract = \"\"\"The dominant sequence transduction models are based on complex recurrent or\n",
        "convolutional neural networks that include an encoder and a decoder. The best\n",
        "performing models also connect the encoder and decoder through an attention\n",
        "mechanism. We propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
        "entirely. Experiments on two machine translation tasks show these models to\n",
        "be superior in quality while being more parallelizable and requiring significantly\n",
        "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...\n",
        "\"\"\"\n",
        "question = \"What's a transformer'?\"\n",
        "\n",
        "extractive_qa(question=question, context=abstract)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca64b9fc",
      "metadata": {},
      "source": [
        "# What's Next?\n",
        "\n",
        "Try it yourself:\n",
        "\n",
        "- [Rent a GPU at Vast.ai (referral link)](https://cloud.vast.ai/?ref_id=202191)\n",
        "\n",
        "\n",
        "Additional Resources:\n",
        "\n",
        "- [HuggingFace Fine-Tuning Tutorial](https://huggingface.co/docs/transformers/en/training)\n",
        "- [Keras Transfer Learning and Fine-Tuning Guide](https://keras.io/guides/transfer_learning/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
