{
  "cells": [
    {
      "cell_type": "raw",
      "id": "3c4a832b",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "categories:\n",
        "- transformer\n",
        "- llm\n",
        "- machine learning\n",
        "- HuggingFace\n",
        "- Vast.ai\n",
        "draft: true\n",
        "date: '2025-02-09'\n",
        "date-modified: '2025-02-09'\n",
        "title: Fine-Tuning a Pre-Trained LLM\n",
        "description: Fine-tuning a pre-trained language model on a custom dataset for long-form question answering using HuggingFace.\n",
        "image: images/hf-logo-with-title.png\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294d36c7",
      "metadata": {},
      "source": [
        "Large language models (LLMs) can perform all kinds of tasks ranging from translation over summarization to text generation or even multi-modal tasks involving sound, images, or videos.\n",
        "Usually, there are LLMs readily available for any kind of task, which can be easily found on [HuggingFace](https://huggingface.co/models).\n",
        "However, if there is no available model doing just what you want, then fine-tuning is the way to go.\n",
        "During fine-tuning, a pre-trained base or foundation model is further trained on a comparably small, task-specific dataset.\n",
        "Fine-tuning is much faster and cheaper than pre-training a new model from scratch.\n",
        "\n",
        "In my case, I was looking for a model to answer questions about long documents in natural language.\n",
        "Most models I could find were limited to short context lengths, i.e., could not handle entire documents as input, or were not trained to output generated natural answers (see my post [here](https://stefanbschneider.github.io/blog/posts/generative-qa/)). \n",
        "\n",
        "Hence, in this blog post, I fine-tune a pre-trained Longformer Encoder-Decoder (LED) base model for generative question answering.\n",
        "\n",
        "\n",
        "\n",
        "# Longformer Encoder-Decoder (LED) Base Model\n",
        "\n",
        "As base model, I use the Longformer Encoder-Decoder (LED) from AllenAI: [allenai/led-base-16384](https://huggingface.co/allenai/led-base-16384)\n",
        "This base model supports very long contexts as input but, as I understand, is not yet trained for any specific downstream tasks.\n",
        "Note, there is a larger version of the LED base model with even more trainable weights/parameters: [allenai/led-large-16384](https://huggingface.co/allenai/led-large-16384)\n",
        "Fine-tuning this larger model could lead to better results but also needs more resources and time to train.\n",
        "\n",
        "There is a fine-tuned LED model for [text summarization](https://huggingface.co/allenai/led-large-16384-arxiv), but I did not see any for question answering.\n",
        "Taking the (smaller) base model directly for answering questions does not work at all:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "272edc2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "%pip install -U datasets evaluate transformers accelerate rouge_score wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "18436498",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"question: What's a transformer? In this context: The dominant sequence transduction models are based on complex recurrent or non-convolutional neural networks that include an encoder and a decoder. The best-performing models also connect the encoder and decoder through an attention-mechanism. We propose a new simple network architecture, the Transformer, that is based on a network architecturebased solely on attention mechanisms, dispensing with recurrence and convolutions, and integrating them\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(task=\"text2text-generation\", model=\"allenai/led-base-16384\")\n",
        "\n",
        "# Abstract from \"Attention is all you need\" by Vaswani et al.: https://arxiv.org/abs/1706.03762\n",
        "abstract = \"\"\"The dominant sequence transduction models are based on complex recurrent or\n",
        "convolutional neural networks that include an encoder and a decoder. The best\n",
        "performing models also connect the encoder and decoder through an attention\n",
        "mechanism. We propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
        "entirely. Experiments on two machine translation tasks show these models to\n",
        "be superior in quality while being more parallelizable and requiring significantly\n",
        "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...\n",
        "\"\"\"\n",
        "question = \"What's a transformer'?\"\n",
        "input_text = f\"question: {question} context: {abstract}\"\n",
        "\n",
        "qa_pipeline(input_text, max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3228adff",
      "metadata": {},
      "source": [
        "The model's \"answer\" is basically just a repetition of the provided context, including the question.\n",
        "\n",
        "Let's see if fine-tuning can improve the answers.\n",
        "But first, we need a suitable dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "939dfd73",
      "metadata": {},
      "source": [
        "# Task-Specific Dataset (Long-Form Question Answering)\n",
        "\n",
        "## Finding a Suitable Dataset\n",
        "\n",
        "My task of interest is answering questions in natural language given a (potentially long) context.\n",
        "Since I do not have the means of collecting and creating my own dataset, I was looking for a suitable dataset online.\n",
        "\n",
        "The well-known [SQuAD dataset](https://huggingface.co/datasets/rajpurkar/squad_v2) is only suitable for extractive question answering, where the answer is a span text from the provided context.\n",
        "The [DuoRC dataset](https://huggingface.co/datasets/ibm-research/duorc) with questions and answers about a given movie plot can be used for both extractive and generative/abstracitve Q&A.\n",
        "However, I found the answers to be overly short, often just a few words, and not always very natural.\n",
        "\n",
        "Finally, I found a [suitable dataset for long-form question answering (LFQA)](https://huggingface.co/datasets/LLukas22/lfqa_preprocessed) with natural, multi-sentence answers to questions based on provided contexts (details on this dataset).\n",
        "The dataset is a successor of [facebook's ELI5 dataset](https://facebookresearch.github.io/ELI5/index.html) (explain like I'm five), which is [no longer available](https://huggingface.co/datasets/defunct-datasets/eli5).\n",
        "Details are in [this blog post by the dataset's authors](https://towardsdatascience.com/long-form-qa-beyond-eli5-an-updated-dataset-and-approach-319cb841aabb/).\n",
        "\n",
        "\n",
        "\n",
        "## Analyzing and Adjusting the Dataset\n",
        "\n",
        "To get familiar with the dataset and understand what kind of inputs and outputs the fine-tuned model has to handle,\n",
        "I visualized the length of contexts (model input, together with the questions) as well as the length of the expected answers (model output) in the dataset.\n",
        "Since I am interested in the length in terms of number of tokens (relevant for fine-tuning later), I first tokenized the contexts and answers with the LED base model's tokenizer.\n",
        "\n",
        "The context lengths are quite normally distributed with a few quite long contexts, but most rather short:\n",
        "\n",
        "![Distribution of context lengths (in number of tokens).](images/context-lengths-tokens-original.png)\n",
        "\n",
        "Since I am interested in long contexts, it is fine to have a few contexts that are much longer than the others.\n",
        "\n",
        "The answer lengths have an even stronger long-tail distribution with some few answers that were overly long (up to ~6000 tokens, even longer than the context!).\n",
        "\n",
        "![Distribution of answer lengths in the original dataset (in number of tokens).](images/answer-lengths-tokens-original.png)\n",
        "\n",
        "Since I did not want my fine-tuned model to create overly long answers, I filtered these examples out of the dataset and made my own version of the dataset with answers only up 512 tokens.\n",
        "This means the maximum answer length is roughly 12x shorter at the cost of 10% less training data.\n",
        "\n",
        "My filtered dataset is available here: [stefanbschneider/lfqa-max-answer-length-512](https://huggingface.co/datasets/stefanbschneider/lfqa-max-answer-length-512)\n",
        "The notebook I used for creating the filtered dataset as well as the plots is also in the repository: [`process-lfqa-dataset.ipynb`](https://huggingface.co/datasets/stefanbschneider/lfqa-max-answer-length-512/blob/main/process-lfqa-dataset.ipynb)\n",
        "\n",
        "An example in the dataset looks like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"question\": \"what's the difference between a forest and a wood?\",\n",
        "    \"answer\": \"They're used interchangeably a lot. You'll get different answers from different resources, but the ...\",\n",
        "    \"context\": [\n",
        "        \"Wood is divided, according to its botanical origin, into two kinds: softwoods, ...\",\n",
        "        \"Processing and products differs especially with regard to the distinction between softwood and hardwood ...\"\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "# Fine-Tuning\n",
        "\n",
        "Now that the dataset is ready, the data has to be prepared and the model has to be loaded and configured for fine-tuning.\n",
        "\n",
        "Let's start by importing the necessary libraries and loading the LED base model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693079d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    GenerationConfig,\n",
        ")\n",
        "\n",
        "# load model and tokenizer\n",
        "model_name = \"allenai/led-base-16384\"\n",
        "# Load model and enable gradient checkpointing to reduce memory during training (at the cost of speed)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model.gradient_checkpointing_enable()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "879ab44f",
      "metadata": {},
      "source": [
        "Next, I create two functions for processing the data for training and validation.\n",
        "These functions prepare the data in batches of size 2 (larger batches do not fit onto my GPU).\n",
        "I found that batch size 1 did not work at all; the loss quickly dropped to zero and the model stopped learning.\n",
        "\n",
        "Since each question is paired with a list of multiple contexts, these contexts are concatenated to the corresponding question into one single string, which is given as input to the model.\n",
        "Note that the expected output length is also set to 512 tokens here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "92d96637",
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE: int = 2\n",
        "\n",
        "def process_data_to_model_inputs(batch):\n",
        "    # combine context strings and questions to one input\n",
        "    input = [\n",
        "        f\"question: {question}, context: {' '.join(context)}\"\n",
        "        for question, context in zip(batch[\"question\"], batch[\"context\"])\n",
        "    ]\n",
        "\n",
        "    # tokenize the inputs and labels\n",
        "    inputs = tokenizer(\n",
        "        input,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        # Max supported article/context length + question.\n",
        "        max_length=8192,\n",
        "    )\n",
        "    outputs = tokenizer(\n",
        "        batch[\"answer\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        # Since I limit the answers to 512 tokens in the dataset, I can also limit the max_length here\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "    # The following settings are copied from the fine-tuning notebook provided by AllenAI:\n",
        "    # https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "    # create 0 global_attention_mask lists\n",
        "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
        "    ]\n",
        "\n",
        "    # since above lists are references, the following line changes the 0 index for all samples\n",
        "    batch[\"global_attention_mask\"][0][0] = 1\n",
        "    batch[\"labels\"] = outputs.input_ids\n",
        "\n",
        "    # We have to make sure that the PAD token is ignored\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
        "        for labels in batch[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "def load_and_process_dataset(split: str, dataset_limit: Optional[int] = None):\n",
        "    \"\"\"Load and process the dataset for training or validation. Optionally limit the number of samples.\"\"\"\n",
        "    dataset = load_dataset(\"stefanbschneider/lfqa-max-answer-length-512\", split=split)\n",
        "\n",
        "    # optionally reduce the data sets to a small fraction\n",
        "    if dataset_limit is not None:\n",
        "        dataset = dataset.select(range(dataset_limit))\n",
        "\n",
        "    # Process the dataset with the function above. Afterwards, remove the original columns.\n",
        "    dataset = dataset.map(\n",
        "        process_data_to_model_inputs,\n",
        "        batched=True,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        remove_columns=[\"context\", \"question\", \"answer\"],\n",
        "    )\n",
        "\n",
        "    # Format the dataset to torch\n",
        "    dataset.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        "    )\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9ffac80",
      "metadata": {},
      "source": [
        "For development and experimentation, it is useful to only load a small fraction of the dataset, using the `dataset_limit` argument I introduced above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "165d82ba",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1e9264cac0f4de0bb2a9936cbf4b7a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8346d4b459a241c8951548eccedbf7ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load and process datasets; limit to small size for experimentation\n",
        "train_data = load_and_process_dataset(\"train\", dataset_limit=128)\n",
        "val_data = load_and_process_dataset(\"validation\", dataset_limit=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb821b0",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ca64b9fc",
      "metadata": {},
      "source": [
        "# What's Next?\n",
        "\n",
        "Try it yourself:\n",
        "\n",
        "- [Rent a GPU at Vast.ai (referral link)](https://cloud.vast.ai/?ref_id=202191)\n",
        "\n",
        "\n",
        "Additional Resources:\n",
        "\n",
        "- [HuggingFace Fine-Tuning Tutorial](https://huggingface.co/docs/transformers/en/training)\n",
        "- [Keras Transfer Learning and Fine-Tuning Guide](https://keras.io/guides/transfer_learning/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
