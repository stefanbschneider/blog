{
  "cells": [
    {
      "cell_type": "raw",
      "id": "3c4a832b",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "categories:\n",
        "- transformer\n",
        "- llm\n",
        "- machine learning\n",
        "- HuggingFace\n",
        "- Vast.ai\n",
        "draft: true\n",
        "date: '2025-02-09'\n",
        "date-modified: '2025-02-09'\n",
        "title: Fine-Tuning a Pre-Trained LLM\n",
        "description: Fine-tuning a pre-trained language model on a custom dataset for long-form question answering using HuggingFace.\n",
        "image: images/hf-logo-with-title.png\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294d36c7",
      "metadata": {},
      "source": [
        "Large language models (LLMs) can perform all kinds of tasks ranging from translation over summarization to text generation or even image and video generation.\n",
        "Usually, there are LLMs readily available for any kind of task, which can be easily found on [HuggingFace](https://huggingface.co/models).\n",
        "However, if there is no available model doing just what you want, then you use a pre-trained foundation or base model and fine-tune it for the specific task it should perform.\n",
        "\n",
        "In my case, I was looking for a model to answer questions about long documents in natural language.\n",
        "Most models I could find were limited to short context lengths, i.e., could not handle entire documents as input, or were not trained to output generated natural answers (see my post [here](https://stefanbschneider.github.io/blog/posts/generative-qa/)). \n",
        "\n",
        "In this blog post, I fine-tune a pre-trained Longformer Encoder-Decoder (LED) model for generative question answering.\n",
        "Fine-tuning builds on available foundation or base models, i.e., LLMs that were pre-trained on massive amounts of data (usually Internet scale) in a self-supervised fashion.\n",
        "During fine-tuning, such a base model is then further trained in a supervised way on a much smaller task-specific dataset.\n",
        "As a result, fine-tuning is much faster and cheaper than pre-training from scratch, but it does require a suitable dataset.\n",
        "\n",
        "\n",
        "\n",
        "# Base Model\n",
        "\n",
        "As base model, I use the Longformer Encoder-Decoder (LED) from AllenAI: [allenai/led-base-16384](https://huggingface.co/allenai/led-base-16384)\n",
        "\n",
        "There is a fine-tuned version of this model for [text summarization](https://huggingface.co/allenai/led-large-16384-arxiv), but I did not see any for question answering.\n",
        "\n",
        "TODO: Demonstrate lack of question answering when using the base model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "272edc2d",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "939dfd73",
      "metadata": {},
      "source": [
        "# Task-Specific Dataset (Long-Form Question Answering)\n",
        "\n",
        "My task of interest is answering questions in natural language given a (potentially long) context.\n",
        "Since I do not have the means of collecting and creating my own dataset, I was looking for a suitable dataset online.\n",
        "\n",
        "The well-known [SQuAD dataset](https://huggingface.co/datasets/rajpurkar/squad_v2) is only suitable for extractive question answering, where the answer is a span text from the provided context.\n",
        "The [DuoRC dataset](https://huggingface.co/datasets/ibm-research/duorc) with questions and answers about a given movie plot can be used for both extractive and generative/abstracitve Q&A.\n",
        "However, I found the answers to be overly short, often just a few words, and not always very natural.\n",
        "\n",
        "Finally, I found a [suitable dataset for long-form question answering (LFQA)](https://huggingface.co/datasets/LLukas22/lfqa_preprocessed) with natural, multi-sentence answers to questions based on provided contexts.\n",
        "Other, similar datasets are the [ELI5 dataset](https://huggingface.co/datasets/defunct-datasets/eli5) (no longer available) and the [natural questions dataset](https://huggingface.co/datasets/google-research-datasets/natural_questions).\n",
        "\n",
        "Looking at the long-tail distribution of answer lengths in my selected dataset, I found that a few answers were overly long (up to ~6000 tokens).\n",
        "\n",
        "![Distribution of answer lengths in the original dataset (in number of tokens).](images/answer-lengths-tokens-original.png)\n",
        "\n",
        "Since I did not want my fine-tuned model to create overly long answers, I filtered these examples out of the dataset and made my own version of the dataset with answers only up 512 tokens.\n",
        "This means the maximum answer length is roughly 12x shorter at the cost of 10% less training data.\n",
        "\n",
        "My filtered dataset is available here: [stefanbschneider/lfqa-max-answer-length-512]\n",
        "\n",
        "An example in the dataset looks like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"question\": \"what's the difference between a forest and a wood?\",\n",
        "    \"answer\": \"They're used interchangeably a lot. You'll get different answers from different resources, but the ...\",\n",
        "    \"context\": [\n",
        "        \"Wood is divided, according to its botanical origin, into two kinds: softwoods, ...\",\n",
        "        \"Processing and products differs especially with regard to the distinction between softwood and hardwood ...\"\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "# Fine-Tuning\n",
        "\n",
        "Now that the dataset is ready, the data has to be prepared and the model has to be loaded and configured for fine-tuning.\n",
        "\n",
        "First, let's install all dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b531e18",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "id": "92d96637",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "id": "165d82ba",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c066d39f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "%pip install -U pypdf torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "85f89229",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.4559027850627899,\n",
              " 'start': 287,\n",
              " 'end': 302,\n",
              " 'answer': 'the Transformer'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "extractive_qa = pipeline(task=\"question-answering\")\n",
        "\n",
        "# Abstract from \"Attention is all you need\" by Vaswani et al.: https://arxiv.org/abs/1706.03762\n",
        "abstract = \"\"\"The dominant sequence transduction models are based on complex recurrent or\n",
        "convolutional neural networks that include an encoder and a decoder. The best\n",
        "performing models also connect the encoder and decoder through an attention\n",
        "mechanism. We propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
        "entirely. Experiments on two machine translation tasks show these models to\n",
        "be superior in quality while being more parallelizable and requiring significantly\n",
        "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...\n",
        "\"\"\"\n",
        "question = \"What's a transformer'?\"\n",
        "\n",
        "extractive_qa(question=question, context=abstract)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
