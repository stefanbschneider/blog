{
  "cells": [
    {
      "cell_type": "raw",
      "id": "3c4a832b",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "categories:\n",
        "- transformer\n",
        "- llm\n",
        "- machine learning\n",
        "- HuggingFace\n",
        "- Vast.ai\n",
        "date: '2025-02-09'\n",
        "date-modified: '2025-02-09'\n",
        "title: Fine-Tuning a Pre-Trained LLM\n",
        "description: Fine-tuning a pre-trained language model on a custom dataset for long-form question answering using HuggingFace.\n",
        "image: images/fine-tuning.jpg\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294d36c7",
      "metadata": {},
      "source": [
        "Large language models (LLMs) can perform all kinds of tasks ranging from translation over summarization to text generation or even multi-modal tasks involving sound, images, or videos.\n",
        "Usually, there are LLMs readily available for any kind of task, which can be easily found on [HuggingFace](https://huggingface.co/models).\n",
        "However, if there is no available model doing just what you want, then fine-tuning is the way to go.\n",
        "During fine-tuning, a pre-trained base or foundation model is further trained on a comparably small, task-specific dataset.\n",
        "Fine-tuning is much faster and cheaper than pre-training a new model from scratch.\n",
        "\n",
        "![Illustration of the fine-tuning process.](images/fine-tuning.jpg)\n",
        "\n",
        "In my case, I was looking for a model to answer questions about long documents in natural language.\n",
        "Most models I could find were limited to short context lengths, i.e., could not handle entire documents as input, or were not trained to output generated natural answers (see my post [here](https://stefanbschneider.github.io/blog/posts/generative-qa/)). \n",
        "\n",
        "Hence, in this blog post, I fine-tune a pre-trained Longformer Encoder-Decoder (LED) base model for generative question answering.\n",
        "\n",
        "\n",
        "\n",
        "# Longformer Encoder-Decoder (LED) Base Model\n",
        "\n",
        "As base model, I use the Longformer Encoder-Decoder (LED) from AllenAI: [allenai/led-base-16384](https://huggingface.co/allenai/led-base-16384)\n",
        "This base model supports very long contexts as input but, as I understand, is not yet trained for any specific downstream tasks.\n",
        "Note, there is a larger version of the LED base model with even more trainable weights/parameters: [allenai/led-large-16384](https://huggingface.co/allenai/led-large-16384)\n",
        "Fine-tuning this larger model could lead to better results but also needs more resources and time to train.\n",
        "\n",
        "There is a fine-tuned LED model for [text summarization](https://huggingface.co/allenai/led-large-16384-arxiv), but I did not see any for question answering.\n",
        "Taking the (smaller) base model directly for answering questions does not work at all:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "272edc2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "%pip install -U datasets evaluate transformers accelerate rouge_score wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "18436498",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n",
            "Input ids are automatically padded from 144 to 1024 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"question: What's a transformer? In this context: The dominant sequence transduction models are based on complex recurrent or non-convolutional neural networks that include an encoder and a decoder. The best-performing models also connect the encoder and decoder through an attention-mechanism. We propose a new simple network architecture, the Transformer, that is based on a network architecturebased solely on attention mechanisms, dispensing with recurrence and convolutions, and integrating them\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(task=\"text2text-generation\", model=\"allenai/led-base-16384\")\n",
        "\n",
        "# Abstract from \"Attention is all you need\" by Vaswani et al.: https://arxiv.org/abs/1706.03762\n",
        "abstract = \"\"\"The dominant sequence transduction models are based on complex recurrent or\n",
        "convolutional neural networks that include an encoder and a decoder. The best\n",
        "performing models also connect the encoder and decoder through an attention\n",
        "mechanism. We propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
        "entirely. Experiments on two machine translation tasks show these models to\n",
        "be superior in quality while being more parallelizable and requiring significantly\n",
        "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...\n",
        "\"\"\"\n",
        "question = \"What's a transformer'?\"\n",
        "input_text = f\"question: {question} context: {abstract}\"\n",
        "\n",
        "qa_pipeline(input_text, max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3228adff",
      "metadata": {},
      "source": [
        "The model's \"answer\" is basically just a repetition of the provided context, including the question.\n",
        "\n",
        "Let's see if fine-tuning can improve the answers.\n",
        "But first, we need a suitable dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "939dfd73",
      "metadata": {},
      "source": [
        "# Task-Specific Dataset (Long-Form Question Answering)\n",
        "\n",
        "## Finding a Suitable Dataset\n",
        "\n",
        "My task of interest is answering questions in natural language given a (potentially long) context.\n",
        "Since I do not have the means of collecting and creating my own dataset, I was looking for a suitable dataset online.\n",
        "\n",
        "The well-known [SQuAD dataset](https://huggingface.co/datasets/rajpurkar/squad_v2) is only suitable for extractive question answering, where the answer is a span text from the provided context.\n",
        "The [DuoRC dataset](https://huggingface.co/datasets/ibm-research/duorc) with questions and answers about a given movie plot can be used for both extractive and generative/abstracitve Q&A.\n",
        "However, I found the answers to be overly short, often just a few words, and not always very natural.\n",
        "\n",
        "Finally, I found a [suitable dataset for long-form question answering (LFQA)](https://huggingface.co/datasets/LLukas22/lfqa_preprocessed) with natural, multi-sentence answers to questions based on provided contexts (details on this dataset).\n",
        "The dataset is a successor of [facebook's ELI5 dataset](https://facebookresearch.github.io/ELI5/index.html) (explain like I'm five), which is [no longer available](https://huggingface.co/datasets/defunct-datasets/eli5).\n",
        "Details are in [this blog post by the dataset's authors](https://towardsdatascience.com/long-form-qa-beyond-eli5-an-updated-dataset-and-approach-319cb841aabb/).\n",
        "\n",
        "\n",
        "\n",
        "## Analyzing and Adjusting the Dataset\n",
        "\n",
        "To get familiar with the dataset and understand what kind of inputs and outputs the fine-tuned model has to handle,\n",
        "I visualized the length of contexts (model input, together with the questions) as well as the length of the expected answers (model output) in the dataset.\n",
        "Since I am interested in the length in terms of number of tokens (relevant for fine-tuning later), I first tokenized the contexts and answers with the LED base model's tokenizer.\n",
        "\n",
        "The context lengths are quite normally distributed with a few quite long contexts, but most rather short:\n",
        "\n",
        "![Distribution of context lengths (in number of tokens).](images/context-lengths-tokens-original.png)\n",
        "\n",
        "Since I am interested in long contexts, it is fine to have a few contexts that are much longer than the others.\n",
        "\n",
        "The answer lengths have an even stronger long-tail distribution with some few answers that were overly long (up to ~6000 tokens, even longer than the context!).\n",
        "\n",
        "![Distribution of answer lengths in the original dataset (in number of tokens).](images/answer-lengths-tokens-original.png)\n",
        "\n",
        "Since I did not want my fine-tuned model to create overly long answers, I filtered these examples out of the dataset and made my own version of the dataset with answers only up 512 tokens.\n",
        "This means the maximum answer length is roughly 12x shorter at the cost of 10% less training data.\n",
        "\n",
        "My filtered dataset is available here: [stefanbschneider/lfqa-max-answer-length-512](https://huggingface.co/datasets/stefanbschneider/lfqa-max-answer-length-512)\n",
        "The notebook I used for creating the filtered dataset as well as the plots is also in the repository: [`process-lfqa-dataset.ipynb`](https://huggingface.co/datasets/stefanbschneider/lfqa-max-answer-length-512/blob/main/process-lfqa-dataset.ipynb)\n",
        "\n",
        "An example in the dataset looks like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"question\": \"what's the difference between a forest and a wood?\",\n",
        "    \"answer\": \"They're used interchangeably a lot. You'll get different answers from different resources, but the ...\",\n",
        "    \"context\": [\n",
        "        \"Wood is divided, according to its botanical origin, into two kinds: softwoods, ...\",\n",
        "        \"Processing and products differs especially with regard to the distinction between softwood and hardwood ...\"\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "# Fine-Tuning\n",
        "\n",
        "Now that the dataset is ready, the data has to be prepared and the model has to be loaded and configured for fine-tuning.\n",
        "\n",
        "Let's start by importing the necessary libraries and loading the LED base model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "693079d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    GenerationConfig,\n",
        ")\n",
        "\n",
        "# load model and tokenizer\n",
        "model_name = \"allenai/led-base-16384\"\n",
        "# Load model and enable gradient checkpointing to reduce memory during training (at the cost of speed)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model.gradient_checkpointing_enable()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "879ab44f",
      "metadata": {},
      "source": [
        "## Preparing the Data\n",
        "\n",
        "Next, I create two functions for processing the data for training and validation.\n",
        "These functions prepare the data in batches of size 2 (larger batches do not fit onto my GPU).\n",
        "I found that batch size 1 did not work at all; the loss quickly dropped to zero and the model stopped learning.\n",
        "\n",
        "Since each question is paired with a list of multiple contexts, these contexts are concatenated to the corresponding question into one single string, which is given as input to the model.\n",
        "Note that the expected output length is also set to 512 tokens here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "92d96637",
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE: int = 2\n",
        "\n",
        "def process_data_to_model_inputs(batch):\n",
        "    # combine context strings and questions to one input\n",
        "    input = [\n",
        "        f\"question: {question}, context: {' '.join(context)}\"\n",
        "        for question, context in zip(batch[\"question\"], batch[\"context\"])\n",
        "    ]\n",
        "\n",
        "    # tokenize the inputs and labels\n",
        "    inputs = tokenizer(\n",
        "        input,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        # Max supported article/context length + question.\n",
        "        max_length=8192,\n",
        "    )\n",
        "    outputs = tokenizer(\n",
        "        batch[\"answer\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        # Since I limit the answers to 512 tokens in the dataset, I can also limit the max_length here\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "    # The following settings are copied from the fine-tuning notebook provided by AllenAI:\n",
        "    # https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "    # create 0 global_attention_mask lists\n",
        "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
        "    ]\n",
        "\n",
        "    # since above lists are references, the following line changes the 0 index for all samples\n",
        "    batch[\"global_attention_mask\"][0][0] = 1\n",
        "    batch[\"labels\"] = outputs.input_ids\n",
        "\n",
        "    # We have to make sure that the PAD token is ignored\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
        "        for labels in batch[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "def load_and_process_dataset(split: str, dataset_limit: Optional[int] = None):\n",
        "    \"\"\"Load and process the dataset for training or validation. Optionally limit the number of samples.\"\"\"\n",
        "    dataset = load_dataset(\"stefanbschneider/lfqa-max-answer-length-512\", split=split)\n",
        "\n",
        "    # optionally reduce the data sets to a small fraction\n",
        "    if dataset_limit is not None:\n",
        "        dataset = dataset.select(range(dataset_limit))\n",
        "\n",
        "    # Process the dataset with the function above. Afterwards, remove the original columns.\n",
        "    dataset = dataset.map(\n",
        "        process_data_to_model_inputs,\n",
        "        batched=True,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        remove_columns=[\"context\", \"question\", \"answer\"],\n",
        "    )\n",
        "\n",
        "    # Format the dataset to torch\n",
        "    dataset.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        "    )\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9ffac80",
      "metadata": {},
      "source": [
        "For development and experimentation, it is useful to only load a small fraction of the dataset, using the `dataset_limit` argument I introduced above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "165d82ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df511791d7854e3994ac7fbf8f18d6c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load and process datasets; limit to small size for experimentation\n",
        "train_data = load_and_process_dataset(\"train\", dataset_limit=128)\n",
        "val_data = load_and_process_dataset(\"validation\", dataset_limit=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb821b0",
      "metadata": {},
      "source": [
        "## Configuring the Model and Training\n",
        "\n",
        "With the data loaded and ready for training, both the model and the training have to be configured.\n",
        "The generation config defines how the model generates new answers.\n",
        "Here, I set answers to be 100 to 512 tokens long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1c090f73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and set the model's generation config\n",
        "generation_config = GenerationConfig(\n",
        "    # The generated answer should be 100-512 tokens long\n",
        "    max_length=512,\n",
        "    min_length=100,\n",
        "    early_stopping=True,\n",
        "    num_beams=4,\n",
        "    length_penalty=2.0,\n",
        "    # Don't repeat n=3-grams (same words in same order) in the generated text --> more natural\n",
        "    no_repeat_ngram_size=3,\n",
        "    decoder_start_token_id=tokenizer.cls_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        ")\n",
        "model.generation_config = generation_config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b86df4",
      "metadata": {},
      "source": [
        "The training arguments control the training procedure as well as how (often) the model is saved, evaluated, and how training is monitored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "14bc59d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    # fp16 only works on GPU, not on M1 mps. mps is used by default if it's available\n",
        "    #fp16=True,\n",
        "    output_dir=\"models\",\n",
        "    logging_steps=10,    # for proper training: 100,\n",
        "    eval_steps=1000,\n",
        "    #save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    # Save to HF hub & log to wandb\n",
        "    #push_to_hub=True,\n",
        "    #hub_model_id=\"stefanbschneider/led-base-16384-lfqa-ans-len-512\",\n",
        "    log_level=\"info\",\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"test-run\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca9b95c",
      "metadata": {},
      "source": [
        "I use batch size 2 (set above) and save the model locally to \"models\".\n",
        "\n",
        "For logging, I use [Weights & Biases](https://wandb.ai/home), which is also the default.\n",
        "For that to work, you need a free W&B account.\n",
        "`logging_steps` determines the frequency (in number of optimization steps) in which the training results are logged locally and to W&B.\n",
        "Similarly, `eval_steps` controls how often the evaluation is run on the `val_data` prepared above.\n",
        "During training, a model checkpoint is saved locally every `save_steps`, maintaining at most `save_total_limit` checkpoints locally (then overwriting old checkpoints).\n",
        "I do not use [gradient accumulation](https://huggingface.co/docs/transformers/en/perf_train_gpu_one#gradient-accumulation), which can mimic higher batch sizes even with less memory.\n",
        "Finally, `num_train_epochs` sets the number of training epochs, i.e., how often training is repeated on the training set.\n",
        "\n",
        "These paramters together with the size of the training set (above limited to 128 for debugging) determine the overall number of optimization steps during training:\n",
        "\n",
        "$$\\text{num\\_steps} = \\frac{\\text{num\\_examples\\_in\\_data}}{\\text{batch\\_size} * \\text{gradient\\_accumulation}} * \\text{epochs}$$\n",
        "\n",
        "If `push_to_hub=True`, new checkpoints are automatically pushed to the HuggingFace hub.\n",
        "This is definitely recommended when running the full fine-tuning, so that the model is safely stored online and can easily be used later on.\n",
        "This needs a HuggingFace account. \n",
        "Once the account is created, log in locally with the CLI: `huggingface-cli login` and paste an [access token](https://huggingface.co/settings/tokens) to connect with your account.\n",
        "\n",
        "\n",
        "## Training and Monitoring\n",
        "\n",
        "To evaluate the model on the validation set, we need to define a validation function that computes some metric of interest.\n",
        "It seems like long-form question answering is generally hard to evaluate automatically (see [paper](https://arxiv.org/abs/2305.18201)),\n",
        "so I just resort to the [ROUGE score](https://huggingface.co/spaces/evaluate-metric/rouge), which is often used in summarization.\n",
        "It measures the similarity between the predicted answer by the model and the expected answer from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "50219e38",
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(pred) -> dict[str, float]:\n",
        "    \"\"\"Compute rouge score during validation/evaluation\"\"\"\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(\n",
        "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"]\n",
        "\n",
        "    # Return rouge2 F1 score\n",
        "    return {\"rouge2\": round(rouge_output, 4)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ffdf224",
      "metadata": {},
      "source": [
        "Finally, putting all this together, we can start training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e6f37746",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "***** Running training *****\n",
            "  Num examples = 128\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 64\n",
            "  Number of trainable parameters = 161,844,480\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstefanbschneider\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/stefanshschneider/Projects/private/blog/posts/llm-fine-tuning/wandb/run-20250209_132357-haehdkmt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/stefanbschneider/huggingface/runs/haehdkmt' target=\"_blank\">test-run</a></strong> to <a href='https://wandb.ai/stefanbschneider/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/stefanbschneider/huggingface' target=\"_blank\">https://wandb.ai/stefanbschneider/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/stefanbschneider/huggingface/runs/haehdkmt' target=\"_blank\">https://wandb.ai/stefanbschneider/huggingface/runs/haehdkmt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 16:11, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to models/checkpoint-64\n",
            "Configuration saved in models/checkpoint-64/config.json\n",
            "Configuration saved in models/checkpoint-64/generation_config.json\n",
            "Model weights saved in models/checkpoint-64/model.safetensors\n",
            "tokenizer config file saved in models/checkpoint-64/tokenizer_config.json\n",
            "Special tokens file saved in models/checkpoint-64/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=64, training_loss=3.9099715799093246, metrics={'train_runtime': 989.3982, 'train_samples_per_second': 0.129, 'train_steps_per_second': 0.065, 'total_flos': 691252437712896.0, 'train_loss': 3.9099715799093246, 'epoch': 1.0})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        ")\n",
        "trainer.train()\n",
        "# Optionally, push the final trained model to the HuggingFace hub\n",
        "# trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf729a4e",
      "metadata": {},
      "source": [
        "Based on the calculation above, we get $\\frac{128}{2} = 64$ optimization steps for this small example training run.\n",
        "Training progress is logged every 10 steps to Weights & Biases.\n",
        "There, you can see, among other metrics, that the training loss is gradually decreasing, indicating that the model is learning something:\n",
        "\n",
        "![Training loss decreasing during training in Weights & Biases](images/train-loss-wandb.png)\n",
        "\n",
        "\n",
        "\n",
        "## Full Training Run on [Vast.ai*](https://cloud.vast.ai/?ref_id=202191)\n",
        "\n",
        "In the example above, I limited the size of the dataset to a very small fraction.\n",
        "For proper fine-tuning, the full dataset should be used and training should be repeated for multiple epochs.\n",
        "Similarly, logging, eval, and save frequency should be adjusted and the trained model should be pushed to the HuggingFace hub.\n",
        "\n",
        "As I do not have a GPU locally, I rented a cheap GPU from [Vast.ai*](https://cloud.vast.ai/?ref_id=202191).\n",
        "\n",
        "![Cheap RTX 4070s Ti rented from [Vast.ai*](https://cloud.vast.ai/?ref_id=202191)](images/vastai.png)\n",
        "\n",
        "Training on 50% of the full dataset (a bit over 100k samples) for one epoch took roughly 24 hours on the rented RTX 4070s Ti.\n",
        "\n",
        "Monitoring the training and evaluation metrics on Weights & Biases, shows the loss slowly going down, both in training and validation, as well as the ROUGE score gradually increasing.\n",
        "If the loss would only decrease on the training set but not the validation set, this would indicate overfitting to the training set.\n",
        "\n",
        "![Monitoring training and validation loss and ROUGE score on Weights & Biases](images/wandb.png)\n",
        "\n",
        "**I pushed the final fine-tuned model to HuggingFace: [stefanbschneider/led-base-16384-lfqa-ans-len-512](https://huggingface.co/stefanbschneider/led-base-16384-lfqa-ans-len-512)**\n",
        "\n",
        "\n",
        "# Testing the Fine-Tuned Model\n",
        "\n",
        "To see if the fine-tuned model learned answering questions, I load it from the HuggingFace hub and ask it the same question as the base model above: What is a transformer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "996f0ebb",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "tuned_pipeline = pipeline(\n",
        "    task=\"text2text-generation\",\n",
        "    model=\"stefanbschneider/led-base-16384-lfqa-ans-len-512\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "48baa479",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Input ids are automatically padded from 144 to 1024 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'A transformer is a type of neural network architecture.  It\\'s basically a network architecture that uses a set of neural networks to communicate with each other.  \\n\\nFor example, if you have a neural network that is connected to a computer, it will send a signal to the computer.  If you connect a computer to a network, the computer will send the signal to your computer, which will then send it to the network.  This is called a \"transformer\".\\n\\nThe Transformer is basically a computer that connects a network to another network, and sends the signal back and forth.\\n\\n_URL_0_\\n\\n  \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n > \\n\\n   \\n\\n \\n\\n\\n \\n,\\n\\n\\n > \\n \\n\\n  ,\\n\\nI\\'m not sure what you mean by \" transformer\"., I\\'m sure you\\'re talking about \" transformer\" or \" transformer.\"\\n\\nIf you\\'re referring to \" transformer\", then you\\'re probably referring to the \"Transformer\" and \" transformer\":\\n\\n* Transformer\" is the same thing as transformer.\\n\\n**Transformer* is a transformer.**.. .\\n,.\\n\\n:\\n:: _:\\n\\n,  = ;\\n\\n\\n\"Transformer\":\\n\\n\\n\" transformer\" is a term used to describe the transformer. transformer is the transformer that is used to refer to the transformer.: \" transformer.\\n\\n\\n*Transformer:\\n\\n\\n\\\\* Transmitter:\\n\\n\\n\\n*Transmitter:\\n\\n- Transmitter\\n\\n---\\n\\n--\\n\\n\\\\- transformer\\n transformer is a transmitter.\\n\\n\\n\\n -?. I\\'m not an expert in this field, but I\\'m an expert.\\n\\n  &  :! Transmitter. is an expert,s (_URL_\\n\\n\\n\"Transmitter\"\"transformer\"\"transmitter\"\\n\" Transmitter\"\\n\\n(_URL_1_)\\rut(_URL)\\n\\n\\r\\n(_)'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Abstract from \"Attention is all you need\" by Vaswani et al.: https://arxiv.org/abs/1706.03762\n",
        "abstract = \"\"\"The dominant sequence transduction models are based on complex recurrent or\n",
        "convolutional neural networks that include an encoder and a decoder. The best\n",
        "performing models also connect the encoder and decoder through an attention\n",
        "mechanism. We propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
        "entirely. Experiments on two machine translation tasks show these models to\n",
        "be superior in quality while being more parallelizable and requiring significantly\n",
        "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...\n",
        "\"\"\"\n",
        "question = \"What's a transformer'?\"\n",
        "input_text = f\"question: {question} context: {abstract}\"\n",
        "\n",
        "tuned_pipeline(input_text)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca64b9fc",
      "metadata": {},
      "source": [
        "Remember, the base model above simply repeated the input (its answer was \"\"question: What's a transformer? In this context: The dominant sequence transduction models are based on complex recurrent...\").\n",
        "In comparison, the answer of the fine-tuned model is a lot better.\n",
        "The first sentence is a sensible answer to the question: \"A transformer is a type of neural network architecture.\"\n",
        "However, after that, the model starts rambling and the answer quality gets worse.\n",
        "\n",
        "I am not sure why a) the answer is so long and b) why it is not better than that. \n",
        "Since the training and validation loss were still going down, more training should further increase the answer quality.\n",
        "\n",
        "If you have other suggestions for improving the answer quality, I am happy to hear your suggestions (contact info is on [my website](https://stefanbschneider.github.io/))!\n",
        "\n",
        "\n",
        "\n",
        "# What's Next?\n",
        "\n",
        "Try it yourself:\n",
        "\n",
        "- Load and use my fine-tuned model directly: [stefanbschneider/led-base-16384-lfqa-ans-len-512](https://huggingface.co/stefanbschneider/led-base-16384-lfqa-ans-len-512)\n",
        "- Or load the dataset ([stefanbschneider/lfqa-max-answer-length-512](https://huggingface.co/datasets/stefanbschneider/lfqa-max-answer-length-512)) and follow the script for fine-tuning it yourself: [`led-finetune-lfqa-train.py` ](https://huggingface.co/stefanbschneider/led-base-16384-lfqa-ans-len-512/blob/main/led-finetune-lfqa-train.py)\n",
        "- All scripts are also on my [GitHub](https://github.com/stefanbschneider/llm-research-buddy)\n",
        "- [Rent a GPU at Vast.ai*](https://cloud.vast.ai/?ref_id=202191)\n",
        "\n",
        "Related blog posts:\n",
        "\n",
        "- [Generative Document Question Answering with HuggingFace](https://stefanbschneider.github.io/blog/posts/generative-qa/)\n",
        "- [Building a Simple Q&A App with HuggingFace and Gradio](https://stefanbschneider.github.io/blog/posts/question-answering-huggingface/)\n",
        "\n",
        "Additional Resources:\n",
        "\n",
        "- [HuggingFace Fine-Tuning Tutorial](https://huggingface.co/docs/transformers/en/training)\n",
        "- [Keras Transfer Learning and Fine-Tuning Guide](https://keras.io/guides/transfer_learning/)\n",
        "\n",
        "\n",
        "*: Links to Vast.ai are affiliate links."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
