{
  "cells": [
    {
      "cell_type": "raw",
      "id": "3c4a832b",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "draft: false\n",
        "categories:\n",
        "- transformer\n",
        "- machine learning\n",
        "- deployment\n",
        "- HuggingFace\n",
        "date: '2024-02-09'\n",
        "date-modified: '2025-01-04'\n",
        "title: Building a Simple Q&A App with HuggingFace and Gradio\n",
        "description: Using pre-trained LLMs with HuggingFace and Gradio to build and deploy a simple question answering app in few lines of Python code.\n",
        "image: images/gradio.webp\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294d36c7",
      "metadata": {},
      "source": [
        "Large language models (LLMs) like GPT, BART, etc. have demonstrated incredible abilities in natural language.\n",
        "\n",
        "This blog post describes how you can use LLMs to build and deploy your own app in just a few lines of Python code with the [HuggingFace](https://huggingface.co/) ecosystem.\n",
        "HuggingFace provides pre-trained models, datasets, and other tools that are handy when working with machine learning models without having to understand all the underlying theory.\n",
        "If you are interested in how LLMs work, see [my other blog post on the underlying transformer architecture](https://stefanbschneider.github.io/blog/posts/understanding-transformers-attention/).\n",
        "\n",
        "As an example, the goal of this post is to build an app that answers questions about a given PDF document.\n",
        "The focus is on showing a simple proof of concept rather than high-quality answers.\n",
        "\n",
        "First, let's install the necessary dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c066d39f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "pip install -U pypdf torch transformers gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a4089c9",
      "metadata": {},
      "source": [
        "# Question Answering with HuggingFace\n",
        "\n",
        "We can read the text of PDF document with `pypdf`. As an example, I'm using the author version of a [paper](https://ieeexplore.ieee.org/document/9789886) I wrote on [`mobile-env`](https://github.com/stefanbschneider/mobile-env). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4828c301",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mobile-env: An Open Platform for Reinforcement\\nLearning in Wireless Mobile Networks\\nStefan Schneider, Stefan Werner\\nPaderborn University, Germany\\n{stschn, stwerner}@mail.upb.de\\nRamin Khalili, Artur Hecker\\nHuawei Technologies, Germany\\n{ramin.khalili, artur.hecker}@huawei.com\\nHolger Karl\\nHasso Plattner Institute,\\nUniversity of Potsdam, Germany\\nholger.karl@hpi.de\\nAbstract—Recent reinforcement learning approaches for con-\\ntinuous control in wireless mobile networks have shown im-\\npressive results. But due to the lack of open and compatible\\nsimulators, authors typically create their own simulation en-\\nvironments for training and evaluation. This is cumbersome\\nand time-consuming for authors and limits reproducibility and\\ncomparability, ultimately impeding progress in the ﬁeld.\\nTo this end, we proposemobile-env, a simple and open platform\\nfor training, evaluating, and comparing reinforcement learning\\nand conventional approaches for continuous control in mobile\\nwireless networks. mobile-env is lightweight and implements\\nthe common OpenAI Gym interface and additional wrappers,\\nwhich allows connecting virtually any single-agent or multi-agent\\nreinforcement learning framework to the environment. While\\nmobile-env provides sensible default values and can be used out\\nof the box, it also has many conﬁguration options and is easy to\\nextend. We therefore believe mobile-env to be a valuable platform\\nfor driving meaningful progress in autonomous coordination of\\nwireless mobile networks.\\nIndex T'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from typing import Union\n",
        "from pypdf import PdfReader\n",
        "\n",
        "\n",
        "def get_text_from_pdf(pdf_file: Union[str, Path]) -> str:\n",
        "    \"\"\"Read the PDF from the given path and return a string with its entire content.\"\"\"\n",
        "    reader = PdfReader(pdf_file)\n",
        "\n",
        "    # Extract text from all pages\n",
        "    full_text = \"\"\n",
        "    for page in reader.pages:\n",
        "        full_text += page.extract_text()\n",
        "    return full_text\n",
        "\n",
        "# Read and print parts of the PDF\n",
        "pdf_text = get_text_from_pdf(\"mobileenv_author_version.pdf\")\n",
        "pdf_text[:1500]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8662ed07",
      "metadata": {},
      "source": [
        "Now we can create a question answering pipeline using HuggingFace, loading a pre-trained model. Then we can ask some questions, providing the PDF text as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2325a098",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(task=\"question-answering\", model=\"deepset/tinyroberta-squad2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "871b3b95",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.9887111186981201,\n",
              " 'start': 16488,\n",
              " 'end': 16505,\n",
              " 'answer': 'GitHub repository'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_answerer(\"What is mobile-env?\", pdf_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "89f9bbc6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.9665615558624268, 'start': 3552, 'end': 3558, 'answer': 'Python'}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_answerer(\"What programming language is mobile-env written in?\", pdf_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ced00482",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.6506955027580261,\n",
              " 'start': 12539,\n",
              " 'end': 12570,\n",
              " 'answer': 'more ﬂexible, better documented'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_answerer(\"What is the main difference between mobile-env and other simulators?\", pdf_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0df359f1",
      "metadata": {},
      "source": [
        "The pipeline returns a dict, where the answer is a quote from the given context, here the PDF document. This is called *extractive* question answering.\n",
        "\n",
        "It also provides a score indicating the model's confindence in the answer and the start/end index from where the answer is quoted. \n",
        "\n",
        "That's it! Let's see how we can build a simple app on top of this.\n",
        "\n",
        "# Building an App with Gradio\n",
        "\n",
        "[Gradio](https://www.gradio.app/) allows building simple apps tailored for machine learning use cases.\n",
        "You can define the inputs, a function to where to pass these inputs, and how to display the functions outputs.\n",
        "\n",
        "Here, our inputs are the PDF document and the question.\n",
        "The function loads the document and passes the question and text to the pre-trained model.\n",
        "It then outputs the models answer to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f5a8daee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7862\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def answer_doc_question(pdf_file, question):\n",
        "    pdf_text = get_text_from_pdf(pdf_file)\n",
        "    answer = question_answerer(question, pdf_text)\n",
        "    return answer[\"answer\"]\n",
        "\n",
        "# Add default a file and question, so it's easy to try out the app.\n",
        "pdf_input = gr.File(\n",
        "    value=\"https://ris.uni-paderborn.de/download/30236/30237/author_version.pdf\",\n",
        "    file_types=[\".pdf\"],\n",
        "    label=\"Upload a PDF document and ask a question about it.\",\n",
        ")\n",
        "question = gr.Textbox(\n",
        "    value=\"What is mobile-env?\",\n",
        "    label=\"Type a question regarding the uploaded document here.\",\n",
        ")\n",
        "gr.Interface(\n",
        "    fn=answer_doc_question, inputs=[pdf_input, question], outputs=\"text\"\n",
        ").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a903f86",
      "metadata": {},
      "source": [
        "If you run this locally, you should see a rendered app based on the question answering pipeline we built above!\n",
        "\n",
        "# Deploying the app in HuggingFace Spaces\n",
        "\n",
        "You can easily host the app on [HuggingFace Spaces](https://huggingface.co/spaces), which provide free (and slow) hosting (or fast paid hosting).\n",
        "\n",
        "You simply create a new space under your account and add an `app.py`, which contains all code above. The requirements go into a `requirements.txt`. That's it!\n",
        "\n",
        "This is the app we built here: [https://huggingface.co/spaces/stefanbschneider/pdf-question-answering](https://huggingface.co/spaces/stefanbschneider/pdf-question-answering)\n",
        "\n",
        "<script\n",
        "\ttype=\"module\"\n",
        "\tsrc=\"https://gradio.s3-us-west-2.amazonaws.com/4.17.0/gradio.js\"\n",
        "></script>\n",
        "\n",
        "<gradio-app src=\"https://stefanbschneider-pdf-question-answering.hf.space\"></gradio-app>\n",
        "\n",
        "\n",
        "# What's Next?\n",
        "\n",
        "* [Read about the underlying transformer architecture powering most LLMs](https://stefanbschneider.github.io/blog/posts/understanding-transformers-attention/)\n",
        "* Improve the quality of the question answering app. Some ideas:\n",
        "    * Fine-tune the pre-trained model on a domain dataset, eg, [Arxiv Q&A](https://huggingface.co/datasets/taesiri/arxiv_qa)\n",
        "    * [Domain adaptation by fine-tuning a masked model directly on the document](https://huggingface.co/learn/nlp-course/en/chapter7/3)\n",
        "    * Using the [document-question-answering pipeline on HuggingFace](https://huggingface.co/tasks/document-question-answering)\n",
        "    * Trying a model that supports *generative* question answering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f513d0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "blog",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
