[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "What I Learned or Want to Learn",
    "section": "",
    "text": "Fine-Tuning a Pre-Trained LLM\n\n\n\n\n\n\n\ntransformer\n\n\nllm\n\n\nmachine learning\n\n\nHuggingFace\n\n\nVast.ai\n\n\n\n\nFine-tuning a pre-trained language model on a custom dataset for long-form question answering using HuggingFace.\n\n\n\n\n\n\nFeb 9, 2025\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nGenerative Document Question Answering with HuggingFace\n\n\n\n\n\n\n\ntransformer\n\n\nllm\n\n\nmachine learning\n\n\nHuggingFace\n\n\n\n\nGenerating answers about a given document/article using pre-trained models on HuggingFace.\n\n\n\n\n\n\nJan 11, 2025\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Simple Q&A App with HuggingFace and Gradio\n\n\n\n\n\n\n\ntransformer\n\n\nmachine learning\n\n\ndeployment\n\n\nHuggingFace\n\n\n\n\nUsing pre-trained LLMs with HuggingFace and Gradio to build and deploy a simple question answering app in few lines of Python code.\n\n\n\n\n\n\nFeb 9, 2024\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Transformers and Attention\n\n\n\n\n\n\n\nresearch\n\n\ntransformer\n\n\nattention\n\n\nmachine learning\n\n\n\n\nMy notes for understanding the attention mechanism and transformer architecture used by GPT-4, BERT, and other LLMs.\n\n\n\n\n\n\nNov 28, 2023\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nTracking Link Clicks With Google Analytics\n\n\n\n\n\n\n\ngoogle analytics\n\n\n\n\nMonitor clicks to specific links on your website (outgoing, affiliate, ads, etc.) using Google Analytics GA4.\n\n\n\n\n\n\nJan 16, 2022\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nDealing with Partial Observability In Reinforcement Learning\n\n\n\n\n\n\n\npython\n\n\nray\n\n\ntensorflow\n\n\nmachine learning\n\n\nreinforcement learning\n\n\nattention\n\n\n\n\nUsing RLlib, frame stacking, and self-attention to master environments with partial observations.\n\n\n\n\n\n\nNov 29, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nEasily editing videos with Descript\n\n\n\n\n\n\n\ntools\n\n\nresearch\n\n\n\n\nAutomatically removing “uhms” and editing videos with text-to-speech.\n\n\n\n\n\n\nNov 2, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nCell Selection with Deep Reinforcement Learning\n\n\n\n\n\n\n\nresearch\n\n\nwireless\n\n\nreinforcement learning\n\n\nray\n\n\npython\n\n\ntensorflow\n\n\n\n\nSelf-adaptive and self-learning multi-cell selection for 5G and beyond with deep reinforcement learning.\n\n\n\n\n\n\nMay 2, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nUsing PyTorch Inside a Django App\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\npytorch\n\n\ndjango\n\n\ndeployment\n\n\n\n\nBuilding a simple image classification web app using Django and a pretrained PyTorch DenseNet model.\n\n\n\n\n\n\nApr 11, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nScaling Deep Reinforcement Learning to a Private Cluster\n\n\n\n\n\n\n\npython\n\n\nray\n\n\nmachine learning\n\n\nreinforcement learning\n\n\ndeployment\n\n\n\n\nUsing Ray RLlib to train a deep reinforcement learning agent (PPO) in a custom environment on a private cluster.\n\n\n\n\n\n\nFeb 15, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nUsing Bootstrap to Style a Django App\n\n\n\n\n\n\n\npython\n\n\ndjango\n\n\nbootstrap\n\n\n\n\nUsing the crispy tag and Bootstrap 5 for nicer styling of Django apps.\n\n\n\n\n\n\nFeb 3, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nLessons Learned from Leading My First Project\n\n\n\n\n\n\n\nleadership\n\n\n\n\nSeven lessons learned from leading my first project, RealVNF.\n\n\n\n\n\n\nJan 31, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nAdding a Database to a Django App\n\n\n\n\n\n\n\ndjango\n\n\ndeployment\n\n\ndatabase\n\n\ngithub\n\n\n\n\nPersistent storage using a SQL-based database for Django (development and deployment on Heroku).\n\n\n\n\n\n\nJan 29, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nAdding Google Analytics to a Django App\n\n\n\n\n\n\n\npython\n\n\ndjango\n\n\ngoogle analytics\n\n\n\n\nUsing the new Google Analytics GA4 for monitoring and analyzing user traffic on a Django app.\n\n\n\n\n\n\nJan 21, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Django App and Deploying It on Heroku\n\n\n\n\n\n\n\npython\n\n\ndjango\n\n\ndeployment\n\n\ngithub\n\n\n\n\nBuilding a ‘hello world’ web app using Django and deploying it on Heroku.\n\n\n\n\n\n\nJan 19, 2021\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nGetting Started with PyTorch\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\npytorch\n\n\n\n\nSimple vision and image classification (CIFAR10) with CNNs using PyTorch.\n\n\n\n\n\n\nDec 30, 2020\n\n\nStefan Schneider\n\n\n\n\n\n\n  \n\n\n\n\nFastpages Notebook Blog Post\n\n\n\n\n\n\n\nblogging\n\n\n\n\nA tutorial of fastpages for Jupyter notebooks.\n\n\n\n\n\n\nFeb 20, 2020\n\n\nfastpages\n\n\n\n\n\n\n  \n\n\n\n\nAn Example Markdown Post\n\n\n\n\n\n\n\nblogging\n\n\n\n\nA minimal example of using markdown with fastpages/Quarto.\n\n\n\n\n\n\nJan 14, 2020\n\n\nfastpages & quarto\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/django-db/index.html",
    "href": "posts/django-db/index.html",
    "title": "Adding a Database to a Django App",
    "section": "",
    "text": "Most Django apps store and work with data, e.g., entered by users. Django supports multiple relational databases for persistent storage of such data out of the box. In this blog post, I show how to a database to a Django app. For local development, I use the default SQLite database and for production deployment on Heroku, I use PostgreSQL® 1.\nI use the Django “Hello World” App as simple example. I add a button that users can click to say “Hello World!” and count how often it was clicked. The number of clicks is stored in a database and displayed in the app.\nThe whole process is very simple as Django does most of the work in the background: Creating the database, making the queries, etc."
  },
  {
    "objectID": "posts/django-db/index.html#using-the-model-in-the-django-app",
    "href": "posts/django-db/index.html#using-the-model-in-the-django-app",
    "title": "Adding a Database to a Django App",
    "section": "Using the Model in the Django App",
    "text": "Using the Model in the Django App\nTo integrate the model into the “Hello World” app, I first create a view that handles the interaction with the model. In helloworld/views.py:\nfrom django.shortcuts import render\n\nfrom .models import Counter\n\n\ndef index(request):\n    # retriever counter model instance from DB or create it if it doesn't exist yet\n    counter, created = Counter.objects.get_or_create(name='hello-world-button')\n\n    # increment counter when a POST request arrives (from the button click)\n    if request.method == 'POST':\n        counter.value += 1\n        counter.save()\n\n    context = {\n        'clicks': counter.value,\n    }\n    return render(request, 'helloworld/index.html', context)\nThis view replaces the previous TemplateView in the app’s URL settings (helloworld/urls.py):\nfrom django.urls import path\n\nfrom . import views\n\n\napp_name = 'helloworld'\n\nurlpatterns = [\n    # path('', TemplateView.as_view(template_name='helloworld/index.html'), name='index'),\n    path('', views.index, name='index'),\n]\nFinally, I need to adjust the template to add a button and display the number of times it was clicked (helloworld/templates/helloworld/index.html):\nHello World! ({{ clicks }}x)\n\n&lt;form action=\"{% url 'helloworld:index' %}\" method=\"post\"&gt;\n    {% csrf_token %}\n    &lt;button type=\"submit\" name=\"hello-world-button\"&gt;Greet the world!&lt;/button&gt;\n&lt;/form&gt;\nWhen running the app, it should now look like this:\n\nClicking the button should reload the page and show an increased click count (here, “2x”). The click count should persist even when the app server is restarted or the app is updated.\nOf course, this is super slow and just an example for illustrating persistent storage in Django. In production, this should happen asynchronously with some frontend framework."
  },
  {
    "objectID": "posts/django-db/index.html#footnotes",
    "href": "posts/django-db/index.html#footnotes",
    "title": "Adding a Database to a Django App",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPostgres, PostgreSQL and the Slonik Logo are trademarks or registered trademarks of the PostgreSQL Community Association of Canada, and used with their permission.↩︎"
  },
  {
    "objectID": "posts/understanding-transformers-attention/index.html",
    "href": "posts/understanding-transformers-attention/index.html",
    "title": "Understanding Transformers and Attention",
    "section": "",
    "text": "The famous paper “Attention is all you need” by Vaswani et al. presents the transformer architecture. This neural network architecture builds on the attention mechanism and is at the core of current large language models (LLMs) such as ChatGPT, GPT-4, BERT, etc.\nHere are some notes to help understand the transformer neural network architecture and the underlying attention mechanism."
  },
  {
    "objectID": "posts/understanding-transformers-attention/index.html#encoder",
    "href": "posts/understanding-transformers-attention/index.html#encoder",
    "title": "Understanding Transformers and Attention",
    "section": "Encoder",
    "text": "Encoder\nThe encoder (left side of the figure above), processes the tokens of the input sequence. Each token is initially embedded into a d- dimensional vector, e.g., using word2vec. It is then processed individually and compared to other tokens in the input. The process works with variable-length sequences.\nAfter embedding the words as vectors, a positional encoding is added, which indicates the absolute and relative position of the tokens in the sequence. This is necessary for many tasks, particularly in natural language, since the later parts of the encoder are permutation invariant. This means they do not distinguish the position of the tokens in the sequence. However, this position may convey important information. For example, in “The cat watches the dog run fast.”, if the position of “cat” and “dog” were interchanged, it would alter subject and object and thus the meaning of the sentence. In the paper, the authors feed the word position in the sequence into a sine and cosine function and add the resulting value to the input vector.\nAfter positional encoding, follows a self-attention block. During self-attention, the tokens within the input sequence are compared to each other, computing attention weights. A high attention weight for a pair of two tokens indicates that they are syntactically or semantically related. Thus high weights indicate important tokens that the model should “attend” to. In the example sentence “The cat watches the dog run fast.”, “run” and “fast” could be two tokens where we would expect high attention weights since they are semantically related. In contrast, “the” and “fast” should lead to low attention weights. More details on attention follow in the sections below.\nFinally, a normal feed-forward neural network follows the attention block and outputs the encoded tokens one by one. This encoder block including both attention and the feed-forward network can be repeated multiple times. Finally, the encoded tokens are then fed into the decoder.\nNote that both the output of the attention block and the feed-forward block are added to their original inputs and normalized using residual connections. These residual connections help back-propagating gradient updates, which might otherwise get lost due to diminishing gradients."
  },
  {
    "objectID": "posts/understanding-transformers-attention/index.html#decoder",
    "href": "posts/understanding-transformers-attention/index.html#decoder",
    "title": "Understanding Transformers and Attention",
    "section": "Decoder",
    "text": "Decoder\nThe decoder (right part of the architecture above) is quite similar to the encoder but operates on the output sequence. It takes both the encoded input tokens and the output tokens produced so far as input. Using both, the decoder outputs new tokens, e.g., a translation of the input sequence into another language or a summary of the input text.\nSimilar to the encoder, the tokens of the output sequence are also embedded into \\(d\\)-dimensional vectors and added with a positional encoding regarding the position in the output sequence. As in the encoder, they are then processed using self-attention, comparing the tokens of the output sequence with each other.\nIn contrast to the encoder, the decoder contains a cross-attention block, which compares to the encoded tokens of the input sequence using cross-attention (explained in more detail below). This cross-attention is at the heart of the decoder and helps producing the next output token, taking both the full input and the output produced so far into account.\nSimilar to the encoder, the decoder also has a normal, fully-connected feed-forward neural network following the attention blocks. The decoder block can also be stacked multiple times. Finally, a linear layer and softmax activation output probabilities for all possible tokens. The token with highest probability is then selected as next token. Producing the output sequence continues until a special end-of-sequence (EOS) token is selected. This allows producing variable-length output sequences independent of the length of the input sequence."
  },
  {
    "objectID": "posts/understanding-transformers-attention/index.html#self-attention-cross-attention",
    "href": "posts/understanding-transformers-attention/index.html#self-attention-cross-attention",
    "title": "Understanding Transformers and Attention",
    "section": "Self-Attention & Cross-Attention",
    "text": "Self-Attention & Cross-Attention\nSelf- and cross-attention only differ in which tokens are given as inputs Q, K, and V. In self-attention, the same tokens are used for all three inputs, i.e., each token is used for Q, K, and V. In cross-attention, the same tokens are given to K and V, but different tokens are used for Q.\nSelf-attention is applied to the input sequence in the encoder and to the output sequence in the decoder. The decoder’s cross-attention module then uses the encoded input tokens as K and V and the produced output tokens as Q.\nIn either case, attention weights between tokens in Q and in K are calculated using scaled dot-product attention. While there are other forms of attention such as learned attention weights, scaled dot-product attention is faster and more space efficient. In scaled dot-product attention, attention weights are computed without extra parameters as illustrated in the figure below: Tokens in Q and K are multiplied, scaled, and passed through a softmax function to obtain attention weights. The attention weights are then multiplied with the tokens in V to select values with high corresponding weights.\n\n\n\nIllustration of scaled dot-product attention from “Attention is all you need”."
  },
  {
    "objectID": "posts/understanding-transformers-attention/index.html#example-computing-attention-weights",
    "href": "posts/understanding-transformers-attention/index.html#example-computing-attention-weights",
    "title": "Understanding Transformers and Attention",
    "section": "Example: Computing Attention Weights",
    "text": "Example: Computing Attention Weights\nConsider two tokens “cat” and “dog”, where “cat” is used as query Q and “dog” as key K and value V. Attention is computed as follows:\n\nThe embedded Q and K tokens are multiplied using the dot product. Let’s assume “cat” is embedded as vector \\((0.6, 0.3, 0.8)\\) and “dog” as vector \\((0.5, 0.1, 0.9)\\), i.e., we have \\(d=3\\) dimensional embedding vectors. In practice, embedding vectors are much longer (larger \\(d\\)) and could be computed using techniques such as word2vec. The dot product of the two vectors here is \\(0.6*0.5 + 0.3*0.1 + 0.8*0.9 = 1.05\\). Note that the product is larger for more similar vectors, which are closer in the embedding space.\nThe result of the multiplication is scaled by dividing by \\(\\sqrt(d)\\). In this example: \\(1.05 / \\sqrt(3) = 0.606\\). If d is large, the dot product may grow very large. In this case, scaling helps reducing the magnitude and ensuring suitable inputs to the softmax function.\nNext, the scaled product is fed into a softmax function, which outputs the attention weights. The returned attention weights of all pairs of tokens are between 0 and 1 and sum up to 1.  In our example, assume we just have two other token pairs with scaled products of \\(0.2\\) and \\(-0.8\\) in addition to the \\(0.606\\). In this case, \\(softmax([0.606, 0.2, -0.8]) = [0.523, 0.348, 0.128])\\), i.e., we have the highest attention weights for our two tokens “cat” and “dog” and smaller weights for the other two pairs. We can also mask (=ignore) certain positions by setting them to negative infinity, where \\(softmax(-∞)=0\\). This is used in the decoder during training.\nThe computed attention weights are then multiplied with the values V. In our example, “dog” is used as value and its embedding vector \\((0.5, 0.1, 0.9)\\) is multiplied with attention weight \\(0.523\\), resulting in \\((0.262, 0.052, 0.471)\\). Other token pairs where Q and K are less compatible may have lower attention weights closer to zero. Consequently, their values V are also reduced to vectors close to zero. The model attends less to these values than to others with high attention scores."
  },
  {
    "objectID": "posts/understanding-transformers-attention/index.html#multi-head-attention",
    "href": "posts/understanding-transformers-attention/index.html#multi-head-attention",
    "title": "Understanding Transformers and Attention",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\n\n\n\nIllustration of multi-head attention from “Attention is all you need”.\n\n\nMulti-head attention repeats the attention mechanism explained above multiple times (\\(h\\) times) in parallel. Before passing the input tokens of dimension \\(d\\) into these h attention blocks, they projected into smaller embeddings of size \\(\\frac{d}{h}\\) by using small linear neural networks. Each of the h linear networks has different weights and leads to different projections. Consequently, each attention “head” can learn to focus on different aspects, e.g., subject and object of a sentence. The outputs of the h heads are then concatenated and passed through another linear neural network. Multi-head attention is used both in the self-attention blocks as well as the cross-attention block. Note that increasing h allows the network to learn different kinds of attention, e.g., one head for the subject, one for the object, one for the verb of a sentence. h is not directly related to the length of the sequence.\n\nOverall, attention is a surprisingly simple yet effective mechanism to emphasize meaningful tokens and ignore less meaningful tokens. Based on this mechanism, transformers can take capture the context even in long sequences and produce coherent outputs."
  },
  {
    "objectID": "posts/llm-fine-tuning/index.html",
    "href": "posts/llm-fine-tuning/index.html",
    "title": "Fine-Tuning a Pre-Trained LLM",
    "section": "",
    "text": "Large language models (LLMs) can perform all kinds of tasks ranging from translation over summarization to text generation or even multi-modal tasks involving sound, images, or videos. Usually, there are LLMs readily available for any kind of task, which can be easily found on HuggingFace. However, if there is no available model doing just what you want, then fine-tuning is the way to go. During fine-tuning, a pre-trained base or foundation model is further trained on a comparably small, task-specific dataset. Fine-tuning is much faster and cheaper than pre-training a new model from scratch.\nIn my case, I was looking for a model to answer questions about long documents in natural language. Most models I could find were limited to short context lengths, i.e., could not handle entire documents as input, or were not trained to output generated natural answers (see my post here).\nHence, in this blog post, I fine-tune a pre-trained Longformer Encoder-Decoder (LED) base model for generative question answering."
  },
  {
    "objectID": "posts/llm-fine-tuning/index.html#finding-a-suitable-dataset",
    "href": "posts/llm-fine-tuning/index.html#finding-a-suitable-dataset",
    "title": "Fine-Tuning a Pre-Trained LLM",
    "section": "Finding a Suitable Dataset",
    "text": "Finding a Suitable Dataset\nMy task of interest is answering questions in natural language given a (potentially long) context. Since I do not have the means of collecting and creating my own dataset, I was looking for a suitable dataset online.\nThe well-known SQuAD dataset is only suitable for extractive question answering, where the answer is a span text from the provided context. The DuoRC dataset with questions and answers about a given movie plot can be used for both extractive and generative/abstracitve Q&A. However, I found the answers to be overly short, often just a few words, and not always very natural.\nFinally, I found a suitable dataset for long-form question answering (LFQA) with natural, multi-sentence answers to questions based on provided contexts (details on this dataset). The dataset is a successor of facebook’s ELI5 dataset (explain like I’m five), which is no longer available. Details are in this blog post by the dataset’s authors."
  },
  {
    "objectID": "posts/llm-fine-tuning/index.html#analyzing-and-adjusting-the-dataset",
    "href": "posts/llm-fine-tuning/index.html#analyzing-and-adjusting-the-dataset",
    "title": "Fine-Tuning a Pre-Trained LLM",
    "section": "Analyzing and Adjusting the Dataset",
    "text": "Analyzing and Adjusting the Dataset\nTo get familiar with the dataset and understand what kind of inputs and outputs the fine-tuned model has to handle, I visualized the length of contexts (model input, together with the questions) as well as the length of the expected answers (model output) in the dataset. Since I am interested in the length in terms of number of tokens (relevant for fine-tuning later), I first tokenized the contexts and answers with the LED base model’s tokenizer.\nThe context lengths are quite normally distributed with a few quite long contexts, but most rather short:\n\n\n\nDistribution of context lengths (in number of tokens).\n\n\nSince I am interested in long contexts, it is fine to have a few contexts that are much longer than the others.\nThe answer lengths have an even stronger long-tail distribution with some few answers that were overly long (up to ~6000 tokens, even longer than the context!).\n\n\n\nDistribution of answer lengths in the original dataset (in number of tokens).\n\n\nSince I did not want my fine-tuned model to create overly long answers, I filtered these examples out of the dataset and made my own version of the dataset with answers only up 512 tokens. This means the maximum answer length is roughly 12x shorter at the cost of 10% less training data.\nMy filtered dataset is available here: stefanbschneider/lfqa-max-answer-length-512 The notebook I used for creating the filtered dataset as well as the plots is also in the repository: process-lfqa-dataset.ipynb\nAn example in the dataset looks like this:\n{\n    \"question\": \"what's the difference between a forest and a wood?\",\n    \"answer\": \"They're used interchangeably a lot. You'll get different answers from different resources, but the ...\",\n    \"context\": [\n        \"Wood is divided, according to its botanical origin, into two kinds: softwoods, ...\",\n        \"Processing and products differs especially with regard to the distinction between softwood and hardwood ...\"\n    ]\n}"
  },
  {
    "objectID": "posts/llm-fine-tuning/index.html#preparing-the-data",
    "href": "posts/llm-fine-tuning/index.html#preparing-the-data",
    "title": "Fine-Tuning a Pre-Trained LLM",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nNext, I create two functions for processing the data for training and validation. These functions prepare the data in batches of size 2 (larger batches do not fit onto my GPU). I found that batch size 1 did not work at all; the loss quickly dropped to zero and the model stopped learning.\nSince each question is paired with a list of multiple contexts, these contexts are concatenated to the corresponding question into one single string, which is given as input to the model. Note that the expected output length is also set to 512 tokens here.\n\nBATCH_SIZE: int = 2\n\ndef process_data_to_model_inputs(batch):\n    # combine context strings and questions to one input\n    input = [\n        f\"question: {question}, context: {' '.join(context)}\"\n        for question, context in zip(batch[\"question\"], batch[\"context\"])\n    ]\n\n    # tokenize the inputs and labels\n    inputs = tokenizer(\n        input,\n        padding=\"max_length\",\n        truncation=True,\n        # Max supported article/context length + question.\n        max_length=8192,\n    )\n    outputs = tokenizer(\n        batch[\"answer\"],\n        padding=\"max_length\",\n        truncation=True,\n        # Since I limit the answers to 512 tokens in the dataset, I can also limit the max_length here\n        max_length=512,\n    )\n\n    # The following settings are copied from the fine-tuning notebook provided by AllenAI:\n    # https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing\n    batch[\"input_ids\"] = inputs.input_ids\n    batch[\"attention_mask\"] = inputs.attention_mask\n\n    # create 0 global_attention_mask lists\n    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n    ]\n\n    # since above lists are references, the following line changes the 0 index for all samples\n    batch[\"global_attention_mask\"][0][0] = 1\n    batch[\"labels\"] = outputs.input_ids\n\n    # We have to make sure that the PAD token is ignored\n    batch[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n        for labels in batch[\"labels\"]\n    ]\n\n    return batch\n\n\ndef load_and_process_dataset(split: str, dataset_limit: Optional[int] = None):\n    \"\"\"Load and process the dataset for training or validation. Optionally limit the number of samples.\"\"\"\n    dataset = load_dataset(\"stefanbschneider/lfqa-max-answer-length-512\", split=split)\n\n    # optionally reduce the data sets to a small fraction\n    if dataset_limit is not None:\n        dataset = dataset.select(range(dataset_limit))\n\n    # Process the dataset with the function above. Afterwards, remove the original columns.\n    dataset = dataset.map(\n        process_data_to_model_inputs,\n        batched=True,\n        batch_size=BATCH_SIZE,\n        remove_columns=[\"context\", \"question\", \"answer\"],\n    )\n\n    # Format the dataset to torch\n    dataset.set_format(\n        type=\"torch\",\n        columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n    )\n\n    return dataset\n\nFor development and experimentation, it is useful to only load a small fraction of the dataset, using the dataset_limit argument I introduced above:\n\n# Load and process datasets; limit to small size for experimentation\ntrain_data = load_and_process_dataset(\"train\", dataset_limit=128)\nval_data = load_and_process_dataset(\"validation\", dataset_limit=16)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)"
  },
  {
    "objectID": "posts/llm-fine-tuning/index.html#configuring-the-model-and-training",
    "href": "posts/llm-fine-tuning/index.html#configuring-the-model-and-training",
    "title": "Fine-Tuning a Pre-Trained LLM",
    "section": "Configuring the Model and Training",
    "text": "Configuring the Model and Training\nWith the data loaded and ready for training, both the model and the training have to be configured. The generation config defines how the model generates new answers. Here, I set answers to be 100 to 512 tokens long.\n\n# Create and set the model's generation config\ngeneration_config = GenerationConfig(\n    # The generated answer should be 100-512 tokens long\n    max_length=512,\n    min_length=100,\n    early_stopping=True,\n    num_beams=4,\n    length_penalty=2.0,\n    # Don't repeat n=3-grams (same words in same order) in the generated text --&gt; more natural\n    no_repeat_ngram_size=3,\n    decoder_start_token_id=tokenizer.cls_token_id,\n    bos_token_id=tokenizer.bos_token_id,\n)\nmodel.generation_config = generation_config\n\nThe training arguments control the training procedure as well as how (often) the model is saved, evaluated, and how training is monitored.\n\n# Set training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    eval_strategy=\"steps\",\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    # fp16 only works on GPU, not on M1 mps. mps is used by default if it's available\n    #fp16=True,\n    output_dir=\"models\",\n    logging_steps=10,    # for proper training: 100,\n    eval_steps=1000,\n    #save_steps=500,\n    save_total_limit=1,\n    gradient_accumulation_steps=1,\n    num_train_epochs=1,\n    # Save to HF hub & log to wandb\n    #push_to_hub=True,\n    #hub_model_id=\"stefanbschneider/led-base-16384-lfqa-ans-len-512\",\n    log_level=\"info\",\n    report_to=\"wandb\",\n    run_name=\"test-run\",\n)\n\nI use batch size 2 (set above) and save the model locally to “models”.\nFor logging, I use Weights & Biases, which is also the default. For that to work, you need a free W&B account. logging_steps determines the frequency (in number of optimization steps) in which the training results are logged locally and to W&B. Similarly, eval_steps controls how often the evaluation is run on the val_data prepared above. During training, a model checkpoint is saved locally every save_steps, maintaining at most save_total_limit checkpoints locally (then overwriting old checkpoints). I do not use gradient accumulation, which can mimic higher batch sizes even with less memory. Finally, num_train_epochs sets the number of training epochs, i.e., how often training is repeated on the training set.\nThese paramters together with the size of the training set (above limited to 128 for debugging) determine the overall number of optimization steps during training:\n\\[\\text{num\\_steps} = \\frac{\\text{num\\_examples\\_in\\_data}}{\\text{batch\\_size} * \\text{gradient\\_accumulation}} * \\text{epochs}\\]\nIf push_to_hub=True, new checkpoints are automatically pushed to the HuggingFace hub. This is definitely recommended when running the full fine-tuning, so that the model is safely stored online and can easily be used later on. This needs a HuggingFace account. Once the account is created, log in locally with the CLI: huggingface-cli login and paste an access token to connect with your account."
  },
  {
    "objectID": "posts/llm-fine-tuning/index.html#training-and-monitoring",
    "href": "posts/llm-fine-tuning/index.html#training-and-monitoring",
    "title": "Fine-Tuning a Pre-Trained LLM",
    "section": "Training and Monitoring",
    "text": "Training and Monitoring\nTo evaluate the model on the validation set, we need to define a validation function that computes some metric of interest. It seems like long-form question answering is generally hard to evaluate automatically (see paper), so I just resort to the ROUGE score, which is often used in summarization. It measures the similarity between the predicted answer by the model and the expected answer from the dataset.\n\nimport evaluate\n\nrouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(pred) -&gt; dict[str, float]:\n    \"\"\"Compute rouge score during validation/evaluation\"\"\"\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    rouge_output = rouge.compute(\n        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n    )[\"rouge2\"]\n\n    # Return rouge2 F1 score\n    return {\"rouge2\": round(rouge_output, 4)}\n\nFinally, putting all this together, we can start training:\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    processing_class=tokenizer,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n)\ntrainer.train()\n# Optionally, push the final trained model to the HuggingFace hub\n# trainer.push_to_hub()\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n***** Running training *****\n  Num examples = 128\n  Num Epochs = 1\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 2\n  Gradient Accumulation steps = 1\n  Total optimization steps = 64\n  Number of trainable parameters = 161,844,480\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nwandb: Currently logged in as: stefanbschneider to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSaving model checkpoint to models/checkpoint-64\nConfiguration saved in models/checkpoint-64/config.json\nConfiguration saved in models/checkpoint-64/generation_config.json\nModel weights saved in models/checkpoint-64/model.safetensors\ntokenizer config file saved in models/checkpoint-64/tokenizer_config.json\nSpecial tokens file saved in models/checkpoint-64/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTracking run with wandb version 0.19.5\n\n\nRun data is saved locally in /Users/stefanshschneider/Projects/private/blog/posts/llm-fine-tuning/wandb/run-20250209_132357-haehdkmt\n\n\nSyncing run test-run to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/stefanbschneider/huggingface\n\n\n View run at https://wandb.ai/stefanbschneider/huggingface/runs/haehdkmt\n\n\n\n    \n      \n      \n      [64/64 16:11, Epoch 1/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n\n\nTrainOutput(global_step=64, training_loss=3.9099715799093246, metrics={'train_runtime': 989.3982, 'train_samples_per_second': 0.129, 'train_steps_per_second': 0.065, 'total_flos': 691252437712896.0, 'train_loss': 3.9099715799093246, 'epoch': 1.0})\n\n\nBased on the calculation above, we get \\(\\frac{128}{2} = 64\\) optimization steps for this small example training run. Training progress is logged every 10 steps to Weights & Biases. There, you can see, among other metrics, that the training loss is gradually decreasing, indicating that the model is learning something:\n\n\n\nTraining loss decreasing during training in Weights & Biases"
  },
  {
    "objectID": "posts/llm-fine-tuning/index.html#full-training-run-on-vast.ai",
    "href": "posts/llm-fine-tuning/index.html#full-training-run-on-vast.ai",
    "title": "Fine-Tuning a Pre-Trained LLM",
    "section": "Full Training Run on Vast.ai*",
    "text": "Full Training Run on Vast.ai*\nIn the example above, I limited the size of the dataset to a very small fraction. For proper fine-tuning, the full dataset should be used by setting dataset_limit=None when loading the training data. Possibly, the training should be repeated for multiple epochs; num_train_epochs defaults to 3 epochs if it is not set otherwise in the training arguments. Similarly, logging, eval, and save frequency should be adjusted and the trained model should be pushed to the HuggingFace hub.\nAs I do not have a GPU locally, I rented a cheap GPU from Vast.ai*.\n\n\n\nCheap RTX 4070s Ti rented from Vast.ai*\n\n\nTraining on 50% of the full dataset (a bit over 100k samples) for one epoch took roughly 24 hours on the rented RTX 4070s Ti. Use train_data = load_and_process_dataset(\"train[:50%]\", dataset_limit=None) to load the first half of the training set.\nMonitoring the training and evaluation metrics on Weights & Biases, shows the loss slowly going down, both in training and validation, as well as the ROUGE score gradually increasing. If the loss would only decrease on the training set but not the validation set, this would indicate overfitting to the training set.\n\n\n\nMonitoring training and validation loss and ROUGE score on Weights & Biases\n\n\nI pushed the final fine-tuned model to HuggingFace: stefanbschneider/led-base-16384-lfqa-ans-len-512"
  },
  {
    "objectID": "posts/generative-qa/index.html",
    "href": "posts/generative-qa/index.html",
    "title": "Generative Document Question Answering with HuggingFace",
    "section": "",
    "text": "In a previous blog post, I showed how answer document-related questions with HuggingFace LLMs in just a few lines of Python code and visualize them as simple Gradio App.\nIn that blog post, I used the standard question-answering pipeline from HuggingFace. This pipeline defaults to a DistilBERT model (a smaller BERT model) fine-tuned on the Stanford Question Answering Dataset (SQuAD). This model and dataset are meant for extractive question answering as illustrated in the following example:\n\n%%capture --no-display\npip install -U pypdf torch transformers\n\n\nfrom transformers import pipeline\n\nextractive_qa = pipeline(task=\"question-answering\")\n\n# Abstract from \"Attention is all you need\" by Vaswani et al.: https://arxiv.org/abs/1706.03762\nabstract = \"\"\"The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task...\n\"\"\"\nquestion = \"What's a transformer'?\"\n\nextractive_qa(question=question, context=abstract)\n\nNo model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nDevice set to use mps:0\n\n\n{'score': 0.4559027850627899,\n 'start': 287,\n 'end': 302,\n 'answer': 'the Transformer'}\n\n\nThe pipeline is given a text as input, here parts of the “Attention is all you need” abstract (see arxiv), and a question that should be answered based on the given text/context.\nRather than an answer in natural language, the model outputs an excerpt that is extraced from the original context, given by a start- and end-index within. While this allows concise answers with clear reference to the original source, the answers are not very natural or accurate. The model has no way of combining and merging information from different places of the original text since it can only return a single contiguous excerpt.\nIn the example above, I asked what a transformer is and the model simply answered “the Transformer”. Not very helpful! (Note that the answer may be slightly different in the future, since I did not pin a model and model version in the pipeline.)\nEven passing the entire article into the model as context, does not improve the answer - it still only outputs “Transformer” as answer.\n\n# Read PDF\nfrom pathlib import Path\nfrom typing import Union\nfrom pypdf import PdfReader\n\n\ndef get_text_from_pdf(pdf_file: Union[str, Path]) -&gt; str:\n    \"\"\"Read the PDF from the given path and return a string with its entire content.\"\"\"\n    reader = PdfReader(pdf_file)\n    # Extract text from all pages\n    full_text = \"\"\n    for page in reader.pages:\n        full_text += page.extract_text()\n    return full_text\n\n\n# Read in the full article downloaded from https://arxiv.org/abs/1706.03762\nfull_article = get_text_from_pdf(\"transformer-paper.pdf\")\n# Print first few characters of the paper\nprint(full_article[:300])\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Par\n\n\n\n# Try to answer the same question as before with the full article as context\nextractive_qa(question=question, context=full_article)\n\n{'score': 0.20687614381313324,\n 'start': 22735,\n 'end': 22746,\n 'answer': 'Transformer'}"
  },
  {
    "objectID": "posts/generative-qa/index.html#existing-models-for-generative-qa",
    "href": "posts/generative-qa/index.html#existing-models-for-generative-qa",
    "title": "Generative Document Question Answering with HuggingFace",
    "section": "Existing Models for Generative Q&A",
    "text": "Existing Models for Generative Q&A\nLet’s use an existing encoder-decoder model from HuggingFace to try generative Q&A, e.g., the FLAN-T5. In comparison to the normal T5 model, the FLAN-T5 was fine-tuned on more downstream tasks:\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages\n\nHuggingFace does not have a pre-defined “generative Q&A” pipeline task, instead this belongs to “Text2Text Generation” as the input consists of the context and questions and the output is the generated answer.\nThe following code uses the FLAN-T5 model to generate an answer based on the full “Attention is all you need” article for the same question as above: What’s a transformer?\n\ngenerative_qa_t5 = pipeline(task=\"text2text-generation\", model=\"google/flan-t5-base\")\ninput_text = f\"{full_article} Given this context, please answer the following question. {question}\"\ngenerative_qa_t5(input_text)\n\nDevice set to use mps:0\nToken indices sequence length is longer than the specified maximum sequence length for this model (10385 &gt; 512). Running this sequence through the model will result in indexing errors\n\n\n[{'generated_text': 'a model architecture relying entirely on self-attention to compute representations of its input and'}]\n\n\n“a model architecture relying entirely on self-attention to compute representations of its input and”\nNot bad! The sentence ends out of nowhere, but this generated answer still makes sense. Much more so than the extracted answer above."
  },
  {
    "objectID": "posts/generative-qa/index.html#dealing-with-limited-sequence-length",
    "href": "posts/generative-qa/index.html#dealing-with-limited-sequence-length",
    "title": "Generative Document Question Answering with HuggingFace",
    "section": "Dealing with Limited Sequence Length",
    "text": "Dealing with Limited Sequence Length\nWhile the answer was good, there was a warning in the output of the pipeline above:\nToken indices sequence length is longer than the specified maximum sequence length for this model (10385 &gt; 512).\nThe configured FLAN-T5 model can only handle input sequences of maximum 512 tokens. The full research article is much longer (a bit more than 10k tokens).\nApparently, the HuggingFace pipeline already has some built-in mechanism to handle these overly long sequences, such that the model still output a sensible answer and did not crash despite the sequence being too long.\n\nSplitting the Sequence into Shorter Parts\nA simple approach to handle such overly long sequences is to split them into smaller parts that fit into the model’s maximum sequence length. Let’s split the full text into 20 parts, such that each part has at most 512 tokens.\n\n# Split the full text into parts and use them separately for answering the question.\ndef split_text_into_parts(full_text: str, num_parts: int) -&gt; list[str]:\n    \"\"\"Split the given full text into a list of equally sized parts.\"\"\"\n    len_per_part: int = int(len(full_text) / num_parts)\n    return [full_text[i * len_per_part : (i+1) * len_per_part] for i in range(num_parts)]\n\ntext_parts = split_text_into_parts(full_article, num_parts=20)\nfor text_part in text_parts:\n    input_text = f\"{text_part} Given this context, please answer the following question. {question}\"\n    print(generative_qa_t5(input_text))\n\n[{'generated_text': 'based solely on attention mechanisms'}]\n[{'generated_text': 'tensor2tensor'}]\n[{'generated_text': 'a model architecture eschewing recurrence and instead relying entirely on'}]\n[{'generated_text': 'first transduction model relying entirely on self-attention to compute representations of its input and'}]\n[{'generated_text': 'a decoder'}]\n[{'generated_text': 'a single attention head'}]\n[{'generated_text': 'encoder-decoder attention mechanisms'}]\n[{'generated_text': 'encoder and decoder stacks'}]\n[{'generated_text': 'encoder or decoder'}]\n[{'generated_text': 'self-attention layer'}]\n[{'generated_text': 'regularization'}]\n[{'generated_text': 'transformer'}]\n[{'generated_text': 'translation'}]\n[{'generated_text': 'transformer'}]\n[{'generated_text': 'attention-based model'}]\n[{'generated_text': 'tensorflow'}]\n[{'generated_text': 'LSTM networks'}]\n[{'generated_text': 'neural machine translation'}]\n[{'generated_text': '[34]'}]\n[{'generated_text': 'a syst'}]\n\n\nHaving split the text into 20 parts, we now get 20 answers. Some of them are more useful than others since these parts of the text apparently contain more useful information. Answer 4 sounds very similar to the one provided by the pipeline when passing in the whole article: “first transduction model relying entirely on self-attention to compute representations of its input and”\nIt seems like, under the hood, the HuggingFace pipeline also splits the full text into multiple parts, applying the model to each one. Likely, they use a more sophisticated way of splitting the parts with overlaps such that no information is lost at the boundaries between two parts.\nTo select the best out of all the provided answers, one could compute a score for each answer based on the average per-token score in the generated answer.\n\n\nUsing A Model with Long Sequence Length\nAn alternative to splitting a long sequence into smaller parts is to simply use another model with a longer supported sequence length, for example the Long-T5 model.\n\ngenerative_qa_long_t5 = pipeline(task=\"text2text-generation\", model=\"google/long-t5-local-base\")\ninput_text = f\"{full_article} Given this context, please answer the following question. {question}\"\ngenerative_qa_long_t5(input_text)\n\nSome weights of LongT5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-local-base and are newly initialized: ['lm_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nDevice set to use mps:0\n/opt/homebrew/Caskroom/miniforge/base/envs/llm/lib/python3.12/site-packages/torch/nn/functional.py:5096: UserWarning: MPS: The constant padding of more than 3 dimensions is not currently supported natively. It uses View Ops default implementation to run. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Pad.mm:465.)\n  return torch._C._nn.pad(input, pad, mode, value)\n\n\n\n\n\n[{'generated_text': 'formation formation trains trains trains trains rebuild Destin formationpartnered 1941 1941 nouveaux formation formation formation formationassemblée Lin Lin'}]\n\n\nAs you can see, the new model does not complain about the sequence being too long. Instead, it outputs a warning because the model is not fine-tuned for any downstream tasks such as Q&A. As a result, the generated answer is rubbish.\nFor better results, we should fine-tune the model on a Q&A dataset (such as DuoRC). In addition to the Long-T5, there are other models that focus explicitly on long sequence lengths, e.g., the Longformer and it’s encoder-decoder variant LED (Longformer Encoder-Decoder), which is more useful for generative Q&A.\nI plan to dive deeper into long sequence lengths in a future blog post."
  },
  {
    "objectID": "posts/generative-qa/index.html#whats-next",
    "href": "posts/generative-qa/index.html#whats-next",
    "title": "Generative Document Question Answering with HuggingFace",
    "section": "What’s Next?",
    "text": "What’s Next?\n\nRead some of my related blog posts:\n\nBuilding a Simple Question Answering App with HuggingFace\nUnderstanding Transformers and Attention\n\nFine-tune a long-sequence model for generative Q&A"
  },
  {
    "objectID": "posts/rllib-private-cluster/index.html",
    "href": "posts/rllib-private-cluster/index.html",
    "title": "Scaling Deep Reinforcement Learning to a Private Cluster",
    "section": "",
    "text": "Warning\n\n\n\nThis post is incomplete. See the ongoing discussion for details.\nIn this blog post, I use reinforcement learning (RL) to solve a custom optimization task (here, related to coordination in mobile networks). To this end, I use the scalable RL framework RLlib, which is part of Ray, and a custom environment, which implements the OpenAI Gym interface. As RL algorithm, I use proximal policy optimization (PPO), which is implemented in RLlib and configured in my environment.\nI first show how to train PPO on my environment when running locally. Then, to speed up training, I execute training on a private/on-premise multi-node cluster.\nWhile it is simple in principle, it took me a while to go from running RLlib and my custom environment locally to getting it to work on a private cluster. I’m hoping this guide is useful for anyone in a similar situation. In this blog post, I focus on the general workflow but use my specific environment as an example. I will cover details about my RL approach and environment in a future blog post."
  },
  {
    "objectID": "posts/rllib-private-cluster/index.html#setup",
    "href": "posts/rllib-private-cluster/index.html#setup",
    "title": "Scaling Deep Reinforcement Learning to a Private Cluster",
    "section": "Setup",
    "text": "Setup\nInstallation requires Python 3.8+ and should work on Linux, Windows, and Mac. Inside a virtualenv, install RLlib with\npip install ray[rllib]\nThen install the custom environment. Here, DeepCoMP as described in the readme:\npip install deepcomp\nTest the installation with deepcomp -h, which should show the available CLI options."
  },
  {
    "objectID": "posts/rllib-private-cluster/index.html#training",
    "href": "posts/rllib-private-cluster/index.html#training",
    "title": "Scaling Deep Reinforcement Learning to a Private Cluster",
    "section": "Training",
    "text": "Training\nOnce installation is complete, train a centralized RL agent with PPO in an example scenario. Note, that training will take a while (around 15min on my laptop), so running the command inside a detachable GNU screen or tmux session makes sense.\ndeepcomp --agent central --train-steps 100000 --env medium --slow-ues 3\nThis trains a centralized PPO agent for 100k training steps running on a single core. To use more cores, set the corresponding value via CLI argument --workers. The additional arguments --env and --slow-ues configure my custom DeepCoMP environment (more about that in another blog post). During training, updates should be printed on screen and progress can be monitored with TensorBoard. To start TensorBoard, run (in a separate terminal):\ntensorboard --logdir results/PPO/\nHere, the TensorBoard files are in results/PPO/, but this depends on the environment. Once started, TensorBoard can be accessed at localhost:6006.\n\n\n\nTensorBoard screenshot showing training progress."
  },
  {
    "objectID": "posts/rllib-private-cluster/index.html#results",
    "href": "posts/rllib-private-cluster/index.html#results",
    "title": "Scaling Deep Reinforcement Learning to a Private Cluster",
    "section": "Results",
    "text": "Results\nIn the case of my environment, results are saved in the results directory on the project root (where deepcomp is installed) by default. To specify a custom result path, use the --result-dir CLI argument, which accepts relative paths.\nFiles in folders prefixed with PPO contain neural network weights, configuration, log, and progress files generated by RLlib. They are useful for analyzing training progress or when loading a trained agent for inference (--test arg) or continued training (--continue). Additionally, folders test and videos are generated by DeepCoMP and contain easy-to-parse testing/evaluation results and rendered videos, depending on the DeepCoMP CLI args (--eval and --video).\nOf course, this is just an example. Results are saved differently for each problem and environment."
  },
  {
    "objectID": "posts/rllib-private-cluster/index.html#preparations",
    "href": "posts/rllib-private-cluster/index.html#preparations",
    "title": "Scaling Deep Reinforcement Learning to a Private Cluster",
    "section": "Preparations",
    "text": "Preparations\nWhile there are virtually no code changes required in the environment, some preparation steps were necessary for me to get RLlib to work on our private/on-premise cluster.\n\nCluster Configuration\nThe Ray cluster configuration is saved in a YAML file. My configuration file is here.\nThe most relevant fields concern information about the private cluster:\nprovider:\n    type: local\n    head_ip: &lt;head-machine-ip-or-address&gt;\n    worker_ips:\n        - &lt;worker1-ip&gt;\n        - &lt;worker2-ip&gt;\nHere, type: local indicates that the cluster is local/private/on premise. The head IP or address points to the head node, i.e., the machine that should coordinate the cluster. To execute commands and train my RL agent, I will later attach to the head node, start training and TensorBoard, and finally retrieve results. The workers are other machines in the cluster on which the training is executed.\nDepending on the number of workers listed under worker_ips, also set min_workers and max_workers to the same value.\nFor authentication when logging into the workers and distributing computation across them, also configure auth:\nauth:\n    ssh_user: stefan\n    # Optional if an ssh private key is necessary to ssh to the cluster\n    # This is the SSH key of the local laptop, not of the head node\n    ssh_private_key: ~/.ssh/id_rsa\n\n\nInstallation\nTo run code on the workers, install ray[rllib] and the custom environment deepcomp on each worker machine of the cluster. All nodes in the cluster must have the same Python and same ray version (check with --version inside the virutalenv).\nMaybe this can be avoided, eg, by using Docker images that are pulled automatically?\n\n\nSSH Access\nThe head node needs ssh access to all worker nodes. Ensure the head node’s public SSH key is registered as authorized key (in ssh/authorized_keys) in all worker nodes. The head node’s private key path should be configured in the cluster.yaml.\nIn fact, the private key configured in cluster.yaml is the private key of the local laptop that controls the cluster. Not the head node.\n\n\nray command\nThe ray command needs to be available on all cluster nodes. If the ray command is not available on the cluster, trying to start the cluster will crash with the error Command 'ray' not found ... Failed to setup head node..\nIf ray is installed in a virtual environment, the easiest option is to automatically source the virtualenv on each login. Particularly, adding the following line to .bashrc will source the virtualenv:\nsource path/to/venv/bin/activate\nWhere path/to/venv needs to point to the virtualenv. The change is in effect after log out and back in.\nThen ray --version should run without errors.\n\n\nConnect to Ray cluster\nTo ensure that running ray connects to the same cluster and the same Redis DB, use ray.init(address='auto'). Without argument address='auto', execution on the cluster does not work.\nHowever, for me, adding address='auto' breaks local execution. Hence, I added an optional CLI argument --cluster to my custom deepcomp environment, which adds address='auto' for running the environment on a cluster without code changes."
  },
  {
    "objectID": "posts/rllib-private-cluster/index.html#starting-the-ray-cluster",
    "href": "posts/rllib-private-cluster/index.html#starting-the-ray-cluster",
    "title": "Scaling Deep Reinforcement Learning to a Private Cluster",
    "section": "Starting the Ray Cluster",
    "text": "Starting the Ray Cluster\n\nOn the local machine\nStart cluster:\n# start the cluster (non-blocking)\nray up cluster.yaml\n\n# forward the cluster dashboard to the local machine (this is a blocking command)\nray dashboard cluster.yaml\nView dashboard: http://localhost:8265\n\n\n\n\n\n\nWarning\n\n\n\nThis currently doesn’t work for me. It only shows the head node, not the workers.\n\n\nConnect to cluster and run command for training. Note, you can attach but not detach. Thus, better to run this in a screen/tmux session.\nray attach cluster.yaml\ndeepcomp --agent central --train-steps 100000 --env medium --slow-ues 3 --cluster --workers XY\nOnce training completed, detach/close terminal with Ctrl+D.\n\n\nMonitoring Training Progress\n\nTraining updates should be printed inside the attached terminal\nOn the cluster’s head node, htop should show ray::RolloutWorker running.\nOn the cluster’s worker nodes, htop should show ray::PPO()::train() (or similar) to indicate the training is running.\nMonitor progress with Tensorboard running tensorboard --host 0.0.0.0 --logdir results/PPO/ on the cluster’s head node. Then access on &lt;head-node-ip&gt;:6006.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis currently doesn’t work for me. It seems like the program is only running on the head nodes, not at all on the workers.\n\n\n\n\nRetrieving Training & Testing Results\nFrom the local laptop, use ray rsync-down to copy the result files from the cluster’s head node to the local laptop:\n# ray rsync-down &lt;cluster-config&gt; &lt;source&gt; &lt;target&gt;\nray rsync-down cluster.yaml ~/DeepCoMP/results .\nWill be copied to local directory into results.\n\n\nTerminating the Cluster\nFrom the local laptop:\nray down cluster.yaml"
  },
  {
    "objectID": "posts/rllib-private-cluster/index.html#debugging",
    "href": "posts/rllib-private-cluster/index.html#debugging",
    "title": "Scaling Deep Reinforcement Learning to a Private Cluster",
    "section": "Debugging",
    "text": "Debugging\nIf the process above does not work, the logs may contain helpful information for debugging the problem. To print the logs, run on the cluster’s head node:\ncat /tmp/ray/session_latest/logs/monitor.*\nTo print a status overview of the cluster:\n ray status --address &lt;address:port&gt;\nWhere &lt;address:port&gt; belongs to the cluster and is displayed when starting it with ray up cluster.yaml (after To connect to this Ray runtime from another node, run ...).\n\nCommon Errors\n\nCommand 'ray' not found when trying to start the cluster\n\nThe ray command is not available on the head node after SSH. One solution is to source the virtualenv with ray in the .bashrc or to install ray system-wide.\n\nRepeatedly autoscaler +4m36s) Adding 1 nodes of type ray-legacy-head-node-type. when training on the cluster\n\n??\n\nWhen trying to run code on the cluster after ray attach cluster.yaml: (raylet) OSError: [Errno 98] Address already in use\n\n?? Is the redis server already running; something wrong with the cluster ??\nStopping and restarting the cluster seems to fix the problem: Detach, then from the laptop stop the cluster: ray down cluster.yaml, then start it again ray up cluster.yaml\n\nAll load seems to be just on the cluster head and nothing is distributed to the workers (when observing with htop)\n\n??\n\nIn the logs:\n\nERROR monitor.py:264 -- Monitor: Cleanup exception. Trying again...\n1 random worker nodes will not be shut down. (due to --keep-min-workers)"
  },
  {
    "objectID": "posts/my-first-project/index.html",
    "href": "posts/my-first-project/index.html",
    "title": "Lessons Learned from Leading My First Project",
    "section": "",
    "text": "Today is the last day of my first bigger project, RealVNF, which I have been leading for 27 months (Nov. 2018 - Jan. 2021). Early 2018, I applied for the project through the Software Campus and received a €100k grant for hiring student research assistants, traveling, equipment, etc. I also had the opportunity to participate in several leadership trainings organized by companies across Germany. The project itself was research-focused and a collaboration with researchers from Huawei Germany (more info the website).\nI found that leading the project was surprisingly challenging but, in the end, definitely successful and a great experience. This blog post reflects on the last 27 months and summarizes seven lessons learned from a leadership/project management perspective."
  },
  {
    "objectID": "posts/my-first-project/index.html#put-effort-into-recruiting-the-right-people",
    "href": "posts/my-first-project/index.html#put-effort-into-recruiting-the-right-people",
    "title": "Lessons Learned from Leading My First Project",
    "section": "Put Effort Into Recruiting the Right People",
    "text": "Put Effort Into Recruiting the Right People\nRecruiting the right people for the project was very important. Most of the people I hired at the beginning stayed until the end of the project.\nInitially, I posted the job openings broadly online and received many applications. I also directly asked talented people I knew to apply. To avoid interviewing dozens of applicants, I introduced a small programming exercise that applicants had to submit. It was doable in maybe 1-3 hours using basic Python and tools like GitHub. I invited applicants who finished this first task to an interview, asking them to read a selected research paper up front and prepare to summarize it during the interview.\nBoth tasks meant initial extra work for both me and the applicants but helped to select the most suitable candidates for the project. In the end, I got to work with motivated and talented people, who made the project a success."
  },
  {
    "objectID": "posts/my-first-project/index.html#talk-to-potential-users-early",
    "href": "posts/my-first-project/index.html#talk-to-potential-users-early",
    "title": "Lessons Learned from Leading My First Project",
    "section": "Talk to Potential Users Early",
    "text": "Talk to Potential Users Early\nWhen preparing the initial project plan, of course, I discussed it with people in my group at university. Still, it was only when talking to the researchers at Huawei, I understood that they had a very different perspective on the problem we were trying to solve. We spent the first months discussing and aligning our understanding of the problem and possible solution approaches. These discussions were very valuable to ensure that the work in the project was actually relevant and helpful in practice. As a leading company in networking, Huawei is a potential user of our developed coordination schemes and thus was an ideal project partner.\nEspecially in academia, it is easy to get excited about an idea without properly thinking (or even understanding) its relevance in practice. Whether in academia or industry, I believe it is very important to talk to potential users/customers early and understand their needs before designing a solution. This is related to the concept of design thinking."
  },
  {
    "objectID": "posts/my-first-project/index.html#start-small-scale-quickly",
    "href": "posts/my-first-project/index.html#start-small-scale-quickly",
    "title": "Lessons Learned from Leading My First Project",
    "section": "Start Small, Scale Quickly",
    "text": "Start Small, Scale Quickly\nWhen starting the project, we had tons of interesting ideas and thought of a variety of relevant problem aspects that we wanted to address. Creating an approach from scratch that incorporates all ideas and considers all problem aspects would have been prohibitively complex. Instead, it was important to quickly identify the most important aspects and most promising ideas and simplify everything else. This helped to get started quickly.\nOnce the simple approach worked, we could extend it to integrate more ideas and address more problem aspects. It is also helpful to have a working prototype at all times: First for the smallest and simplest case, then for more and more complex scenarios. This allows running evaluations for each stage and quantifying progress. Trying to solve everything at once can be overwhelming and makes it difficult to understand the root cause of problems or bugs. Building on something that works already is much simpler, faster, and less error prone. Versioning everything allows going back to previous, working versions if something breaks in between.\nOf course, it makes sense to think of a realistic roadmap and architecture at the beginning to have a clear direction and ensure that initial solutions can easily be extended later."
  },
  {
    "objectID": "posts/my-first-project/index.html#focus-on-just-a-few-topics-at-once",
    "href": "posts/my-first-project/index.html#focus-on-just-a-few-topics-at-once",
    "title": "Lessons Learned from Leading My First Project",
    "section": "Focus on Just a Few Topics at Once",
    "text": "Focus on Just a Few Topics at Once\nAt the beginning, I was wondering whether I should assign all students working on the project to their own topic or to let them all work on the same topic. The former would allow more parallelization and independent work, thus less dependencies and blocking each other. Still, I found that it was much easier focusing all work on just one or two topics (of course, still with different tasks). Not only did it help drive progress faster, it also allowed more productive meetings and discussions between team members.\nWhile this worked well for my project with 3-4 people, it will likely be different for much larger projects."
  },
  {
    "objectID": "posts/my-first-project/index.html#planning-is-everything.-the-plan-is-nothing.",
    "href": "posts/my-first-project/index.html#planning-is-everything.-the-plan-is-nothing.",
    "title": "Lessons Learned from Leading My First Project",
    "section": "Planning is Everything. The Plan is Nothing.",
    "text": "Planning is Everything. The Plan is Nothing.\nEspecially in a research-focused project like RealVNF, there is a lot of uncertainty, e.g., about experiment outcomes, where the outcome of one task affects the next one. This makes mid-/long-term planning very difficult.\nI found it useful to have a high-level “storyline” in mind with intermediate milestones, but to frequently adjust it to new insights or results. When interpreting results and adjusting the plan, we always discussed and decided next steps in the group. It is important for motivation to not only understand what a task is about but also why it is relevant. In general, frequent and clear communication within the team was crucial to quickly adapt to new outcomes and avoid wasting time."
  },
  {
    "objectID": "posts/my-first-project/index.html#clear-frequent-and-open-communication-within-the-team",
    "href": "posts/my-first-project/index.html#clear-frequent-and-open-communication-within-the-team",
    "title": "Lessons Learned from Leading My First Project",
    "section": "Clear, Frequent, and Open Communication Within the Team",
    "text": "Clear, Frequent, and Open Communication Within the Team\nFrequent communication with and within the team is crucial. Still, I did not want to waste everyone’s time with unnecessary meetings. For us, weekly group meetings worked quite well. I structured each meeting into status updates, discussion items, and tasks for the next week and tried to limit them to at most an hour. Upcoming tasks need to be communicated very clearly (goal, scope, time) to avoid misunderstandings. Sending out short notes after each meeting helped to keep everyone on the same page.\nFor an overview of past, current, and upcoming tasks, we also used GitHub issues and project boards (Kanban boards). Additionally, we defined high-level monthly goals/milestones to keep the project a bit more focused. To keep the discussion going during the week and quickly adapt to new experiment outcomes or new insights, we kept in touch via Slack constantly.\nWe also did individual feedback discussions every now and then, which were really useful. In retrospective, I would do one-to-one meetings more regularly to better understand each team member’s needs and interests. It is important to encourage open communication at all times such that team members say if they have too many or too few tasks. Both is demotivating. At the same time, it can be difficult to estimate how quickly someone will progress with a task.\nFinally, while communication and tooling still worked well when working remotely during COVID-19, social meetings were no longer possible. Such social meetings were nice to hang out informally and improve team spirit."
  },
  {
    "objectID": "posts/my-first-project/index.html#embrace-bureaucracy",
    "href": "posts/my-first-project/index.html#embrace-bureaucracy",
    "title": "Lessons Learned from Leading My First Project",
    "section": "Embrace Bureaucracy",
    "text": "Embrace Bureaucracy\nI suppose any bigger project comes with bureaucracy. In my case, I had to communicate with the funding partner and the university’s administration. Sometimes, I was surprised by the amount of rules, formulas, and processes for hiring people, traveling, and buying equipment - even with the money available (i.e., after securing the grant).\nAs a small anecdote, I tried to hire student research assistants for the project to start with the beginning of the project in November. Unfortunately, this was not possible and I was told that new contracts could not start at the end of a year. I had to wait until January for hiring the students. In January, I was asked why I did not follow my original budget plan and did not spend any money on staff during the first two months (November and December)…\nStill, while some rules may be a bit bizarre, all people in the administration were friendly and helpful. I understood that I had to identify the right people to ask and keep asking persistently to eventually understand the rules and get what I needed."
  },
  {
    "objectID": "posts/pytorch-getting-started/index.html",
    "href": "posts/pytorch-getting-started/index.html",
    "title": "Getting Started with PyTorch",
    "section": "",
    "text": "Note\n\n\n\nCode from the official PyTorch 60-min-blitz tutorial."
  },
  {
    "objectID": "posts/pytorch-getting-started/index.html#loading-the-cifar10-dataset",
    "href": "posts/pytorch-getting-started/index.html#loading-the-cifar10-dataset",
    "title": "Getting Started with PyTorch",
    "section": "Loading the CIFAR10 Dataset",
    "text": "Loading the CIFAR10 Dataset\n\n\nCode\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n\n\n\n\n\n ship  frog truck  bird"
  },
  {
    "objectID": "posts/pytorch-getting-started/index.html#building-a-cnn-model-with-pytorch",
    "href": "posts/pytorch-getting-started/index.html#building-a-cnn-model-with-pytorch",
    "title": "Getting Started with PyTorch",
    "section": "Building a CNN Model with PyTorch",
    "text": "Building a CNN Model with PyTorch\nArchitecture:\n\nInput: 32x32-pixel images with 3 channels (RGB) → 3x32x32 images\nConvolutions with 3 input channels, 6 output channels, and 5x5 square convolution → 6x28x28 images\n2x2 max pooling (subsampling) → 6x14x14 images\n6 input channels (from the previous Conv2d layer), 16 output channels, 5x5 square convolutions → 16x10x10 images\n2x2 max pooling (subsampling) → 16x5x5 images\nFully connected linear (=dense) layer with 16x5x5=400 input size and 120 output; ReLU activation\nFully connected layer with 120 input and 84 output; ReLU activation\nFully connected output layer with 84 input and 10 output (for the 10 classes in the CIFAR10 dataset); no/linear activation\n\nNote that the layers are defined in the constructor and the activations applied in the forward function.\nTo calculate the output size of a convolutional layer, use this formula:\n\\(\\frac{W−K+2P}{S} +1\\) with input size \\(W\\) (width and height for square images), convolution size \\(K\\), padding \\(P\\) (default 0), and stride \\(S\\) (default 1).\nFurther explanation on layer sizes: Medium article by Jake Krajewski\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()\n\nDefine the loss as cross entropy loss and SGD as optimizer.\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
  },
  {
    "objectID": "posts/pytorch-getting-started/index.html#training",
    "href": "posts/pytorch-getting-started/index.html#training",
    "title": "Getting Started with PyTorch",
    "section": "Training",
    "text": "Training\nOver 5 epochs.\n\n#collapse-output\n\nfor epoch in range(5):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')\n\n[1,  2000] loss: 2.200\n[1,  4000] loss: 1.837\n[1,  6000] loss: 1.695\n[1,  8000] loss: 1.587\n[1, 10000] loss: 1.534\n[1, 12000] loss: 1.469\n[2,  2000] loss: 1.391\n[2,  4000] loss: 1.377\n[2,  6000] loss: 1.361\n[2,  8000] loss: 1.332\n[2, 10000] loss: 1.306\n[2, 12000] loss: 1.297\n[3,  2000] loss: 1.227\n[3,  4000] loss: 1.220\n[3,  6000] loss: 1.202\n[3,  8000] loss: 1.217\n[3, 10000] loss: 1.180\n[3, 12000] loss: 1.187\n[4,  2000] loss: 1.093\n[4,  4000] loss: 1.094\n[4,  6000] loss: 1.141\n[4,  8000] loss: 1.109\n[4, 10000] loss: 1.127\n[4, 12000] loss: 1.125\n[5,  2000] loss: 1.033\n[5,  4000] loss: 1.047\n[5,  6000] loss: 1.039\n[5,  8000] loss: 1.072\n[5, 10000] loss: 1.039\n[5, 12000] loss: 1.061\nFinished Training\n\n\nSave the trained model locally.\n\nPATH = './cifar_net.pth'\ntorch.save(net.state_dict(), PATH)"
  },
  {
    "objectID": "posts/pytorch-getting-started/index.html#testing-the-trained-model",
    "href": "posts/pytorch-getting-started/index.html#testing-the-trained-model",
    "title": "Getting Started with PyTorch",
    "section": "Testing the Trained Model",
    "text": "Testing the Trained Model\n\ndataiter = iter(testloader)\nimages, labels = dataiter.next()\n\n# print images\nimshow(torchvision.utils.make_grid(images))\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n\n\n\n\nGroundTruth:    cat  ship  ship plane\n\n\n\n# load the saved model (just for show; it's already loaded)\nnet = Net()\nnet.load_state_dict(torch.load(PATH))\n\n# make predictions\noutputs = net(images)\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n                              for j in range(4)))\n\nPredicted:    dog   car   car plane\n\n\n\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\n    100 * correct / total))\n\nAccuracy of the network on the 10000 test images: 60 %\n\n\n\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(4):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(10):\n    print('Accuracy of %5s : %2d %%' % (\n        classes[i], 100 * class_correct[i] / class_total[i]))\n\nAccuracy of plane : 70 %\nAccuracy of   car : 75 %\nAccuracy of  bird : 44 %\nAccuracy of   cat : 33 %\nAccuracy of  deer : 57 %\nAccuracy of   dog : 56 %\nAccuracy of  frog : 74 %\nAccuracy of horse : 61 %\nAccuracy of  ship : 59 %\nAccuracy of truck : 71 %"
  },
  {
    "objectID": "posts/pytorch-getting-started/index.html#what-next",
    "href": "posts/pytorch-getting-started/index.html#what-next",
    "title": "Getting Started with PyTorch",
    "section": "What Next?",
    "text": "What Next?\n\nUsing PyTorch Inside a Django App\nOther blog posts related to PyTorch\nOfficial PyTorch Tutorials"
  },
  {
    "objectID": "posts/partial-observability/index.html",
    "href": "posts/partial-observability/index.html",
    "title": "Dealing with Partial Observability In Reinforcement Learning",
    "section": "",
    "text": "Important\n\n\n\nThis blog post is still work in progress. Currently, there seems to be an issue with attention in RLlib. I have not had the time to look into this again but still wanted to share the current state since it might still be useful.\nIn reinforcement learning (RL), the RL agent typically selects a suitable action based on the last observation. In many practical environments, the full state can only be observed partially, such that important information may be missing when just considering the last observation. This blog post covers options for dealing with missing and only partially observed state, e.g., considering a sequence of last observations and applying self-attention to this sequence."
  },
  {
    "objectID": "posts/partial-observability/index.html#example-the-cartpole-gym-environment",
    "href": "posts/partial-observability/index.html#example-the-cartpole-gym-environment",
    "title": "Dealing with Partial Observability In Reinforcement Learning",
    "section": "Example: The CartPole Gym Environment",
    "text": "Example: The CartPole Gym Environment\nAs an example, consider the popular OpenAI Gym CartPole environment. Here, the task is to move a cart left or right in order to balance a pole on the cart as long as possible.\n\n\n\nOpenAI Gym CartPole-v1 Environment\n\n\nIn the normal CartPole-v1 environment, the RL agent observes four scalar values (defined here): * The cart position, i.e., where the cart currently is. * The cart velocity, i.e., how fast the cart is currently moving and in which direction (can be positive or negative). * The pole angle, i.e., how tilted the pole currently is and in which direction. * The pole angular velocity, i.e., how fast the pole is currently moving and in which direction.\nAll four observations are important to decide whether the cart should move left or right.\nNow, assume the RL agent only has access to an instant snapshot of the cart and the pole (e.g., through a photo/raw pixels) and can neither observe cart velocity nor pole angular velocity. In this case, the RL agent only has partial observations and does not know whether and how fast the pole is currently swinging. As a result, standard RL agents cannot solve the problem and do not learn to balance the pole. How to deal with this problem of partial observations, i.e., missing state (here, cart and pole velocity)?"
  },
  {
    "objectID": "posts/partial-observability/index.html#options-for-dealing-with-partial-observations",
    "href": "posts/partial-observability/index.html#options-for-dealing-with-partial-observations",
    "title": "Dealing with Partial Observability In Reinforcement Learning",
    "section": "Options for Dealing With Partial Observations",
    "text": "Options for Dealing With Partial Observations\nThere are different options for dealing with partial observations/missing state, e.g., missing velocity in the CartPole example:\n\nAdd the missing state explicitly, e.g., measure and observe velocity. Note that this may require installing extra sensors or may even be infeasible in some scenarios.\nIgnore the missing state, i.e., just rely on the available, partial observations. Depending on the missing state, this may be problematic and keep the agent from learning.\nKeep track of a sequence of the last observations. By observing the cart position and pole angle over time, the agent can implicitly derive their velocity. There are different ways to deal with this sequence:\n\nJust use the sequence as is for a standard multi-layer perceptron (MLP)/dense feedforward neural network.\nFeed the sequence into a recurrent neural network (RNN), e.g., with long short-term memory (LSTM).\nFeed the sequence into a neural network with self-attention.\n\n\nIn the following, I go through each option in more detail and illustrate them using simple example code.\n\nSetup\nFor the examples, I use a PPO RL agent from Ray RLlib with the CartPole environment, described above.\nTo install these dependencies, run the following code (tested with Python 3.8 on Windows):\n\n#collapse-output\nimport os.path\n!pip install ray[rllib]==1.8.0\n!pip install tensorflow==2.7.0\n!pip install seaborn==0.11.2\n!pip install gym==0.21.0\n!pip install pyglet==1.5.21\n\nStart up ray, load the default PPO config, and determine the number of training iterations, which is the same for all options (for comparability).\n\nimport ray\nfrom ray.rllib.agents import ppo\n\n# adjust num_cpus and num_gpus to your system\n# for some reason, num_cpus=2 gets stuck on my system (when trying to train)\nray.init(num_cpus=3, ignore_reinit_error=True)\n\n# stop conditions based on training iterations (each with 4000 train steps)\ntrain_iters = 10\nstop = {\"training_iteration\": train_iters}\n\n2021-12-01 22:52:23,565 INFO worker.py:832 -- Calling ray.init() again after it has already been called.\n\n\n\n\nOption 1: Explicitly Add Missing State\nSometimes, it is possible to extend the observations and explicitly add important state that was previously unobserved. In the CartPole example, the cart and pole velocity can simply be “added” by using the default CartPole-v1 environment. Here, the cart velocity and pole velocity are already included in the observations.\nNote that in many practical scenarios such “missing” state cannot be added and observed simply. Instead, it may require installing additional sensors or may even be completely infeasible.\nLet’s start with the best case, i.e., explicitly including the missing state.\n\nimport gym\n\n# the default CartPole env has all 4 observations: position and velocity of both cart and pole\nenv = gym.make(\"CartPole-v1\")\nenv.observation_space.shape\n\n(4,)\n\n\n\n#collapse-output\n\n# run PPO on the default CartPole-v1 env\nconfig1 = ppo.DEFAULT_CONFIG.copy()\nconfig1[\"env\"] = \"CartPole-v1\"\n\n# training takes a while\nresults1 = ray.tune.run(\"PPO\", config=config1, stop=stop)\nprint(\"Option 1: Training finished successfully\")\n\n== Status ==Current time: 2021-12-01 22:52:39 (running for 00:00:00.16)Memory usage on this node: 9.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nPENDING\n\n\n\n\n\n\n\n(pid=16556) 2021-12-01 22:52:50,305 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n(pid=16556) 2021-12-01 22:52:50,310 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n(pid=16556) 2021-12-01 22:52:50,310 INFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n(pid=2436) 2021-12-01 22:53:02,005  WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=16556) 2021-12-01 22:53:04,522 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=16556) 2021-12-01 22:53:05,755 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n(pid=16556) 2021-12-01 22:53:05,755 INFO trainable.py:110 -- Trainable.setup took 15.450 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n(pid=16556) 2021-12-01 22:53:05,755 WARNING util.py:57 -- Install gputil for GPU system monitoring.\n(pid=16556) 2021-12-01 22:53:12,536 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n(pid=16556) Windows fatal exception: access violation\n(pid=16556) \n(pid=2436) [2021-12-01 22:54:37,017 E 2436 8960] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=2436) Windows fatal exception: access violation\n(pid=2436) \n(pid=14216) [2021-12-01 22:54:37,018 E 14216 11712] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=14216) Windows fatal exception: access violation\n(pid=14216) \n2021-12-01 22:54:37,138 INFO tune.py:630 -- Total run time: 118.07 seconds (117.55 seconds for the tuning loop).\n\n\n== Status ==Current time: 2021-12-01 22:53:05 (running for 00:00:26.46)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:53:06 (running for 00:00:27.52)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:53:12 (running for 00:00:32.87)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:53:17 (running for 00:00:37.94)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n\n\n\n\n\n\nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 4000\n  custom_metrics: {}\n  date: 2021-12-01_22-53-17\n  done: false\n  episode_len_mean: 20.331632653061224\n  episode_media: {}\n  episode_reward_max: 69.0\n  episode_reward_mean: 20.331632653061224\n  episode_reward_min: 8.0\n  episodes_this_iter: 196\n  episodes_total: 196\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6666923761367798\n          entropy_coeff: 0.0\n          kl: 0.02727562002837658\n          model: {}\n          policy_loss: -0.03548957407474518\n          total_loss: 163.0438232421875\n          vf_explained_var: 0.02411726862192154\n          vf_loss: 163.0738525390625\n    num_agent_steps_sampled: 4000\n    num_agent_steps_trained: 4000\n    num_steps_sampled: 4000\n    num_steps_trained: 4000\n  iterations_since_restore: 1\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 85.05625\n    ram_util_percent: 85.14375\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1063987523513995\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12976663512813205\n    mean_inference_ms: 2.738906715446057\n    mean_raw_obs_processing_ms: 0.29470778093728145\n  time_since_restore: 11.613266468048096\n  time_this_iter_s: 11.613266468048096\n  time_total_s: 11.613266468048096\n  timers:\n    learn_throughput: 829.341\n    learn_time_ms: 4823.104\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 590.463\n    sample_time_ms: 6774.35\n    update_time_ms: 2.998\n  timestamp: 1638395597\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 4000\n  training_iteration: 1\n  trial_id: 0091e_00000\n  \nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 8000\n  custom_metrics: {}\n  date: 2021-12-01_22-53-27\n  done: false\n  episode_len_mean: 43.5\n  episode_media: {}\n  episode_reward_max: 128.0\n  episode_reward_mean: 43.5\n  episode_reward_min: 9.0\n  episodes_this_iter: 85\n  episodes_total: 281\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.30000001192092896\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6100984811782837\n          entropy_coeff: 0.0\n          kl: 0.018913770094513893\n          model: {}\n          policy_loss: -0.03986572101712227\n          total_loss: 392.36260986328125\n          vf_explained_var: 0.05626700446009636\n          vf_loss: 392.3967590332031\n    num_agent_steps_sampled: 8000\n    num_agent_steps_trained: 8000\n    num_steps_sampled: 8000\n    num_steps_trained: 8000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 2\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 85.2\n    ram_util_percent: 85.08571428571429\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10915718929193942\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12762452156562074\n    mean_inference_ms: 2.478078257762592\n    mean_raw_obs_processing_ms: 0.24854778323044394\n  time_since_restore: 21.935389518737793\n  time_this_iter_s: 10.322123050689697\n  time_total_s: 21.935389518737793\n  timers:\n    learn_throughput: 806.601\n    learn_time_ms: 4959.079\n    load_throughput: 8015870.043\n    load_time_ms: 0.499\n    sample_throughput: 474.211\n    sample_time_ms: 8435.067\n    update_time_ms: 3.001\n  timestamp: 1638395607\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 8000\n  training_iteration: 2\n  trial_id: 0091e_00000\n  \nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 12000\n  custom_metrics: {}\n  date: 2021-12-01_22-53-37\n  done: false\n  episode_len_mean: 70.15\n  episode_media: {}\n  episode_reward_max: 292.0\n  episode_reward_mean: 70.15\n  episode_reward_min: 11.0\n  episodes_this_iter: 36\n  episodes_total: 317\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.30000001192092896\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5675911903381348\n          entropy_coeff: 0.0\n          kl: 0.009604203514754772\n          model: {}\n          policy_loss: -0.02363675646483898\n          total_loss: 785.93994140625\n          vf_explained_var: 0.09917476773262024\n          vf_loss: 785.960693359375\n    num_agent_steps_sampled: 12000\n    num_agent_steps_trained: 12000\n    num_steps_sampled: 12000\n    num_steps_trained: 12000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 3\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 83.96428571428571\n    ram_util_percent: 85.07857142857142\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11086732721675017\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.129464220418653\n    mean_inference_ms: 2.403483071642798\n    mean_raw_obs_processing_ms: 0.23551804810537205\n  time_since_restore: 31.98438596725464\n  time_this_iter_s: 10.048996448516846\n  time_total_s: 31.98438596725464\n  timers:\n    learn_throughput: 827.692\n    learn_time_ms: 4832.716\n    load_throughput: 6088260.312\n    load_time_ms: 0.657\n    sample_throughput: 436.537\n    sample_time_ms: 9163.022\n    update_time_ms: 2.669\n  timestamp: 1638395617\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 12000\n  training_iteration: 3\n  trial_id: 0091e_00000\n  \nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 16000\n  custom_metrics: {}\n  date: 2021-12-01_22-53-46\n  done: false\n  episode_len_mean: 97.99\n  episode_media: {}\n  episode_reward_max: 371.0\n  episode_reward_mean: 97.99\n  episode_reward_min: 11.0\n  episodes_this_iter: 20\n  episodes_total: 337\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.30000001192092896\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5582262873649597\n          entropy_coeff: 0.0\n          kl: 0.0037608244456350803\n          model: {}\n          policy_loss: -0.012490973807871342\n          total_loss: 696.2131958007812\n          vf_explained_var: 0.2233099341392517\n          vf_loss: 696.2244873046875\n    num_agent_steps_sampled: 16000\n    num_agent_steps_trained: 16000\n    num_steps_sampled: 16000\n    num_steps_trained: 16000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 4\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 83.60769230769232\n    ram_util_percent: 85.76923076923075\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1126154093070354\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1320370369396816\n    mean_inference_ms: 2.352884663632094\n    mean_raw_obs_processing_ms: 0.2294510967493389\n  time_since_restore: 40.960214138031006\n  time_this_iter_s: 8.975828170776367\n  time_total_s: 40.960214138031006\n  timers:\n    learn_throughput: 839.249\n    learn_time_ms: 4766.169\n    load_throughput: 8117680.416\n    load_time_ms: 0.493\n    sample_throughput: 438.213\n    sample_time_ms: 9127.987\n    update_time_ms: 2.002\n  timestamp: 1638395626\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 16000\n  training_iteration: 4\n  trial_id: 0091e_00000\n  \nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 20000\n  custom_metrics: {}\n  date: 2021-12-01_22-53-56\n  done: false\n  episode_len_mean: 132.51\n  episode_media: {}\n  episode_reward_max: 500.0\n  episode_reward_mean: 132.51\n  episode_reward_min: 12.0\n  episodes_this_iter: 15\n  episodes_total: 352\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.15000000596046448\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5601204037666321\n          entropy_coeff: 0.0\n          kl: 0.0012711239978671074\n          model: {}\n          policy_loss: -0.007236114237457514\n          total_loss: 605.6217041015625\n          vf_explained_var: 0.29318979382514954\n          vf_loss: 605.6287841796875\n    num_agent_steps_sampled: 20000\n    num_agent_steps_trained: 20000\n    num_steps_sampled: 20000\n    num_steps_trained: 20000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 5\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 83.5\n    ram_util_percent: 86.91538461538461\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11232943522683439\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1322710531601683\n    mean_inference_ms: 2.318162079591458\n    mean_raw_obs_processing_ms: 0.22431872747963477\n  time_since_restore: 50.56009912490845\n  time_this_iter_s: 9.599884986877441\n  time_total_s: 50.56009912490845\n  timers:\n    learn_throughput: 851.742\n    learn_time_ms: 4696.257\n    load_throughput: 10147100.52\n    load_time_ms: 0.394\n    sample_throughput: 431.646\n    sample_time_ms: 9266.849\n    update_time_ms: 1.601\n  timestamp: 1638395636\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 20000\n  training_iteration: 5\n  trial_id: 0091e_00000\n  \nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 24000\n  custom_metrics: {}\n  date: 2021-12-01_22-54-04\n  done: false\n  episode_len_mean: 162.46\n  episode_media: {}\n  episode_reward_max: 500.0\n  episode_reward_mean: 162.46\n  episode_reward_min: 13.0\n  episodes_this_iter: 16\n  episodes_total: 368\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.07500000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5491490960121155\n          entropy_coeff: 0.0\n          kl: 0.012883742339909077\n          model: {}\n          policy_loss: -0.014221735298633575\n          total_loss: 350.70465087890625\n          vf_explained_var: 0.5025997757911682\n          vf_loss: 350.7178955078125\n    num_agent_steps_sampled: 24000\n    num_agent_steps_trained: 24000\n    num_steps_sampled: 24000\n    num_steps_trained: 24000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 6\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 69.30000000000001\n    ram_util_percent: 87.18181818181819\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11068721601459906\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1316580125011057\n    mean_inference_ms: 2.2647407230497265\n    mean_raw_obs_processing_ms: 0.21722746948586308\n  time_since_restore: 58.025999307632446\n  time_this_iter_s: 7.465900182723999\n  time_total_s: 58.025999307632446\n  timers:\n    learn_throughput: 884.584\n    learn_time_ms: 4521.899\n    load_throughput: 12176520.624\n    load_time_ms: 0.329\n    sample_throughput: 439.637\n    sample_time_ms: 9098.425\n    update_time_ms: 1.335\n  timestamp: 1638395644\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 24000\n  training_iteration: 6\n  trial_id: 0091e_00000\n  \nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 28000\n  custom_metrics: {}\n  date: 2021-12-01_22-54-12\n  done: false\n  episode_len_mean: 196.68\n  episode_media: {}\n  episode_reward_max: 500.0\n  episode_reward_mean: 196.68\n  episode_reward_min: 15.0\n  episodes_this_iter: 8\n  episodes_total: 376\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.07500000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5566104650497437\n          entropy_coeff: 0.0\n          kl: 0.0053793760016560555\n          model: {}\n          policy_loss: -0.009221607819199562\n          total_loss: 434.2621765136719\n          vf_explained_var: 0.1736932396888733\n          vf_loss: 434.2709655761719\n    num_agent_steps_sampled: 28000\n    num_agent_steps_trained: 28000\n    num_steps_sampled: 28000\n    num_steps_trained: 28000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 7\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 71.21818181818182\n    ram_util_percent: 87.13636363636364\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11012958319487443\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.13087477368874187\n    mean_inference_ms: 2.2335852221532058\n    mean_raw_obs_processing_ms: 0.2131696843540125\n  time_since_restore: 66.21467590332031\n  time_this_iter_s: 8.188676595687866\n  time_total_s: 66.21467590332031\n  timers:\n    learn_throughput: 900.369\n    learn_time_ms: 4442.624\n    load_throughput: 14205940.728\n    load_time_ms: 0.282\n    sample_throughput: 447.857\n    sample_time_ms: 8931.421\n    update_time_ms: 1.144\n  timestamp: 1638395652\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 28000\n  training_iteration: 7\n  trial_id: 0091e_00000\n  \nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 32000\n  custom_metrics: {}\n  date: 2021-12-01_22-54-21\n  done: false\n  episode_len_mean: 229.19\n  episode_media: {}\n  episode_reward_max: 500.0\n  episode_reward_mean: 229.19\n  episode_reward_min: 15.0\n  episodes_this_iter: 9\n  episodes_total: 385\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.07500000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5450154542922974\n          entropy_coeff: 0.0\n          kl: 0.0061668953858315945\n          model: {}\n          policy_loss: -0.006067643407732248\n          total_loss: 457.9305114746094\n          vf_explained_var: 0.032116785645484924\n          vf_loss: 457.9361267089844\n    num_agent_steps_sampled: 32000\n    num_agent_steps_trained: 32000\n    num_steps_sampled: 32000\n    num_steps_trained: 32000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 8\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 76.9\n    ram_util_percent: 87.25833333333333\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10949012056065484\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12963875953335788\n    mean_inference_ms: 2.2011992223423453\n    mean_raw_obs_processing_ms: 0.20929612392916075\n  time_since_restore: 74.98133444786072\n  time_this_iter_s: 8.766658544540405\n  time_total_s: 74.98133444786072\n  timers:\n    learn_throughput: 920.017\n    learn_time_ms: 4347.744\n    load_throughput: 16235360.832\n    load_time_ms: 0.246\n    sample_throughput: 446.868\n    sample_time_ms: 8951.185\n    update_time_ms: 1.001\n  timestamp: 1638395661\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 32000\n  training_iteration: 8\n  trial_id: 0091e_00000\n  \nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 36000\n  custom_metrics: {}\n  date: 2021-12-01_22-54-28\n  done: false\n  episode_len_mean: 260.25\n  episode_media: {}\n  episode_reward_max: 500.0\n  episode_reward_mean: 260.25\n  episode_reward_min: 15.0\n  episodes_this_iter: 8\n  episodes_total: 393\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.07500000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5320675373077393\n          entropy_coeff: 0.0\n          kl: 0.0075341472402215\n          model: {}\n          policy_loss: -0.0072624157182872295\n          total_loss: 404.454345703125\n          vf_explained_var: 0.05579644814133644\n          vf_loss: 404.4610290527344\n    num_agent_steps_sampled: 36000\n    num_agent_steps_trained: 36000\n    num_steps_sampled: 36000\n    num_steps_trained: 36000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 9\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 68.26999999999998\n    ram_util_percent: 87.2\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10868070777730038\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1282069423856235\n    mean_inference_ms: 2.1709855087013663\n    mean_raw_obs_processing_ms: 0.2055371812270087\n  time_since_restore: 82.31100749969482\n  time_this_iter_s: 7.3296730518341064\n  time_total_s: 82.31100749969482\n  timers:\n    learn_throughput: 939.355\n    learn_time_ms: 4258.24\n    load_throughput: 18264780.936\n    load_time_ms: 0.219\n    sample_throughput: 455.121\n    sample_time_ms: 8788.864\n    update_time_ms: 0.89\n  timestamp: 1638395668\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 36000\n  training_iteration: 9\n  trial_id: 0091e_00000\n  \nResult for PPO_CartPole-v1_0091e_00000:\n  agent_timesteps_total: 40000\n  custom_metrics: {}\n  date: 2021-12-01_22-54-36\n  done: true\n  episode_len_mean: 292.74\n  episode_media: {}\n  episode_reward_max: 500.0\n  episode_reward_mean: 292.74\n  episode_reward_min: 15.0\n  episodes_this_iter: 9\n  episodes_total: 402\n  experiment_id: 9b99b97d259948058ce175fdb437bf92\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.07500000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5234162211418152\n          entropy_coeff: 0.0\n          kl: 0.004971951246261597\n          model: {}\n          policy_loss: -0.0019533345475792885\n          total_loss: 415.5965576171875\n          vf_explained_var: 0.15562385320663452\n          vf_loss: 415.59814453125\n    num_agent_steps_sampled: 40000\n    num_agent_steps_trained: 40000\n    num_steps_sampled: 40000\n    num_steps_trained: 40000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 10\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 79.35000000000001\n    ram_util_percent: 87.0\n  pid: 16556\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10759807711359962\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12680782099974997\n    mean_inference_ms: 2.1360139834425205\n    mean_raw_obs_processing_ms: 0.20154871655205042\n  time_since_restore: 90.66206645965576\n  time_this_iter_s: 8.351058959960938\n  time_total_s: 90.66206645965576\n  timers:\n    learn_throughput: 951.379\n    learn_time_ms: 4204.422\n    load_throughput: 6695620.386\n    load_time_ms: 0.597\n    sample_throughput: 458.038\n    sample_time_ms: 8732.908\n    update_time_ms: 1.201\n  timestamp: 1638395676\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 40000\n  training_iteration: 10\n  trial_id: 0091e_00000\n  \nOption 1: Training finished successfully\n\n\n== Status ==Current time: 2021-12-01 22:53:22 (running for 00:00:43.31)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n1\n11.6133\n4000\n20.3316\n69\n8\n20.3316\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:53:27 (running for 00:00:48.40)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n1\n11.6133\n4000\n20.3316\n69\n8\n20.3316\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:53:32 (running for 00:00:53.64)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n2\n21.9354\n8000\n43.5\n128\n9\n43.5\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:53:38 (running for 00:00:59.59)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n3\n31.9844\n12000\n70.15\n292\n11\n70.15\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:53:43 (running for 00:01:04.64)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n3\n31.9844\n12000\n70.15\n292\n11\n70.15\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:53:49 (running for 00:01:10.64)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n4\n40.9602\n16000\n97.99\n371\n11\n97.99\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:53:55 (running for 00:01:15.74)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n4\n40.9602\n16000\n97.99\n371\n11\n97.99\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:54:00 (running for 00:01:21.29)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n5\n50.5601\n20000\n132.51\n500\n12\n132.51\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:54:06 (running for 00:01:26.77)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n6\n58.026\n24000\n162.46\n500\n13\n162.46\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:54:11 (running for 00:01:31.87)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n6\n58.026\n24000\n162.46\n500\n13\n162.46\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:54:16 (running for 00:01:37.04)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n7\n66.2147\n28000\n196.68\n500\n15\n196.68\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:54:23 (running for 00:01:43.79)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n8\n74.9813\n32000\n229.19\n500\n15\n229.19\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:54:28 (running for 00:01:48.84)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n8\n74.9813\n32000\n229.19\n500\n15\n229.19\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:54:34 (running for 00:01:55.21)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nRUNNING\n127.0.0.1:16556\n9\n82.311\n36000\n260.25\n500\n15\n260.25\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:54:36 (running for 00:01:57.59)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\git-repos\\private\\blog\\_notebooks\\results\\PPONumber of trials: 1/1 (1 TERMINATED)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_CartPole-v1_0091e_00000\nTERMINATED\n127.0.0.1:16556\n10\n90.6621\n40000\n292.74\n500\n15\n292.74\n\n\n\n\n\n\n\n# check and print results\ndef print_reward(results):\n    results.default_metric = \"episode_reward_mean\"\n    results.default_mode = \"max\"\n    # print mean number of time steps the pole was balanced (higher = better)\n    reward = results.best_result[\"episode_reward_mean\"]\n    print(f\"Reward after {train_iters} training iterations: {reward}\")\n\nprint_reward(results1)\n\nReward after 10 training iterations: 292.74\n\n\n\n# plot the last 100 episode rewards\nimport seaborn as sns\n\ndef plot_rewards(results):\n    \"\"\"Plot scatter plot of the last 100 training episodes\"\"\"\n    eps_rewards = results.best_result[\"hist_stats\"][\"episode_reward\"]\n    eps = [i for i in range(len(eps_rewards))]\n    ax = sns.scatterplot(eps, eps_rewards)\n    ax.set_title(\"Reward over the last 100 Episodes\")\n    ax.set_xlabel(\"Episodes\")\n    ax.set_ylabel(\"Episode Reward\")\n\n\nplot_rewards(results1)\n\nc:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\nimport os\nimport pandas as pd\n\n# plot complete learning curve based on logged progress\ndef plot_learning(results, label=None):\n    \"\"\"Plot lineplot of the mean episode reward over all training iterations\"\"\"\n    progress_path = os.path.join(results.best_logdir, \"progress.csv\")\n    df = pd.read_csv(progress_path)\n    ax = sns.lineplot(x=df[\"training_iteration\"], y=df[\"episode_reward_mean\"], label=label)\n    ax.set_title(\"Mean Episode Reward over Training Iterations\")\n\nplot_learning(results1, label=\"1: Full Observations\")\n\n\n\n\nIncluding the missing state helps the agent learn a good policy quickly, leading to high reward."
  },
  {
    "objectID": "posts/partial-observability/index.html#option-2-ignore-missing-state",
    "href": "posts/partial-observability/index.html#option-2-ignore-missing-state",
    "title": "Dealing with Partial Observability In Reinforcement Learning",
    "section": "Option 2: Ignore Missing State",
    "text": "Option 2: Ignore Missing State\nIn many practical scenarios, missing state cannot be simply added to complete the partial observations, e.g., because measuring/capturing the missing observations incurs prohibitive costs or is physically not feasible.\nIn this case, the simplest alternative is using the partial observations as they are available. This works if the observations still include enough information to learn a useful policy.\nHowever, if too much important information is missing, learning a useful policy becomes slow or even impossible. In the CartPole example, partial observations that do not include the velocity of the cart and the pole keep the agent from learning a useful policy.\n\n#collapse-output\n\nfrom ray.rllib.examples.env.stateless_cartpole import StatelessCartPole\nfrom ray.tune import registry\n\nregistry.register_env(\"StatelessCartPole\", lambda _: StatelessCartPole())\nconfig2 = ppo.DEFAULT_CONFIG.copy()\nconfig2[\"env\"] = \"StatelessCartPole\"\n# train; this takes a while\nresults2 = ray.tune.run(\"PPO\", config=config2, stop=stop)\nprint(\"Option 2: Training finished successfully\")\n\n== Status ==Current time: 2021-12-01 22:57:23 (running for 00:00:00.16)Memory usage on this node: 9.7/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nPENDING\n\n\n\n\n\n\n\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=9044) 2021-12-01 22:57:37,705  INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n(pid=9044) 2021-12-01 22:57:37,705  INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n(pid=9044) 2021-12-01 22:57:37,705  INFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=18476) 2021-12-01 22:57:53,455 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=9044) 2021-12-01 22:57:54,972  WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=9044) 2021-12-01 22:57:56,141  WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n(pid=9044) 2021-12-01 22:57:56,141  INFO trainable.py:110 -- Trainable.setup took 18.440 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n(pid=9044) 2021-12-01 22:57:56,141  WARNING util.py:57 -- Install gputil for GPU system monitoring.\n(pid=9044) 2021-12-01 22:58:00,922  WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n2021-12-01 22:59:20,276 INFO tune.py:630 -- Total run time: 116.62 seconds (116.23 seconds for the tuning loop).\n(pid=9044) [2021-12-01 22:59:20,145 E 9044 18960] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=9044) Windows fatal exception: access violation\n(pid=9044) \n(pid=18476) [2021-12-01 22:59:20,149 E 18476 12556] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=18476) Windows fatal exception: access violation\n(pid=18476) \n(pid=16556) [2021-12-01 22:59:20,148 E 16556 1448] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=16556) Windows fatal exception: access violation\n(pid=16556) \n\n\n== Status ==Current time: 2021-12-01 22:57:28 (running for 00:00:05.16)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nPENDING\n\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:57:56 (running for 00:00:32.50)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:57:57 (running for 00:00:33.51)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:02 (running for 00:00:38.63)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n\n\n\n\n\n\nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 4000\n  custom_metrics: {}\n  date: 2021-12-01_22-58-06\n  done: false\n  episode_len_mean: 22.44632768361582\n  episode_media: {}\n  episode_reward_max: 85.0\n  episode_reward_mean: 22.44632768361582\n  episode_reward_min: 8.0\n  episodes_this_iter: 177\n  episodes_total: 177\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6807681322097778\n          entropy_coeff: 0.0\n          kl: 0.012478094547986984\n          model: {}\n          policy_loss: -0.02269022725522518\n          total_loss: 180.2766876220703\n          vf_explained_var: 0.0005618375726044178\n          vf_loss: 180.296875\n    num_agent_steps_sampled: 4000\n    num_agent_steps_trained: 4000\n    num_steps_sampled: 4000\n    num_steps_trained: 4000\n  iterations_since_restore: 1\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 83.74285714285713\n    ram_util_percent: 88.00714285714285\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10327919820561605\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12135616748037466\n    mean_inference_ms: 1.9038462404123306\n    mean_raw_obs_processing_ms: 0.16696460223270435\n  time_since_restore: 10.051005840301514\n  time_this_iter_s: 10.051005840301514\n  time_total_s: 10.051005840301514\n  timers:\n    learn_throughput: 757.885\n    learn_time_ms: 5277.849\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 837.645\n    sample_time_ms: 4775.29\n    update_time_ms: 5.515\n  timestamp: 1638395886\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 4000\n  training_iteration: 1\n  trial_id: aa22d_00000\n  \nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 8000\n  custom_metrics: {}\n  date: 2021-12-01_22-58-13\n  done: false\n  episode_len_mean: 30.083333333333332\n  episode_media: {}\n  episode_reward_max: 106.0\n  episode_reward_mean: 30.083333333333332\n  episode_reward_min: 8.0\n  episodes_this_iter: 132\n  episodes_total: 309\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.648204505443573\n          entropy_coeff: 0.0\n          kl: 0.00953536108136177\n          model: {}\n          policy_loss: -0.010645464062690735\n          total_loss: 191.36209106445312\n          vf_explained_var: 0.02945260889828205\n          vf_loss: 191.37083435058594\n    num_agent_steps_sampled: 8000\n    num_agent_steps_trained: 8000\n    num_steps_sampled: 8000\n    num_steps_trained: 8000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 2\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 73.69090909090909\n    ram_util_percent: 88.29090909090907\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.09029685607221599\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1000974098193085\n    mean_inference_ms: 1.6932173327936377\n    mean_raw_obs_processing_ms: 0.18978260286703064\n  time_since_restore: 17.443942308425903\n  time_this_iter_s: 7.39293646812439\n  time_total_s: 17.443942308425903\n  timers:\n    learn_throughput: 899.218\n    learn_time_ms: 4448.31\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 577.076\n    sample_time_ms: 6931.495\n    update_time_ms: 5.261\n  timestamp: 1638395893\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 8000\n  training_iteration: 2\n  trial_id: aa22d_00000\n  \nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 12000\n  custom_metrics: {}\n  date: 2021-12-01_22-58-24\n  done: false\n  episode_len_mean: 37.31481481481482\n  episode_media: {}\n  episode_reward_max: 143.0\n  episode_reward_mean: 37.31481481481482\n  episode_reward_min: 9.0\n  episodes_this_iter: 108\n  episodes_total: 417\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6112045049667358\n          entropy_coeff: 0.0\n          kl: 0.006910913623869419\n          model: {}\n          policy_loss: -0.015092005021870136\n          total_loss: 245.5015411376953\n          vf_explained_var: 0.021608643233776093\n          vf_loss: 245.5152587890625\n    num_agent_steps_sampled: 12000\n    num_agent_steps_trained: 12000\n    num_steps_sampled: 12000\n    num_steps_trained: 12000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 3\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 92.86\n    ram_util_percent: 88.22666666666667\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10518439466006081\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11606732866669264\n    mean_inference_ms: 1.8527932982897477\n    mean_raw_obs_processing_ms: 0.19247182033120946\n  time_since_restore: 28.011292934417725\n  time_this_iter_s: 10.567350625991821\n  time_total_s: 28.011292934417725\n  timers:\n    learn_throughput: 856.291\n    learn_time_ms: 4671.309\n    load_throughput: 11972323.501\n    load_time_ms: 0.334\n    sample_throughput: 522.229\n    sample_time_ms: 7659.481\n    update_time_ms: 4.504\n  timestamp: 1638395904\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 12000\n  training_iteration: 3\n  trial_id: aa22d_00000\n  \nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 16000\n  custom_metrics: {}\n  date: 2021-12-01_22-58-32\n  done: false\n  episode_len_mean: 42.79\n  episode_media: {}\n  episode_reward_max: 152.0\n  episode_reward_mean: 42.79\n  episode_reward_min: 10.0\n  episodes_this_iter: 94\n  episodes_total: 511\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5654925107955933\n          entropy_coeff: 0.0\n          kl: 0.004174998961389065\n          model: {}\n          policy_loss: -0.012154466472566128\n          total_loss: 252.0902862548828\n          vf_explained_var: 0.03405797854065895\n          vf_loss: 252.1016082763672\n    num_agent_steps_sampled: 16000\n    num_agent_steps_trained: 16000\n    num_steps_sampled: 16000\n    num_steps_trained: 16000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 4\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 79.5\n    ram_util_percent: 86.57272727272728\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10566344965230401\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12007891816714177\n    mean_inference_ms: 1.8777825593616126\n    mean_raw_obs_processing_ms: 0.19165146846813094\n  time_since_restore: 36.43282437324524\n  time_this_iter_s: 8.421531438827515\n  time_total_s: 36.43282437324524\n  timers:\n    learn_throughput: 915.336\n    learn_time_ms: 4369.982\n    load_throughput: 15963098.002\n    load_time_ms: 0.251\n    sample_throughput: 483.585\n    sample_time_ms: 8271.563\n    update_time_ms: 3.884\n  timestamp: 1638395912\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 16000\n  training_iteration: 4\n  trial_id: aa22d_00000\n  \nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 20000\n  custom_metrics: {}\n  date: 2021-12-01_22-58-39\n  done: false\n  episode_len_mean: 43.32\n  episode_media: {}\n  episode_reward_max: 133.0\n  episode_reward_mean: 43.32\n  episode_reward_min: 11.0\n  episodes_this_iter: 90\n  episodes_total: 601\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.10000000149011612\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5446197986602783\n          entropy_coeff: 0.0\n          kl: 0.004509765654802322\n          model: {}\n          policy_loss: -0.005693237762898207\n          total_loss: 206.11839294433594\n          vf_explained_var: 0.1073232963681221\n          vf_loss: 206.1236572265625\n    num_agent_steps_sampled: 20000\n    num_agent_steps_trained: 20000\n    num_steps_sampled: 20000\n    num_steps_trained: 20000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 5\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 69.06000000000002\n    ram_util_percent: 86.75\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10193395594952362\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11727529060495807\n    mean_inference_ms: 1.7989170802559789\n    mean_raw_obs_processing_ms: 0.18388387052508867\n  time_since_restore: 43.44782853126526\n  time_this_iter_s: 7.0150041580200195\n  time_total_s: 43.44782853126526\n  timers:\n    learn_throughput: 957.628\n    learn_time_ms: 4176.985\n    load_throughput: 19953872.502\n    load_time_ms: 0.2\n    sample_throughput: 497.425\n    sample_time_ms: 8041.409\n    update_time_ms: 3.708\n  timestamp: 1638395919\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 20000\n  training_iteration: 5\n  trial_id: aa22d_00000\n  \nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 24000\n  custom_metrics: {}\n  date: 2021-12-01_22-58-48\n  done: false\n  episode_len_mean: 47.98\n  episode_media: {}\n  episode_reward_max: 159.0\n  episode_reward_mean: 47.98\n  episode_reward_min: 11.0\n  episodes_this_iter: 81\n  episodes_total: 682\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.05000000074505806\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5157366991043091\n          entropy_coeff: 0.0\n          kl: 0.002469780156388879\n          model: {}\n          policy_loss: -0.00011549380724318326\n          total_loss: 223.69801330566406\n          vf_explained_var: 0.14510144293308258\n          vf_loss: 223.697998046875\n    num_agent_steps_sampled: 24000\n    num_agent_steps_trained: 24000\n    num_steps_sampled: 24000\n    num_steps_trained: 24000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 6\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 77.66666666666667\n    ram_util_percent: 86.87499999999999\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10315175733432268\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11747585144831862\n    mean_inference_ms: 1.796743386961408\n    mean_raw_obs_processing_ms: 0.18361450972900276\n  time_since_restore: 52.08714461326599\n  time_this_iter_s: 8.639316082000732\n  time_total_s: 52.08714461326599\n  timers:\n    learn_throughput: 963.146\n    learn_time_ms: 4153.057\n    load_throughput: 23944647.003\n    load_time_ms: 0.167\n    sample_throughput: 497.199\n    sample_time_ms: 8045.069\n    update_time_ms: 5.866\n  timestamp: 1638395928\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 24000\n  training_iteration: 6\n  trial_id: aa22d_00000\n  \nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 28000\n  custom_metrics: {}\n  date: 2021-12-01_22-58-56\n  done: false\n  episode_len_mean: 50.24\n  episode_media: {}\n  episode_reward_max: 159.0\n  episode_reward_mean: 50.24\n  episode_reward_min: 11.0\n  episodes_this_iter: 80\n  episodes_total: 762\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.02500000037252903\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.4738757014274597\n          entropy_coeff: 0.0\n          kl: 0.005073909182101488\n          model: {}\n          policy_loss: 0.00443687941879034\n          total_loss: 240.7548065185547\n          vf_explained_var: 0.14891892671585083\n          vf_loss: 240.75022888183594\n    num_agent_steps_sampled: 28000\n    num_agent_steps_trained: 28000\n    num_steps_sampled: 28000\n    num_steps_trained: 28000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 7\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 72.13636363636364\n    ram_util_percent: 86.82727272727271\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10310251236597129\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11948908895760547\n    mean_inference_ms: 1.781021410338414\n    mean_raw_obs_processing_ms: 0.18258329446260468\n  time_since_restore: 59.83448100090027\n  time_this_iter_s: 7.747336387634277\n  time_total_s: 59.83448100090027\n  timers:\n    learn_throughput: 983.057\n    learn_time_ms: 4068.942\n    load_throughput: 27935421.503\n    load_time_ms: 0.143\n    sample_throughput: 495.114\n    sample_time_ms: 8078.951\n    update_time_ms: 5.028\n  timestamp: 1638395936\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 28000\n  training_iteration: 7\n  trial_id: aa22d_00000\n  \nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 32000\n  custom_metrics: {}\n  date: 2021-12-01_22-59-04\n  done: false\n  episode_len_mean: 50.37\n  episode_media: {}\n  episode_reward_max: 155.0\n  episode_reward_mean: 50.37\n  episode_reward_min: 9.0\n  episodes_this_iter: 81\n  episodes_total: 843\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.02500000037252903\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.44857272505760193\n          entropy_coeff: 0.0\n          kl: 0.005331501364707947\n          model: {}\n          policy_loss: -0.00537552684545517\n          total_loss: 236.2506103515625\n          vf_explained_var: 0.16449585556983948\n          vf_loss: 236.25584411621094\n    num_agent_steps_sampled: 32000\n    num_agent_steps_trained: 32000\n    num_steps_sampled: 32000\n    num_steps_trained: 32000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 8\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 73.15454545454546\n    ram_util_percent: 86.82727272727271\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10669802327244614\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11682882882097279\n    mean_inference_ms: 1.7686794144275058\n    mean_raw_obs_processing_ms: 0.1813271875273223\n  time_since_restore: 67.72298955917358\n  time_this_iter_s: 7.888508558273315\n  time_total_s: 67.72298955917358\n  timers:\n    learn_throughput: 996.781\n    learn_time_ms: 4012.918\n    load_throughput: 31926196.004\n    load_time_ms: 0.125\n    sample_throughput: 496.729\n    sample_time_ms: 8052.687\n    update_time_ms: 4.527\n  timestamp: 1638395944\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 32000\n  training_iteration: 8\n  trial_id: aa22d_00000\n  \nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 36000\n  custom_metrics: {}\n  date: 2021-12-01_22-59-12\n  done: false\n  episode_len_mean: 49.87\n  episode_media: {}\n  episode_reward_max: 110.0\n  episode_reward_mean: 49.87\n  episode_reward_min: 11.0\n  episodes_this_iter: 81\n  episodes_total: 924\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.02500000037252903\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.42752259969711304\n          entropy_coeff: 0.0\n          kl: 0.005028429441154003\n          model: {}\n          policy_loss: -0.0017633900279179215\n          total_loss: 193.06703186035156\n          vf_explained_var: 0.2048284411430359\n          vf_loss: 193.06866455078125\n    num_agent_steps_sampled: 36000\n    num_agent_steps_trained: 36000\n    num_steps_sampled: 36000\n    num_steps_trained: 36000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 9\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 75.8\n    ram_util_percent: 86.93333333333334\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10545672991997837\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11722357639711418\n    mean_inference_ms: 1.7522968685832132\n    mean_raw_obs_processing_ms: 0.17896026200224338\n  time_since_restore: 75.84080076217651\n  time_this_iter_s: 8.11781120300293\n  time_total_s: 75.84080076217651\n  timers:\n    learn_throughput: 994.383\n    learn_time_ms: 4022.594\n    load_throughput: 35916970.504\n    load_time_ms: 0.111\n    sample_throughput: 499.317\n    sample_time_ms: 8010.943\n    update_time_ms: 4.024\n  timestamp: 1638395952\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 36000\n  training_iteration: 9\n  trial_id: aa22d_00000\n  \nResult for PPO_StatelessCartPole_aa22d_00000:\n  agent_timesteps_total: 40000\n  custom_metrics: {}\n  date: 2021-12-01_22-59-19\n  done: true\n  episode_len_mean: 46.75\n  episode_media: {}\n  episode_reward_max: 125.0\n  episode_reward_mean: 46.75\n  episode_reward_min: 13.0\n  episodes_this_iter: 84\n  episodes_total: 1008\n  experiment_id: 99df0008334f43779394474d46d27ce1\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.02500000037252903\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.45696982741355896\n          entropy_coeff: 0.0\n          kl: 0.0024534217081964016\n          model: {}\n          policy_loss: 0.0026924554258584976\n          total_loss: 184.4345245361328\n          vf_explained_var: 0.2629404664039612\n          vf_loss: 184.43174743652344\n    num_agent_steps_sampled: 40000\n    num_agent_steps_trained: 40000\n    num_steps_sampled: 40000\n    num_steps_trained: 40000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 10\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 70.31\n    ram_util_percent: 86.9\n  pid: 9044\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10223809426816181\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11759765514824477\n    mean_inference_ms: 1.7250900206764819\n    mean_raw_obs_processing_ms: 0.17782050917712555\n  time_since_restore: 83.26664853096008\n  time_this_iter_s: 7.425847768783569\n  time_total_s: 83.26664853096008\n  timers:\n    learn_throughput: 1002.298\n    learn_time_ms: 3990.829\n    load_throughput: 7987248.75\n    load_time_ms: 0.501\n    sample_throughput: 500.117\n    sample_time_ms: 7998.131\n    update_time_ms: 3.621\n  timestamp: 1638395959\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 40000\n  training_iteration: 10\n  trial_id: aa22d_00000\n  \nOption 2: Training finished successfully\n\n\n== Status ==Current time: 2021-12-01 22:58:08 (running for 00:00:44.63)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n1\n10.051\n4000\n22.4463\n85\n8\n22.4463\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:13 (running for 00:00:49.73)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n1\n10.051\n4000\n22.4463\n85\n8\n22.4463\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:18 (running for 00:00:55.09)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n2\n17.4439\n8000\n30.0833\n106\n8\n30.0833\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:23 (running for 00:01:00.16)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n2\n17.4439\n8000\n30.0833\n106\n8\n30.0833\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:29 (running for 00:01:05.71)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n3\n28.0113\n12000\n37.3148\n143\n9\n37.3148\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:34 (running for 00:01:11.09)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n4\n36.4328\n16000\n42.79\n152\n10\n42.79\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:39 (running for 00:01:16.14)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n5\n43.4478\n20000\n43.32\n133\n11\n43.32\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:44 (running for 00:01:21.20)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n5\n43.4478\n20000\n43.32\n133\n11\n43.32\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:50 (running for 00:01:26.88)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n6\n52.0871\n24000\n47.98\n159\n11\n47.98\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:58:55 (running for 00:01:31.95)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n6\n52.0871\n24000\n47.98\n159\n11\n47.98\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:59:01 (running for 00:01:37.71)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n7\n59.8345\n28000\n50.24\n159\n11\n50.24\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:59:07 (running for 00:01:43.60)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n8\n67.723\n32000\n50.37\n155\n9\n50.37\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:59:12 (running for 00:01:48.70)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n8\n67.723\n32000\n50.37\n155\n9\n50.37\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:59:17 (running for 00:01:53.79)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nRUNNING\n127.0.0.1:9044\n9\n75.8408\n36000\n49.87\n110\n11\n49.87\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 22:59:19 (running for 00:01:56.27)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 TERMINATED)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_aa22d_00000\nTERMINATED\n127.0.0.1:9044\n10\n83.2666\n40000\n46.75\n125\n13\n46.75\n\n\n\n\n\n\n\nprint_reward(results2)\n\nReward after 10 training iterations: 46.75\n\n\n\nplot_rewards(results2)\n\nc:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\n# compare learning curves\nplot_learning(results1, label=\"1: Full Observations\")\nplot_learning(results2, label=\"2: Partial Observations\")\n\n\n\n\nWith only the partial observations, i.e., without observing velocity, the RL agent does not learn a useful policy. The reward does not increase notably over time and the resulting episode reward is much smaller than with full obsevations."
  },
  {
    "objectID": "posts/partial-observability/index.html#option-3-use-sequence-of-last-observations",
    "href": "posts/partial-observability/index.html#option-3-use-sequence-of-last-observations",
    "title": "Dealing with Partial Observability In Reinforcement Learning",
    "section": "Option 3: Use Sequence of Last Observations",
    "text": "Option 3: Use Sequence of Last Observations\nEven if the velocity of cart and pole are not explicitly available in this example, it can be derived by the RL agent by looking at a sequence of previous observations. If the cart is always at the same position, its velocity is likely close to zero. If its position varies greatly, it likely has high velocity.\nHence, one useful approach is to simply stack the last \\(n\\) observations and providing this sequence as input to the RL agent.\n\nOption 3a: Use Raw Sequence as Input\nHere, I consider the same default feed-forward neural network with PPO, just providing the stacked, partial observations as input.\n\nStacking Observations Using Gym’s FrameStack Wrapper\nTo stack the last \\(n\\) observations, I use Gym’s FrameStack wrapper. As an example, I choose \\(n=4\\).\n\nfrom gym.wrappers import FrameStack\n\nNUM_FRAMES = 4\n\n# stateless CartPole --&gt; only 2 observations: position of cart & angle of pole (not: velocity of cart or pole)\nenv = StatelessCartPole()\nprint(f\"Shape of observation space (stateless CartPole): {env.observation_space.shape}\")\n\n# stack last n observations into sequence --&gt; n x 2\nenv_stacked = FrameStack(env, NUM_FRAMES)\nprint(f\"Shape of observation space (stacked stateless CartPole): {env_stacked.observation_space.shape}\")\n\n# register env for RLlib\nregistry.register_env(\"StackedStatelessCartPole\", lambda _: FrameStack(StatelessCartPole(), NUM_FRAMES))\n\nShape of observation space (stateless CartPole): (2,)\nShape of observation space (stacked stateless CartPole): (4, 2)\n\n\n\n#collapse-output\n\n# use PPO with vanilla MLP\nconfig3a = ppo.DEFAULT_CONFIG.copy()\nconfig3a[\"env\"] = \"StackedStatelessCartPole\"\n# train; this takes a while\nresults3a = ray.tune.run(\"PPO\", config=config3a, stop=stop)\nprint(\"Option 3a with FrameStack: Training finished successfully\")\n\n== Status ==Current time: 2021-12-01 23:02:44 (running for 00:00:00.15)Memory usage on this node: 9.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nPENDING\n\n\n\n\n\n\n\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=13456) 2021-12-01 23:03:00,839 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n(pid=13456) 2021-12-01 23:03:00,839 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n(pid=13456) 2021-12-01 23:03:00,839 INFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=19996) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\gym\\spaces\\box.py:142: UserWarning: WARN: Casting input x to numpy array.\n(pid=19996)   logger.warn(\"Casting input x to numpy array.\")\n(pid=3484) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\gym\\spaces\\box.py:142: UserWarning: WARN: Casting input x to numpy array.\n(pid=3484)   logger.warn(\"Casting input x to numpy array.\")\n(pid=3484) 2021-12-01 23:03:17,245  WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=13456) 2021-12-01 23:03:19,489 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=13456) 2021-12-01 23:03:20,834 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n(pid=13456) 2021-12-01 23:03:20,834 INFO trainable.py:110 -- Trainable.setup took 19.995 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n(pid=13456) 2021-12-01 23:03:20,839 WARNING util.py:57 -- Install gputil for GPU system monitoring.\n(pid=13456) 2021-12-01 23:03:26,389 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n2021-12-01 23:04:57,071 INFO tune.py:630 -- Total run time: 132.65 seconds (132.43 seconds for the tuning loop).ayletClient] Failed to disconnect from raylet.\n\n(pid=13456) Windows fatal exception: access violation\n(pid=13456) \n(pid=19996) [2021-12-01 23:04:56,970 E 19996 3460] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=19996) Windows fatal exception: access violation\n(pid=19996) \n(pid=3484) [2021-12-01 23:04:56,971 E 3484 17628] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=3484) Windows fatal exception: access violation\n(pid=3484) \n\n\n== Status ==Current time: 2021-12-01 23:02:49 (running for 00:00:05.15)Memory usage on this node: 9.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nPENDING\n\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:03:20 (running for 00:00:36.40)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:03:21 (running for 00:00:37.50)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:03:28 (running for 00:00:43.61)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n\n\n\n\n\n\nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 4000\n  custom_metrics: {}\n  date: 2021-12-01_23-03-31\n  done: false\n  episode_len_mean: 20.91578947368421\n  episode_media: {}\n  episode_reward_max: 61.0\n  episode_reward_mean: 20.91578947368421\n  episode_reward_min: 8.0\n  episodes_this_iter: 190\n  episodes_total: 190\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6832552552223206\n          entropy_coeff: 0.0\n          kl: 0.010132171213626862\n          model: {}\n          policy_loss: -0.017918335273861885\n          total_loss: 126.10237121582031\n          vf_explained_var: 0.01804439164698124\n          vf_loss: 126.1182632446289\n    num_agent_steps_sampled: 4000\n    num_agent_steps_trained: 4000\n    num_steps_sampled: 4000\n    num_steps_trained: 4000\n  iterations_since_restore: 1\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 80.80666666666666\n    ram_util_percent: 85.86666666666669\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11120850849435063\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.18972639138395245\n    mean_inference_ms: 2.0335766275739453\n    mean_raw_obs_processing_ms: 0.3751123260983276\n  time_since_restore: 10.363371133804321\n  time_this_iter_s: 10.363371133804321\n  time_total_s: 10.363371133804321\n  timers:\n    learn_throughput: 833.897\n    learn_time_ms: 4796.753\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 719.056\n    sample_time_ms: 5562.852\n    update_time_ms: 5.016\n  timestamp: 1638396211\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 4000\n  training_iteration: 1\n  trial_id: '69565_00000'\n  \nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 8000\n  custom_metrics: {}\n  date: 2021-12-01_23-03-40\n  done: false\n  episode_len_mean: 29.455882352941178\n  episode_media: {}\n  episode_reward_max: 136.0\n  episode_reward_mean: 29.455882352941178\n  episode_reward_min: 8.0\n  episodes_this_iter: 136\n  episodes_total: 326\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6507855653762817\n          entropy_coeff: 0.0\n          kl: 0.010778849013149738\n          model: {}\n          policy_loss: -0.01948031783103943\n          total_loss: 162.9302215576172\n          vf_explained_var: 0.03349286690354347\n          vf_loss: 162.94754028320312\n    num_agent_steps_sampled: 8000\n    num_agent_steps_trained: 8000\n    num_steps_sampled: 8000\n    num_steps_trained: 8000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 2\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 84.03076923076924\n    ram_util_percent: 85.8076923076923\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10394749777843926\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15974853905888772\n    mean_inference_ms: 1.9997662326250156\n    mean_raw_obs_processing_ms: 0.3263675950258694\n  time_since_restore: 20.015674591064453\n  time_this_iter_s: 9.652303457260132\n  time_total_s: 20.015674591064453\n  timers:\n    learn_throughput: 849.072\n    learn_time_ms: 4711.025\n    load_throughput: 7966389.364\n    load_time_ms: 0.502\n    sample_throughput: 518.103\n    sample_time_ms: 7720.474\n    update_time_ms: 4.509\n  timestamp: 1638396220\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 8000\n  training_iteration: 2\n  trial_id: '69565_00000'\n  \nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 12000\n  custom_metrics: {}\n  date: 2021-12-01_23-03-49\n  done: false\n  episode_len_mean: 45.47\n  episode_media: {}\n  episode_reward_max: 200.0\n  episode_reward_mean: 45.47\n  episode_reward_min: 9.0\n  episodes_this_iter: 83\n  episodes_total: 409\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6116749048233032\n          entropy_coeff: 0.0\n          kl: 0.0092014130204916\n          model: {}\n          policy_loss: -0.017703521996736526\n          total_loss: 372.5731201171875\n          vf_explained_var: 0.04724571481347084\n          vf_loss: 372.5889892578125\n    num_agent_steps_sampled: 12000\n    num_agent_steps_trained: 12000\n    num_steps_sampled: 12000\n    num_steps_trained: 12000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 3\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 79.78333333333333\n    ram_util_percent: 85.93333333333334\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.09840720456414369\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15510985166002864\n    mean_inference_ms: 1.9296228234529118\n    mean_raw_obs_processing_ms: 0.29812654395886573\n  time_since_restore: 28.900269746780396\n  time_this_iter_s: 8.884595155715942\n  time_total_s: 28.900269746780396\n  timers:\n    learn_throughput: 868.178\n    learn_time_ms: 4607.352\n    load_throughput: 5989010.947\n    load_time_ms: 0.668\n    sample_throughput: 488.004\n    sample_time_ms: 8196.646\n    update_time_ms: 4.004\n  timestamp: 1638396229\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 12000\n  training_iteration: 3\n  trial_id: '69565_00000'\n  \nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 16000\n  custom_metrics: {}\n  date: 2021-12-01_23-04-00\n  done: false\n  episode_len_mean: 63.03\n  episode_media: {}\n  episode_reward_max: 272.0\n  episode_reward_mean: 63.03\n  episode_reward_min: 13.0\n  episodes_this_iter: 51\n  episodes_total: 460\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5801236629486084\n          entropy_coeff: 0.0\n          kl: 0.006844064686447382\n          model: {}\n          policy_loss: -0.009995924308896065\n          total_loss: 404.9743957519531\n          vf_explained_var: 0.09591271728277206\n          vf_loss: 404.9830627441406\n    num_agent_steps_sampled: 16000\n    num_agent_steps_trained: 16000\n    num_steps_sampled: 16000\n    num_steps_trained: 16000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 4\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 88.04\n    ram_util_percent: 85.95333333333335\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.0999835854110982\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15354213307036435\n    mean_inference_ms: 1.9525573766667057\n    mean_raw_obs_processing_ms: 0.29008862922916356\n  time_since_restore: 39.33686113357544\n  time_this_iter_s: 10.436591386795044\n  time_total_s: 39.33686113357544\n  timers:\n    learn_throughput: 860.917\n    learn_time_ms: 4646.208\n    load_throughput: 7985347.93\n    load_time_ms: 0.501\n    sample_throughput: 461.014\n    sample_time_ms: 8676.526\n    update_time_ms: 3.003\n  timestamp: 1638396240\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 16000\n  training_iteration: 4\n  trial_id: '69565_00000'\n  \nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 20000\n  custom_metrics: {}\n  date: 2021-12-01_23-04-09\n  done: false\n  episode_len_mean: 83.03\n  episode_media: {}\n  episode_reward_max: 272.0\n  episode_reward_mean: 83.03\n  episode_reward_min: 10.0\n  episodes_this_iter: 41\n  episodes_total: 501\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5570896863937378\n          entropy_coeff: 0.0\n          kl: 0.00568711943924427\n          model: {}\n          policy_loss: -0.01232148241251707\n          total_loss: 408.25262451171875\n          vf_explained_var: 0.11371473968029022\n          vf_loss: 408.26385498046875\n    num_agent_steps_sampled: 20000\n    num_agent_steps_trained: 20000\n    num_steps_sampled: 20000\n    num_steps_trained: 20000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 5\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 79.44615384615385\n    ram_util_percent: 85.80769230769229\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10660624354014989\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.149048118877094\n    mean_inference_ms: 1.9695397741116307\n    mean_raw_obs_processing_ms: 0.28367179033707196\n  time_since_restore: 48.41154980659485\n  time_this_iter_s: 9.07468867301941\n  time_total_s: 48.41154980659485\n  timers:\n    learn_throughput: 873.938\n    learn_time_ms: 4576.986\n    load_throughput: 9981684.912\n    load_time_ms: 0.401\n    sample_throughput: 451.554\n    sample_time_ms: 8858.292\n    update_time_ms: 2.403\n  timestamp: 1638396249\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 20000\n  training_iteration: 5\n  trial_id: '69565_00000'\n  \nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 24000\n  custom_metrics: {}\n  date: 2021-12-01_23-04-19\n  done: false\n  episode_len_mean: 102.34\n  episode_media: {}\n  episode_reward_max: 304.0\n  episode_reward_mean: 102.34\n  episode_reward_min: 10.0\n  episodes_this_iter: 24\n  episodes_total: 525\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5555164217948914\n          entropy_coeff: 0.0\n          kl: 0.004422472789883614\n          model: {}\n          policy_loss: -0.008869567885994911\n          total_loss: 570.493896484375\n          vf_explained_var: 0.24427081644535065\n          vf_loss: 570.5018920898438\n    num_agent_steps_sampled: 24000\n    num_agent_steps_trained: 24000\n    num_steps_sampled: 24000\n    num_steps_trained: 24000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 6\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 85.06923076923076\n    ram_util_percent: 85.82307692307693\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10851443555266879\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.14933858568901648\n    mean_inference_ms: 1.9681416365815767\n    mean_raw_obs_processing_ms: 0.28052371436443274\n  time_since_restore: 57.97208309173584\n  time_this_iter_s: 9.560533285140991\n  time_total_s: 57.97208309173584\n  timers:\n    learn_throughput: 878.415\n    learn_time_ms: 4553.655\n    load_throughput: 11978021.894\n    load_time_ms: 0.334\n    sample_throughput: 446.643\n    sample_time_ms: 8955.693\n    update_time_ms: 2.669\n  timestamp: 1638396259\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 24000\n  training_iteration: 6\n  trial_id: '69565_00000'\n  \nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 28000\n  custom_metrics: {}\n  date: 2021-12-01_23-04-27\n  done: false\n  episode_len_mean: 127.8\n  episode_media: {}\n  episode_reward_max: 321.0\n  episode_reward_mean: 127.8\n  episode_reward_min: 10.0\n  episodes_this_iter: 23\n  episodes_total: 548\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.10000000149011612\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5434969067573547\n          entropy_coeff: 0.0\n          kl: 0.008256432600319386\n          model: {}\n          policy_loss: -0.0062043326906859875\n          total_loss: 453.47607421875\n          vf_explained_var: 0.3077850043773651\n          vf_loss: 453.4814758300781\n    num_agent_steps_sampled: 28000\n    num_agent_steps_trained: 28000\n    num_steps_sampled: 28000\n    num_steps_trained: 28000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 7\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 79.55833333333334\n    ram_util_percent: 85.89166666666667\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.110027565885569\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.14780355323603098\n    mean_inference_ms: 1.9526956031732303\n    mean_raw_obs_processing_ms: 0.27615972972533087\n  time_since_restore: 66.65037989616394\n  time_this_iter_s: 8.6782968044281\n  time_total_s: 66.65037989616394\n  timers:\n    learn_throughput: 887.707\n    learn_time_ms: 4505.99\n    load_throughput: 13974358.877\n    load_time_ms: 0.286\n    sample_throughput: 446.514\n    sample_time_ms: 8958.288\n    update_time_ms: 2.859\n  timestamp: 1638396267\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 28000\n  training_iteration: 7\n  trial_id: '69565_00000'\n  \nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 32000\n  custom_metrics: {}\n  date: 2021-12-01_23-04-38\n  done: false\n  episode_len_mean: 145.51\n  episode_media: {}\n  episode_reward_max: 392.0\n  episode_reward_mean: 145.51\n  episode_reward_min: 10.0\n  episodes_this_iter: 26\n  episodes_total: 574\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.10000000149011612\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5463652014732361\n          entropy_coeff: 0.0\n          kl: 0.010875530540943146\n          model: {}\n          policy_loss: -0.007964679040014744\n          total_loss: 391.5842590332031\n          vf_explained_var: 0.35197633504867554\n          vf_loss: 391.5911865234375\n    num_agent_steps_sampled: 32000\n    num_agent_steps_trained: 32000\n    num_steps_sampled: 32000\n    num_steps_trained: 32000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 8\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 87.83571428571429\n    ram_util_percent: 85.70000000000002\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11137697557468901\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.14710154952868693\n    mean_inference_ms: 1.9442323273031195\n    mean_raw_obs_processing_ms: 0.27244073066886854\n  time_since_restore: 76.93640422821045\n  time_this_iter_s: 10.286024332046509\n  time_total_s: 76.93640422821045\n  timers:\n    learn_throughput: 875.973\n    learn_time_ms: 4566.348\n    load_throughput: 15970695.859\n    load_time_ms: 0.25\n    sample_throughput: 442.806\n    sample_time_ms: 9033.302\n    update_time_ms: 2.879\n  timestamp: 1638396278\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 32000\n  training_iteration: 8\n  trial_id: '69565_00000'\n  \nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 36000\n  custom_metrics: {}\n  date: 2021-12-01_23-04-48\n  done: false\n  episode_len_mean: 164.14\n  episode_media: {}\n  episode_reward_max: 392.0\n  episode_reward_mean: 164.14\n  episode_reward_min: 13.0\n  episodes_this_iter: 20\n  episodes_total: 594\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.10000000149011612\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5408887267112732\n          entropy_coeff: 0.0\n          kl: 0.011410081759095192\n          model: {}\n          policy_loss: -0.013954582624137402\n          total_loss: 419.84454345703125\n          vf_explained_var: 0.34064534306526184\n          vf_loss: 419.8573913574219\n    num_agent_steps_sampled: 36000\n    num_agent_steps_trained: 36000\n    num_steps_sampled: 36000\n    num_steps_trained: 36000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 9\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 84.79285714285716\n    ram_util_percent: 85.70000000000003\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1111877406329047\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.14667458354156745\n    mean_inference_ms: 1.9476374535757386\n    mean_raw_obs_processing_ms: 0.2706214096819341\n  time_since_restore: 86.94219470024109\n  time_this_iter_s: 10.00579047203064\n  time_total_s: 86.94219470024109\n  timers:\n    learn_throughput: 881.266\n    learn_time_ms: 4538.926\n    load_throughput: 5140953.457\n    load_time_ms: 0.778\n    sample_throughput: 433.901\n    sample_time_ms: 9218.702\n    update_time_ms: 2.559\n  timestamp: 1638396288\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 36000\n  training_iteration: 9\n  trial_id: '69565_00000'\n  \nResult for PPO_StackedStatelessCartPole_69565_00000:\n  agent_timesteps_total: 40000\n  custom_metrics: {}\n  date: 2021-12-01_23-04-56\n  done: true\n  episode_len_mean: 176.48\n  episode_media: {}\n  episode_reward_max: 392.0\n  episode_reward_mean: 176.48\n  episode_reward_min: 13.0\n  episodes_this_iter: 19\n  episodes_total: 613\n  experiment_id: dad4489332ba46c8ab9c9ed834879afb\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.10000000149011612\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5288471579551697\n          entropy_coeff: 0.0\n          kl: 0.008251729421317577\n          model: {}\n          policy_loss: -0.007646023295819759\n          total_loss: 273.2549743652344\n          vf_explained_var: 0.5383354425430298\n          vf_loss: 273.2617492675781\n    num_agent_steps_sampled: 40000\n    num_agent_steps_trained: 40000\n    num_steps_sampled: 40000\n    num_steps_trained: 40000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 10\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 80.93333333333332\n    ram_util_percent: 86.14999999999999\n  pid: 13456\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11016966943400075\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.14657990293613932\n    mean_inference_ms: 1.948797938913841\n    mean_raw_obs_processing_ms: 0.2673978140474405\n  time_since_restore: 95.56249117851257\n  time_this_iter_s: 8.620296478271484\n  time_total_s: 95.56249117851257\n  timers:\n    learn_throughput: 893.185\n    learn_time_ms: 4478.354\n    load_throughput: 5712170.508\n    load_time_ms: 0.7\n    sample_throughput: 434.433\n    sample_time_ms: 9207.403\n    update_time_ms: 2.303\n  timestamp: 1638396296\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 40000\n  training_iteration: 10\n  trial_id: '69565_00000'\n  \nOption 3a with FrameStack: Training finished successfully\n\n\n== Status ==Current time: 2021-12-01 23:03:33 (running for 00:00:48.84)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n1\n10.3634\n4000\n20.9158\n61\n8\n20.9158\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:03:38 (running for 00:00:53.93)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n1\n10.3634\n4000\n20.9158\n61\n8\n20.9158\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:03:44 (running for 00:00:59.57)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n2\n20.0157\n8000\n29.4559\n136\n8\n29.4559\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:03:49 (running for 00:01:04.66)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n2\n20.0157\n8000\n29.4559\n136\n8\n29.4559\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:03:55 (running for 00:01:10.55)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n3\n28.9003\n12000\n45.47\n200\n9\n45.47\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:00 (running for 00:01:15.68)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n3\n28.9003\n12000\n45.47\n200\n9\n45.47\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:05 (running for 00:01:21.06)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n4\n39.3369\n16000\n63.03\n272\n13\n63.03\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:11 (running for 00:01:27.06)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n5\n48.4115\n20000\n83.03\n272\n10\n83.03\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:16 (running for 00:01:32.16)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n5\n48.4115\n20000\n83.03\n272\n10\n83.03\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:22 (running for 00:01:37.68)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n6\n57.9721\n24000\n102.34\n304\n10\n102.34\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:27 (running for 00:01:42.89)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n6\n57.9721\n24000\n102.34\n304\n10\n102.34\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:32 (running for 00:01:48.46)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n7\n66.6504\n28000\n127.8\n321\n10\n127.8\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:38 (running for 00:01:53.54)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n7\n66.6504\n28000\n127.8\n321\n10\n127.8\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:43 (running for 00:01:58.76)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n8\n76.9364\n32000\n145.51\n392\n10\n145.51\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:49 (running for 00:02:04.76)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n9\n86.9422\n36000\n164.14\n392\n13\n164.14\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:56 (running for 00:02:11.83)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nRUNNING\n127.0.0.1:13456\n9\n86.9422\n36000\n164.14\n392\n13\n164.14\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:04:56 (running for 00:02:12.48)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 TERMINATED)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_69565_00000\nTERMINATED\n127.0.0.1:13456\n10\n95.5625\n40000\n176.48\n392\n13\n176.48\n\n\n\n\n\n\n\nprint_reward(results3a)\n\nReward after 10 training iterations: 176.48\n\n\n\nplot_rewards(results3a)\n\nc:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\nplot_learning(results1, label=\"1: Full Observations\")\nplot_learning(results2, label=\"2: Partial Observations\")\nplot_learning(results3a, label=\"3a: Stacked, Partial Observations\")\n\n\n\n\nSimply by stacking the last \\(n\\) observations, the RL agent learns a useful policy again - even though each observation is still partial, i.e., missing the cart and pole velocity.\nAs you can see in the learning curves, the agent learns a bit slower than with full observations but still much faster than the agent with only a single partial observation (which does not really learn at all).\n\n\nStacking Observations Using RLlib’s Trajectory API\nAbove, I used Gym’s FrameStack wrapper to stack the last \\(n\\) observations inside the environment. Alternatively, the stacking can be implemented on the model side, e.g., using RLlib’s trajectory API, which reduces space complexity for storing the stacked observations but should lead to similar results.\n\n#collapse-output\n\nfrom ray.rllib.examples.models.trajectory_view_utilizing_models import FrameStackingCartPoleModel\nfrom ray.rllib.models.catalog import ModelCatalog\n\nModelCatalog.register_custom_model(\"stacking_model\", FrameStackingCartPoleModel)\n\nconfig3a2 = ppo.DEFAULT_CONFIG.copy()\nconfig3a2[\"env\"] = \"StatelessCartPole\"\nconfig3a2[\"model\"] = {\n    \"custom_model\": \"stacking_model\",\n    \"custom_model_config\": {\n        \"num_frames\": NUM_FRAMES,\n    }\n}\n\nresults3a2 = ray.tune.run(\"PPO\", config=config3a2, stop=stop)\nprint(\"Option 3a2 with Trajectory API: Training finished successfully\")\n\n== Status ==Current time: 2021-12-01 23:11:27 (running for 00:00:00.14)Memory usage on this node: 9.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nPENDING\n\n\n\n\n\n\n\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=7032) 2021-12-01 23:11:41,672  INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n(pid=7032) 2021-12-01 23:11:41,672  INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n(pid=7032) 2021-12-01 23:11:41,672  INFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=20056) 2021-12-01 23:11:58,655 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=7032) 2021-12-01 23:12:00,488  WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=7032) 2021-12-01 23:12:01,689  WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n(pid=7032) 2021-12-01 23:12:01,689  INFO trainable.py:110 -- Trainable.setup took 20.017 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n(pid=7032) 2021-12-01 23:12:01,689  WARNING util.py:57 -- Install gputil for GPU system monitoring.\n(pid=7032) 2021-12-01 23:12:08,389  WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n(pid=7032) Windows fatal exception: access violation\n(pid=7032) \n(pid=20056) [2021-12-01 23:13:35,836 C 20056 17856] core_worker.cc:796:  Check failed: _s.ok() Bad status: IOError: Unknown error\n(pid=20056) *** StackTrace Information ***\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyInit__raylet\n(pid=20056)     PyNumber_InPlaceLshift\n(pid=20056)     Py_CheckFunctionResult\n(pid=20056)     PyEval_EvalFrameDefault\n(pid=20056)     Py_CheckFunctionResult\n(pid=20056)     PyEval_EvalFrameDefault\n(pid=20056)     PyEval_EvalCodeWithName\n(pid=20056)     PyEval_EvalCodeEx\n(pid=20056)     PyEval_EvalCode\n(pid=20056)     PyArena_New\n(pid=20056)     PyArena_New\n(pid=20056)     PyRun_FileExFlags\n(pid=20056)     PyRun_SimpleFileExFlags\n(pid=20056)     PyRun_AnyFileExFlags\n(pid=20056)     Py_FatalError\n(pid=20056)     Py_RunMain\n(pid=20056)     Py_RunMain\n(pid=20056)     Py_Main\n(pid=20056)     BaseThreadInitThunk\n(pid=20056)     RtlUserThreadStart\n(pid=20056) \n(pid=20056) Windows fatal exception: access violation\n(pid=20056) \n(pid=20056) Stack (most recent call first):\n(pid=20056)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\worker.py\", line 425 in main_loop\n(pid=20056)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\workers/default_worker.py\", line 218 in &lt;module&gt;\n(pid=6936) [2021-12-01 23:13:35,836 C 6936 7600] core_worker.cc:796:  Check failed: _s.ok() Bad status: IOError: Unknown error\n(pid=6936) *** StackTrace Information ***\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyInit__raylet\n(pid=6936)     PyNumber_InPlaceLshift\n(pid=6936)     Py_CheckFunctionResult\n(pid=6936)     PyEval_EvalFrameDefault\n(pid=6936)     Py_CheckFunctionResult\n(pid=6936)     PyEval_EvalFrameDefault\n(pid=6936)     PyEval_EvalCodeWithName\n(pid=6936)     PyEval_EvalCodeEx\n(pid=6936)     PyEval_EvalCode\n(pid=6936)     PyArena_New\n(pid=6936)     PyArena_New\n(pid=6936)     PyRun_FileExFlags\n(pid=6936)     PyRun_SimpleFileExFlags\n(pid=6936)     PyRun_AnyFileExFlags\n(pid=6936)     Py_FatalError\n(pid=6936)     Py_RunMain\n(pid=6936)     Py_RunMain\n(pid=6936)     Py_Main\n(pid=6936)     BaseThreadInitThunk\n(pid=6936)     RtlUserThreadStart\n(pid=6936) \n(pid=6936) Windows fatal exception: access violation\n(pid=6936) \n(pid=6936) Stack (most recent call first):\n(pid=6936)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\worker.py\", line 425 in main_loop\n(pid=6936)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\workers/default_worker.py\", line 218 in &lt;module&gt;\n2021-12-01 23:13:35,937 INFO tune.py:630 -- Total run time: 128.19 seconds (127.68 seconds for the tuning loop).\n\n\n== Status ==Current time: 2021-12-01 23:11:32 (running for 00:00:05.14)Memory usage on this node: 9.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nPENDING\n\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:01 (running for 00:00:33.99)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:02 (running for 00:00:35.21)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:08 (running for 00:00:40.28)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n\n\n\n\n\n\nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 4000\n  custom_metrics: {}\n  date: 2021-12-01_23-12-12\n  done: false\n  episode_len_mean: 22.420454545454547\n  episode_media: {}\n  episode_reward_max: 76.0\n  episode_reward_mean: 22.420454545454547\n  episode_reward_min: 9.0\n  episodes_this_iter: 176\n  episodes_total: 176\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6806640028953552\n          entropy_coeff: 0.0\n          kl: 0.013389287516474724\n          model: {}\n          policy_loss: -0.021481554955244064\n          total_loss: 188.75352478027344\n          vf_explained_var: -0.03809177502989769\n          vf_loss: 188.77232360839844\n    num_agent_steps_sampled: 4000\n    num_agent_steps_trained: 4000\n    num_steps_sampled: 4000\n    num_steps_trained: 4000\n  iterations_since_restore: 1\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 80.06666666666668\n    ram_util_percent: 86.22666666666666\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.12501936271684463\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.18659985231728216\n    mean_inference_ms: 2.6111630884934134\n    mean_raw_obs_processing_ms: 0.3001049366084402\n  time_since_restore: 10.872305870056152\n  time_this_iter_s: 10.872305870056152\n  time_total_s: 10.872305870056152\n  timers:\n    learn_throughput: 952.767\n    learn_time_ms: 4198.298\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 597.657\n    sample_time_ms: 6692.801\n    update_time_ms: 0.0\n  timestamp: 1638396732\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 4000\n  training_iteration: 1\n  trial_id: a1402_00000\n  \nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 8000\n  custom_metrics: {}\n  date: 2021-12-01_23-12-21\n  done: false\n  episode_len_mean: 27.07482993197279\n  episode_media: {}\n  episode_reward_max: 92.0\n  episode_reward_mean: 27.07482993197279\n  episode_reward_min: 9.0\n  episodes_this_iter: 147\n  episodes_total: 323\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6690337657928467\n          entropy_coeff: 0.0\n          kl: 0.006590469740331173\n          model: {}\n          policy_loss: -0.005931881722062826\n          total_loss: 152.8258056640625\n          vf_explained_var: -0.11891558021306992\n          vf_loss: 152.83041381835938\n    num_agent_steps_sampled: 8000\n    num_agent_steps_trained: 8000\n    num_steps_sampled: 8000\n    num_steps_trained: 8000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 2\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 76.04615384615386\n    ram_util_percent: 86.37692307692308\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11535685211184185\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.17986845152071707\n    mean_inference_ms: 2.358616568210475\n    mean_raw_obs_processing_ms: 0.24868940552548166\n  time_since_restore: 19.69421625137329\n  time_this_iter_s: 8.821910381317139\n  time_total_s: 19.69421625137329\n  timers:\n    learn_throughput: 1030.618\n    learn_time_ms: 3881.168\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 493.345\n    sample_time_ms: 8107.914\n    update_time_ms: 0.0\n  timestamp: 1638396741\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 8000\n  training_iteration: 2\n  trial_id: a1402_00000\n  \nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 12000\n  custom_metrics: {}\n  date: 2021-12-01_23-12-29\n  done: false\n  episode_len_mean: 31.515625\n  episode_media: {}\n  episode_reward_max: 107.0\n  episode_reward_mean: 31.515625\n  episode_reward_min: 9.0\n  episodes_this_iter: 128\n  episodes_total: 451\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6504582166671753\n          entropy_coeff: 0.0\n          kl: 0.01009401399642229\n          model: {}\n          policy_loss: -0.014275692403316498\n          total_loss: 183.70419311523438\n          vf_explained_var: -0.09810103476047516\n          vf_loss: 183.71646118164062\n    num_agent_steps_sampled: 12000\n    num_agent_steps_trained: 12000\n    num_steps_sampled: 12000\n    num_steps_trained: 12000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 3\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 74.85000000000001\n    ram_util_percent: 86.45\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11715046978259044\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.16287547160468283\n    mean_inference_ms: 2.2526623070031446\n    mean_raw_obs_processing_ms: 0.22798632078370612\n  time_since_restore: 27.94515609741211\n  time_this_iter_s: 8.250939846038818\n  time_total_s: 27.94515609741211\n  timers:\n    learn_throughput: 1097.145\n    learn_time_ms: 3645.825\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 482.347\n    sample_time_ms: 8292.781\n    update_time_ms: 1.334\n  timestamp: 1638396749\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 12000\n  training_iteration: 3\n  trial_id: a1402_00000\n  \nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 16000\n  custom_metrics: {}\n  date: 2021-12-01_23-12-39\n  done: false\n  episode_len_mean: 41.78\n  episode_media: {}\n  episode_reward_max: 114.0\n  episode_reward_mean: 41.78\n  episode_reward_min: 10.0\n  episodes_this_iter: 94\n  episodes_total: 545\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6398107409477234\n          entropy_coeff: 0.0\n          kl: 0.006905578076839447\n          model: {}\n          policy_loss: -9.581594349583611e-05\n          total_loss: 233.55148315429688\n          vf_explained_var: -0.05927522853016853\n          vf_loss: 233.55018615722656\n    num_agent_steps_sampled: 16000\n    num_agent_steps_trained: 16000\n    num_steps_sampled: 16000\n    num_steps_trained: 16000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 4\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 80.42307692307692\n    ram_util_percent: 86.43846153846154\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11285530769795944\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15454845909642295\n    mean_inference_ms: 2.234221561962398\n    mean_raw_obs_processing_ms: 0.23170078440755937\n  time_since_restore: 37.353280544281006\n  time_this_iter_s: 9.408124446868896\n  time_total_s: 37.353280544281006\n  timers:\n    learn_throughput: 1071.336\n    learn_time_ms: 3733.658\n    load_throughput: 15993532.888\n    load_time_ms: 0.25\n    sample_throughput: 477.101\n    sample_time_ms: 8383.978\n    update_time_ms: 2.002\n  timestamp: 1638396759\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 16000\n  training_iteration: 4\n  trial_id: a1402_00000\n  \nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 20000\n  custom_metrics: {}\n  date: 2021-12-01_23-12-48\n  done: false\n  episode_len_mean: 45.25\n  episode_media: {}\n  episode_reward_max: 112.0\n  episode_reward_mean: 45.25\n  episode_reward_min: 11.0\n  episodes_this_iter: 90\n  episodes_total: 635\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6046473383903503\n          entropy_coeff: 0.0\n          kl: 0.00994083285331726\n          model: {}\n          policy_loss: -0.013053015805780888\n          total_loss: 248.39173889160156\n          vf_explained_var: -0.07114432007074356\n          vf_loss: 248.4027862548828\n    num_agent_steps_sampled: 20000\n    num_agent_steps_trained: 20000\n    num_steps_sampled: 20000\n    num_steps_trained: 20000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 5\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 82.4\n    ram_util_percent: 86.02307692307691\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1164652225945224\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1496187880432378\n    mean_inference_ms: 2.2751065340130405\n    mean_raw_obs_processing_ms: 0.2258053944148459\n  time_since_restore: 46.962218284606934\n  time_this_iter_s: 9.608937740325928\n  time_total_s: 46.962218284606934\n  timers:\n    learn_throughput: 1078.068\n    learn_time_ms: 3710.34\n    load_throughput: 19991916.111\n    load_time_ms: 0.2\n    sample_throughput: 459.08\n    sample_time_ms: 8713.071\n    update_time_ms: 4.935\n  timestamp: 1638396768\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 20000\n  training_iteration: 5\n  trial_id: a1402_00000\n  \nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 24000\n  custom_metrics: {}\n  date: 2021-12-01_23-12-58\n  done: false\n  episode_len_mean: 51.02\n  episode_media: {}\n  episode_reward_max: 167.0\n  episode_reward_mean: 51.02\n  episode_reward_min: 14.0\n  episodes_this_iter: 74\n  episodes_total: 709\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5902564525604248\n          entropy_coeff: 0.0\n          kl: 0.009673806838691235\n          model: {}\n          policy_loss: -0.0029914507176727057\n          total_loss: 325.0210266113281\n          vf_explained_var: -0.020246472209692\n          vf_loss: 325.0221252441406\n    num_agent_steps_sampled: 24000\n    num_agent_steps_trained: 24000\n    num_steps_sampled: 24000\n    num_steps_trained: 24000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 6\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 80.94615384615385\n    ram_util_percent: 85.85384615384616\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.12087068704524867\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15143744867276646\n    mean_inference_ms: 2.305419644492814\n    mean_raw_obs_processing_ms: 0.22585836913700305\n  time_since_restore: 56.67875838279724\n  time_this_iter_s: 9.716540098190308\n  time_total_s: 56.67875838279724\n  timers:\n    learn_throughput: 1089.778\n    learn_time_ms: 3670.472\n    load_throughput: 23990299.333\n    load_time_ms: 0.167\n    sample_throughput: 448.858\n    sample_time_ms: 8911.498\n    update_time_ms: 4.112\n  timestamp: 1638396778\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 24000\n  training_iteration: 6\n  trial_id: a1402_00000\n  \nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 28000\n  custom_metrics: {}\n  date: 2021-12-01_23-13-07\n  done: false\n  episode_len_mean: 58.96\n  episode_media: {}\n  episode_reward_max: 173.0\n  episode_reward_mean: 58.96\n  episode_reward_min: 12.0\n  episodes_this_iter: 64\n  episodes_total: 773\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5728155970573425\n          entropy_coeff: 0.0\n          kl: 0.008897491730749607\n          model: {}\n          policy_loss: -0.011549570597708225\n          total_loss: 301.4852294921875\n          vf_explained_var: -0.004416638985276222\n          vf_loss: 301.4949951171875\n    num_agent_steps_sampled: 28000\n    num_agent_steps_trained: 28000\n    num_steps_sampled: 28000\n    num_steps_trained: 28000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 7\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 81.25384615384615\n    ram_util_percent: 86.0\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.12220176410428243\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.14921622016148267\n    mean_inference_ms: 2.304350481426518\n    mean_raw_obs_processing_ms: 0.22517177176600833\n  time_since_restore: 65.64418339729309\n  time_this_iter_s: 8.96542501449585\n  time_total_s: 65.64418339729309\n  timers:\n    learn_throughput: 1099.932\n    learn_time_ms: 3636.59\n    load_throughput: 27988682.555\n    load_time_ms: 0.143\n    sample_throughput: 448.012\n    sample_time_ms: 8928.331\n    update_time_ms: 3.525\n  timestamp: 1638396787\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 28000\n  training_iteration: 7\n  trial_id: a1402_00000\n  \nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 32000\n  custom_metrics: {}\n  date: 2021-12-01_23-13-16\n  done: false\n  episode_len_mean: 65.85\n  episode_media: {}\n  episode_reward_max: 173.0\n  episode_reward_mean: 65.85\n  episode_reward_min: 12.0\n  episodes_this_iter: 57\n  episodes_total: 830\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5591020584106445\n          entropy_coeff: 0.0\n          kl: 0.010199657641351223\n          model: {}\n          policy_loss: -0.0008332685683853924\n          total_loss: 395.8393859863281\n          vf_explained_var: 0.02246681973338127\n          vf_loss: 395.8381652832031\n    num_agent_steps_sampled: 32000\n    num_agent_steps_trained: 32000\n    num_steps_sampled: 32000\n    num_steps_trained: 32000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 8\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 74.975\n    ram_util_percent: 86.25000000000001\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11971547914161862\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1491953792440581\n    mean_inference_ms: 2.288866379945899\n    mean_raw_obs_processing_ms: 0.21941612443688768\n  time_since_restore: 74.37759304046631\n  time_this_iter_s: 8.733409643173218\n  time_total_s: 74.37759304046631\n  timers:\n    learn_throughput: 1104.214\n    learn_time_ms: 3622.488\n    load_throughput: 31987065.777\n    load_time_ms: 0.125\n    sample_throughput: 449.229\n    sample_time_ms: 8904.153\n    update_time_ms: 3.084\n  timestamp: 1638396796\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 32000\n  training_iteration: 8\n  trial_id: a1402_00000\n  \nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 36000\n  custom_metrics: {}\n  date: 2021-12-01_23-13-26\n  done: false\n  episode_len_mean: 75.45\n  episode_media: {}\n  episode_reward_max: 294.0\n  episode_reward_mean: 75.45\n  episode_reward_min: 15.0\n  episodes_this_iter: 49\n  episodes_total: 879\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5561075806617737\n          entropy_coeff: 0.0\n          kl: 0.010793409310281277\n          model: {}\n          policy_loss: -0.006749303545802832\n          total_loss: 487.8432922363281\n          vf_explained_var: 0.037814658135175705\n          vf_loss: 487.8478698730469\n    num_agent_steps_sampled: 36000\n    num_agent_steps_trained: 36000\n    num_steps_sampled: 36000\n    num_steps_trained: 36000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 9\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 87.64285714285714\n    ram_util_percent: 86.90714285714286\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.12074714910315451\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15026008834474422\n    mean_inference_ms: 2.3119485993450812\n    mean_raw_obs_processing_ms: 0.21632637276503874\n  time_since_restore: 84.76655960083008\n  time_this_iter_s: 10.38896656036377\n  time_total_s: 84.76655960083008\n  timers:\n    learn_throughput: 1111.045\n    learn_time_ms: 3600.213\n    load_throughput: 35985448.999\n    load_time_ms: 0.111\n    sample_throughput: 440.37\n    sample_time_ms: 9083.268\n    update_time_ms: 2.964\n  timestamp: 1638396806\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 36000\n  training_iteration: 9\n  trial_id: a1402_00000\n  \nResult for PPO_StatelessCartPole_a1402_00000:\n  agent_timesteps_total: 40000\n  custom_metrics: {}\n  date: 2021-12-01_23-13-35\n  done: true\n  episode_len_mean: 84.4\n  episode_media: {}\n  episode_reward_max: 294.0\n  episode_reward_mean: 84.4\n  episode_reward_min: 17.0\n  episodes_this_iter: 40\n  episodes_total: 919\n  experiment_id: a99c739e101a4c88ba77c4f9b0d64803\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5557733774185181\n          entropy_coeff: 0.0\n          kl: 0.005785451736301184\n          model: {}\n          policy_loss: -0.0009503072360530496\n          total_loss: 503.70172119140625\n          vf_explained_var: 0.0725829154253006\n          vf_loss: 503.7015380859375\n    num_agent_steps_sampled: 40000\n    num_agent_steps_trained: 40000\n    num_steps_sampled: 40000\n    num_steps_trained: 40000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 10\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 79.30833333333332\n    ram_util_percent: 87.125\n  pid: 7032\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.12150490867421336\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.14914803016875297\n    mean_inference_ms: 2.327153258190586\n    mean_raw_obs_processing_ms: 0.21595014888922642\n  time_since_restore: 93.20761466026306\n  time_this_iter_s: 8.441055059432983\n  time_total_s: 93.20761466026306\n  timers:\n    learn_throughput: 1120.545\n    learn_time_ms: 3569.692\n    load_throughput: 20046858.645\n    load_time_ms: 0.2\n    sample_throughput: 442.479\n    sample_time_ms: 9039.968\n    update_time_ms: 2.868\n  timestamp: 1638396815\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 40000\n  training_iteration: 10\n  trial_id: a1402_00000\n  \nOption 3a2 with Trajectory API: Training finished successfully\n\n\n== Status ==Current time: 2021-12-01 23:12:13 (running for 00:00:45.92)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n1\n10.8723\n4000\n22.4205\n76\n9\n22.4205\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:18 (running for 00:00:51.00)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n1\n10.8723\n4000\n22.4205\n76\n9\n22.4205\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:24 (running for 00:00:56.98)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n2\n19.6942\n8000\n27.0748\n92\n9\n27.0748\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:29 (running for 00:01:02.06)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n3\n27.9452\n12000\n31.5156\n107\n9\n31.5156\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:34 (running for 00:01:07.12)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n3\n27.9452\n12000\n31.5156\n107\n9\n31.5156\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:40 (running for 00:01:12.53)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n4\n37.3533\n16000\n41.78\n114\n10\n41.78\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:45 (running for 00:01:17.88)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n4\n37.3533\n16000\n41.78\n114\n10\n41.78\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:51 (running for 00:01:23.24)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n5\n46.9622\n20000\n45.25\n112\n11\n45.25\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:12:56 (running for 00:01:28.34)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n5\n46.9622\n20000\n45.25\n112\n11\n45.25\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:13:01 (running for 00:01:33.98)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n6\n56.6788\n24000\n51.02\n167\n14\n51.02\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:13:06 (running for 00:01:39.08)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n6\n56.6788\n24000\n51.02\n167\n14\n51.02\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:13:12 (running for 00:01:45.03)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n7\n65.6442\n28000\n58.96\n173\n12\n58.96\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:13:17 (running for 00:01:50.20)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n8\n74.3776\n32000\n65.85\n173\n12\n65.85\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:13:23 (running for 00:01:55.26)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n8\n74.3776\n32000\n65.85\n173\n12\n65.85\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:13:29 (running for 00:02:01.24)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n9\n84.7666\n36000\n75.45\n294\n15\n75.45\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:13:34 (running for 00:02:06.30)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nRUNNING\n127.0.0.1:7032\n9\n84.7666\n36000\n75.45\n294\n15\n75.45\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:13:35 (running for 00:02:07.72)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 TERMINATED)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_a1402_00000\nTERMINATED\n127.0.0.1:7032\n10\n93.2076\n40000\n84.4\n294\n17\n84.4\n\n\n\n\n\n\n\nprint_reward(results3a2)\n\nReward after 10 training iterations: 84.4\n\n\n\nplot_rewards(results3a2)\n\nc:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\n# stacking observations inside the model works worse?\nplot_learning(results3a, label=\"3a: Stacked Obs in Env\")\nplot_learning(results3a2, label=\"3a2: Stacked Obs in Model\")\n\n\n\n\n\n\n\nOption 3b: Use an LSTM for Processing the Sequence\nInstead of stacking the last \\(n\\) observations and providing this sequence as input to a regular feed-forward neural network, a recurrent neural network (RNN) can be used, keeping track of a learned state that is passed onwards from observation to observation.\nLong short-term memory (LSTM) networks are a variant of RNNs that are good at keeping state for longer durations. To use an LSTM with RLlib, simply set the corresponding flag in the model config:\n\n#collapse-output\n\nconfig3b = ppo.DEFAULT_CONFIG.copy()\nconfig3b[\"env\"] = \"StatelessCartPole\"\nconfig3b[\"model\"] = {\n    \"use_lstm\": True,\n    # \"max_seq_len\": 10,\n}\n\nresults3b = ray.tune.run(\"PPO\", config=config3b, stop=stop)\nprint(\"Option 3b: Training finished successfully\")\n\n== Status ==Current time: 2021-12-01 23:14:43 (running for 00:00:00.14)Memory usage on this node: 9.6/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nPENDING\n\n\n\n\n\n\n\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=14344) 2021-12-01 23:14:58,972 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n(pid=14344) 2021-12-01 23:14:58,972 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n(pid=14344) 2021-12-01 23:14:58,972 INFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=8656) 2021-12-01 23:15:15,322  WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=14344) 2021-12-01 23:15:20,461 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=14344) 2021-12-01 23:15:23,593 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n(pid=14344) 2021-12-01 23:15:23,593 INFO trainable.py:110 -- Trainable.setup took 24.634 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n(pid=14344) 2021-12-01 23:15:23,593 WARNING util.py:57 -- Install gputil for GPU system monitoring.\n(pid=14344) 2021-12-01 23:15:32,644 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n(pid=14344) Windows fatal exception: access violation\n(pid=14344) \n2021-12-01 23:26:57,480 INFO tune.py:630 -- Total run time: 733.96 seconds (733.70 seconds for the tuning loop).\n\n\n== Status ==Current time: 2021-12-01 23:14:48 (running for 00:00:05.16)Memory usage on this node: 9.6/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nPENDING\n\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:15:23 (running for 00:00:40.06)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:15:25 (running for 00:00:42.11)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:15:30 (running for 00:00:47.44)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:15:36 (running for 00:00:52.62)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:15:41 (running for 00:00:57.90)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:15:46 (running for 00:01:03.24)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:15:52 (running for 00:01:08.55)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:15:57 (running for 00:01:13.93)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:02 (running for 00:01:19.04)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:07 (running for 00:01:24.29)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:12 (running for 00:01:29.41)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:18 (running for 00:01:35.06)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:23 (running for 00:01:40.21)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:29 (running for 00:01:45.76)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:34 (running for 00:01:50.86)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:39 (running for 00:01:56.00)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n\n\n\n\n\n\nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 4000\n  custom_metrics: {}\n  date: 2021-12-01_23-16-44\n  done: false\n  episode_len_mean: 24.74534161490683\n  episode_media: {}\n  episode_reward_max: 73.0\n  episode_reward_mean: 24.74534161490683\n  episode_reward_min: 9.0\n  episodes_this_iter: 161\n  episodes_total: 161\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6702085137367249\n          entropy_coeff: 0.0\n          kl: 0.01620529219508171\n          model: {}\n          policy_loss: -0.021323969587683678\n          total_loss: 147.9649658203125\n          vf_explained_var: -0.08384507149457932\n          vf_loss: 147.98304748535156\n    num_agent_steps_sampled: 4000\n    num_agent_steps_trained: 4000\n    num_steps_sampled: 4000\n    num_steps_trained: 4000\n  iterations_since_restore: 1\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 98.09908256880733\n    ram_util_percent: 87.94036697247705\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.14404039499775298\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15224504509160317\n    mean_inference_ms: 3.842544658680462\n    mean_raw_obs_processing_ms: 0.25258780840844064\n  time_since_restore: 80.78264856338501\n  time_this_iter_s: 80.78264856338501\n  time_total_s: 80.78264856338501\n  timers:\n    learn_throughput: 55.809\n    learn_time_ms: 71672.947\n    load_throughput: 4001243.978\n    load_time_ms: 1.0\n    sample_throughput: 442.023\n    sample_time_ms: 9049.296\n    update_time_ms: 14.003\n  timestamp: 1638397004\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 4000\n  training_iteration: 1\n  trial_id: 15f17_00000\n  \nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 8000\n  custom_metrics: {}\n  date: 2021-12-01_23-17-57\n  done: false\n  episode_len_mean: 29.848484848484848\n  episode_media: {}\n  episode_reward_max: 85.0\n  episode_reward_mean: 29.848484848484848\n  episode_reward_min: 9.0\n  episodes_this_iter: 132\n  episodes_total: 293\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6473504304885864\n          entropy_coeff: 0.0\n          kl: 0.010533971711993217\n          model: {}\n          policy_loss: -0.009972598403692245\n          total_loss: 134.57276916503906\n          vf_explained_var: 0.15424844622612\n          vf_loss: 134.58062744140625\n    num_agent_steps_sampled: 8000\n    num_agent_steps_trained: 8000\n    num_steps_sampled: 8000\n    num_steps_trained: 8000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 2\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 98.81443298969072\n    ram_util_percent: 88.0278350515464\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1454985114758189\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1627394677104148\n    mean_inference_ms: 3.657513714820443\n    mean_raw_obs_processing_ms: 0.2482827895962494\n  time_since_restore: 153.95946764945984\n  time_this_iter_s: 73.17681908607483\n  time_total_s: 153.95946764945984\n  timers:\n    learn_throughput: 58.598\n    learn_time_ms: 68261.841\n    load_throughput: 8002487.956\n    load_time_ms: 0.5\n    sample_throughput: 89.708\n    sample_time_ms: 44589.185\n    update_time_ms: 10.501\n  timestamp: 1638397077\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 8000\n  training_iteration: 2\n  trial_id: 15f17_00000\n  \nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 12000\n  custom_metrics: {}\n  date: 2021-12-01_23-19-11\n  done: false\n  episode_len_mean: 32.095238095238095\n  episode_media: {}\n  episode_reward_max: 91.0\n  episode_reward_mean: 32.095238095238095\n  episode_reward_min: 9.0\n  episodes_this_iter: 126\n  episodes_total: 419\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6104030013084412\n          entropy_coeff: 0.0\n          kl: 0.01123636681586504\n          model: {}\n          policy_loss: -0.0049535431899130344\n          total_loss: 151.1492919921875\n          vf_explained_var: 0.1538025140762329\n          vf_loss: 151.15199279785156\n    num_agent_steps_sampled: 12000\n    num_agent_steps_trained: 12000\n    num_steps_sampled: 12000\n    num_steps_trained: 12000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 3\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 97.37070707070708\n    ram_util_percent: 87.73737373737374\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1454022761596221\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1557348165264045\n    mean_inference_ms: 3.294093173663449\n    mean_raw_obs_processing_ms: 0.25605837493978906\n  time_since_restore: 227.74660396575928\n  time_this_iter_s: 73.78713631629944\n  time_total_s: 227.74660396575928\n  timers:\n    learn_throughput: 58.855\n    learn_time_ms: 67963.962\n    load_throughput: 6009031.519\n    load_time_ms: 0.666\n    sample_throughput: 74.781\n    sample_time_ms: 53489.736\n    update_time_ms: 8.973\n  timestamp: 1638397151\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 12000\n  training_iteration: 3\n  trial_id: 15f17_00000\n  \nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 16000\n  custom_metrics: {}\n  date: 2021-12-01_23-20-22\n  done: false\n  episode_len_mean: 42.16\n  episode_media: {}\n  episode_reward_max: 97.0\n  episode_reward_mean: 42.16\n  episode_reward_min: 12.0\n  episodes_this_iter: 93\n  episodes_total: 512\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6158422827720642\n          entropy_coeff: 0.0\n          kl: 0.010851425118744373\n          model: {}\n          policy_loss: -0.003959023393690586\n          total_loss: 152.54002380371094\n          vf_explained_var: 0.17243239283561707\n          vf_loss: 152.5417938232422\n    num_agent_steps_sampled: 16000\n    num_agent_steps_trained: 16000\n    num_steps_sampled: 16000\n    num_steps_trained: 16000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 4\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 99.08333333333333\n    ram_util_percent: 85.98645833333335\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1462575659080806\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15407968489488727\n    mean_inference_ms: 3.275941401825844\n    mean_raw_obs_processing_ms: 0.24603215260913444\n  time_since_restore: 298.2684841156006\n  time_this_iter_s: 70.52188014984131\n  time_total_s: 298.2684841156006\n  timers:\n    learn_throughput: 59.973\n    learn_time_ms: 66696.53\n    load_throughput: 8012042.025\n    load_time_ms: 0.499\n    sample_throughput: 67.917\n    sample_time_ms: 58895.107\n    update_time_ms: 7.627\n  timestamp: 1638397222\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 16000\n  training_iteration: 4\n  trial_id: 15f17_00000\n  \nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 20000\n  custom_metrics: {}\n  date: 2021-12-01_23-21-45\n  done: false\n  episode_len_mean: 33.36974789915966\n  episode_media: {}\n  episode_reward_max: 103.0\n  episode_reward_mean: 33.36974789915966\n  episode_reward_min: 10.0\n  episodes_this_iter: 119\n  episodes_total: 631\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6034429669380188\n          entropy_coeff: 0.0\n          kl: 0.012084417045116425\n          model: {}\n          policy_loss: -0.012375776655972004\n          total_loss: 157.4758758544922\n          vf_explained_var: 0.14680173993110657\n          vf_loss: 157.48585510253906\n    num_agent_steps_sampled: 20000\n    num_agent_steps_trained: 20000\n    num_steps_sampled: 20000\n    num_steps_trained: 20000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 5\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 98.08396226415095\n    ram_util_percent: 85.38396226415095\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.14943092584832965\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1572909528662859\n    mean_inference_ms: 3.248902887560971\n    mean_raw_obs_processing_ms: 0.24150111904383836\n  time_since_restore: 381.94007539749146\n  time_this_iter_s: 83.67159128189087\n  time_total_s: 381.94007539749146\n  timers:\n    learn_throughput: 58.34\n    learn_time_ms: 68563.84\n    load_throughput: 10015052.531\n    load_time_ms: 0.399\n    sample_throughput: 65.33\n    sample_time_ms: 61227.393\n    update_time_ms: 8.119\n  timestamp: 1638397305\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 20000\n  training_iteration: 5\n  trial_id: 15f17_00000\n  \nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 24000\n  custom_metrics: {}\n  date: 2021-12-01_23-22-56\n  done: false\n  episode_len_mean: 39.95\n  episode_media: {}\n  episode_reward_max: 100.0\n  episode_reward_mean: 39.95\n  episode_reward_min: 9.0\n  episodes_this_iter: 100\n  episodes_total: 731\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6064294576644897\n          entropy_coeff: 0.0\n          kl: 0.006135161034762859\n          model: {}\n          policy_loss: -0.0012852392392233014\n          total_loss: 144.01748657226562\n          vf_explained_var: 0.2577447295188904\n          vf_loss: 144.01754760742188\n    num_agent_steps_sampled: 24000\n    num_agent_steps_trained: 24000\n    num_steps_sampled: 24000\n    num_steps_trained: 24000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 6\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 97.5808510638298\n    ram_util_percent: 86.03617021276594\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.14752250682829854\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15924224077925142\n    mean_inference_ms: 3.1588659969113992\n    mean_raw_obs_processing_ms: 0.23714881828949555\n  time_since_restore: 452.454083442688\n  time_this_iter_s: 70.51400804519653\n  time_total_s: 452.454083442688\n  timers:\n    learn_throughput: 59.002\n    learn_time_ms: 67794.396\n    load_throughput: 12018063.037\n    load_time_ms: 0.333\n    sample_throughput: 61.729\n    sample_time_ms: 64799.66\n    update_time_ms: 7.201\n  timestamp: 1638397376\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 24000\n  training_iteration: 6\n  trial_id: 15f17_00000\n  \nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 28000\n  custom_metrics: {}\n  date: 2021-12-01_23-23-57\n  done: false\n  episode_len_mean: 34.38461538461539\n  episode_media: {}\n  episode_reward_max: 93.0\n  episode_reward_mean: 34.38461538461539\n  episode_reward_min: 9.0\n  episodes_this_iter: 117\n  episodes_total: 848\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.602990984916687\n          entropy_coeff: 0.0\n          kl: 0.005702142603695393\n          model: {}\n          policy_loss: -0.0026899229269474745\n          total_loss: 126.622802734375\n          vf_explained_var: 0.28406739234924316\n          vf_loss: 126.62434387207031\n    num_agent_steps_sampled: 28000\n    num_agent_steps_trained: 28000\n    num_steps_sampled: 28000\n    num_steps_trained: 28000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 7\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 97.09285714285713\n    ram_util_percent: 85.65119047619051\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.14141074845480212\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1582564423010624\n    mean_inference_ms: 3.1048115340137876\n    mean_raw_obs_processing_ms: 0.23267272113813148\n  time_since_restore: 513.63032746315\n  time_this_iter_s: 61.176244020462036\n  time_total_s: 513.63032746315\n  timers:\n    learn_throughput: 60.697\n    learn_time_ms: 65900.842\n    load_throughput: 14021073.543\n    load_time_ms: 0.285\n    sample_throughput: 60.942\n    sample_time_ms: 65636.225\n    update_time_ms: 6.744\n  timestamp: 1638397437\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 28000\n  training_iteration: 7\n  trial_id: 15f17_00000\n  \nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 32000\n  custom_metrics: {}\n  date: 2021-12-01_23-24-57\n  done: false\n  episode_len_mean: 36.44954128440367\n  episode_media: {}\n  episode_reward_max: 77.0\n  episode_reward_mean: 36.44954128440367\n  episode_reward_min: 11.0\n  episodes_this_iter: 109\n  episodes_total: 957\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5927915573120117\n          entropy_coeff: 0.0\n          kl: 0.012266391888260841\n          model: {}\n          policy_loss: -0.0021681918296962976\n          total_loss: 94.81949615478516\n          vf_explained_var: 0.31072309613227844\n          vf_loss: 94.81922149658203\n    num_agent_steps_sampled: 32000\n    num_agent_steps_trained: 32000\n    num_steps_sampled: 32000\n    num_steps_trained: 32000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 8\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 95.18048780487804\n    ram_util_percent: 85.51341463414633\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.13663044317764386\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.15200395813252837\n    mean_inference_ms: 3.023815960844668\n    mean_raw_obs_processing_ms: 0.22803359086359726\n  time_since_restore: 573.1020631790161\n  time_this_iter_s: 59.47173571586609\n  time_total_s: 573.1020631790161\n  timers:\n    learn_throughput: 62.141\n    learn_time_ms: 64369.633\n    load_throughput: 16024084.05\n    load_time_ms: 0.25\n    sample_throughput: 61.555\n    sample_time_ms: 64982.899\n    update_time_ms: 5.901\n  timestamp: 1638397497\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 32000\n  training_iteration: 8\n  trial_id: 15f17_00000\n  \nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 36000\n  custom_metrics: {}\n  date: 2021-12-01_23-25-57\n  done: false\n  episode_len_mean: 34.80701754385965\n  episode_media: {}\n  episode_reward_max: 93.0\n  episode_reward_mean: 34.80701754385965\n  episode_reward_min: 10.0\n  episodes_this_iter: 114\n  episodes_total: 1071\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5793452262878418\n          entropy_coeff: 0.0\n          kl: 0.009524409659206867\n          model: {}\n          policy_loss: 0.00497779343277216\n          total_loss: 101.02494812011719\n          vf_explained_var: 0.31699204444885254\n          vf_loss: 101.01805877685547\n    num_agent_steps_sampled: 36000\n    num_agent_steps_trained: 36000\n    num_steps_sampled: 36000\n    num_steps_trained: 36000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 9\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 95.77349397590359\n    ram_util_percent: 85.25903614457832\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.13135990601240866\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.14946784585673123\n    mean_inference_ms: 2.9620819680365273\n    mean_raw_obs_processing_ms: 0.22295928348485725\n  time_since_restore: 633.5450580120087\n  time_this_iter_s: 60.442994832992554\n  time_total_s: 633.5450580120087\n  timers:\n    learn_throughput: 63.214\n    learn_time_ms: 63277.532\n    load_throughput: 18027094.556\n    load_time_ms: 0.222\n    sample_throughput: 62.131\n    sample_time_ms: 64380.478\n    update_time_ms: 6.023\n  timestamp: 1638397557\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 36000\n  training_iteration: 9\n  trial_id: 15f17_00000\n  \nResult for PPO_StatelessCartPole_15f17_00000:\n  agent_timesteps_total: 40000\n  custom_metrics: {}\n  date: 2021-12-01_23-26-57\n  done: true\n  episode_len_mean: 41.01\n  episode_media: {}\n  episode_reward_max: 91.0\n  episode_reward_mean: 41.01\n  episode_reward_min: 9.0\n  episodes_this_iter: 97\n  episodes_total: 1168\n  experiment_id: e1f0e5c8896845ff90ba05d38befaef8\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5690697431564331\n          entropy_coeff: 0.0\n          kl: 0.005614957306534052\n          model: {}\n          policy_loss: 0.0013457380700856447\n          total_loss: 97.90939331054688\n          vf_explained_var: 0.3871138393878937\n          vf_loss: 97.90692138671875\n    num_agent_steps_sampled: 40000\n    num_agent_steps_trained: 40000\n    num_steps_sampled: 40000\n    num_steps_trained: 40000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 10\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 95.57160493827159\n    ram_util_percent: 85.20246913580247\n  pid: 14344\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1271907494503554\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1465406708441855\n    mean_inference_ms: 2.916534058088682\n    mean_raw_obs_processing_ms: 0.21554435209484965\n  time_since_restore: 693.0942261219025\n  time_this_iter_s: 59.5491681098938\n  time_total_s: 693.0942261219025\n  timers:\n    learn_throughput: 64.181\n    learn_time_ms: 62324.001\n    load_throughput: 20030105.062\n    load_time_ms: 0.2\n    sample_throughput: 62.515\n    sample_time_ms: 63984.333\n    update_time_ms: 6.121\n  timestamp: 1638397617\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 40000\n  training_iteration: 10\n  trial_id: 15f17_00000\n  \nOption 3b: Training finished successfully\n\n\n== Status ==Current time: 2021-12-01 23:16:45 (running for 00:02:01.96)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:50 (running for 00:02:07.22)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:16:55 (running for 00:02:12.37)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:01 (running for 00:02:17.58)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:06 (running for 00:02:22.68)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:11 (running for 00:02:28.07)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:16 (running for 00:02:33.22)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:22 (running for 00:02:38.61)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:27 (running for 00:02:43.93)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:32 (running for 00:02:49.19)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:37 (running for 00:02:54.33)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:43 (running for 00:02:59.68)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:48 (running for 00:03:04.83)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:53 (running for 00:03:10.02)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n1\n80.7826\n4000\n24.7453\n73\n9\n24.7453\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:17:58 (running for 00:03:15.16)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:03 (running for 00:03:20.24)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:08 (running for 00:03:25.35)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:14 (running for 00:03:30.69)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:19 (running for 00:03:35.82)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:24 (running for 00:03:41.03)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:29 (running for 00:03:46.16)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:34 (running for 00:03:51.40)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:40 (running for 00:03:56.51)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:45 (running for 00:04:01.86)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:50 (running for 00:04:07.05)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:18:55 (running for 00:04:12.42)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:01 (running for 00:04:17.63)Memory usage on this node: 10.6/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:06 (running for 00:04:23.08)Memory usage on this node: 10.7/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n2\n153.959\n8000\n29.8485\n85\n9\n29.8485\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:12 (running for 00:04:29.01)Memory usage on this node: 10.6/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:17 (running for 00:04:34.08)Memory usage on this node: 10.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:22 (running for 00:04:39.15)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:28 (running for 00:04:44.49)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:33 (running for 00:04:49.61)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:38 (running for 00:04:54.90)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:43 (running for 00:04:60.00)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:48 (running for 00:05:05.26)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:53 (running for 00:05:10.39)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:19:59 (running for 00:05:15.79)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:04 (running for 00:05:20.90)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:09 (running for 00:05:26.05)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:14 (running for 00:05:31.20)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:19 (running for 00:05:36.44)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n3\n227.747\n12000\n32.0952\n91\n9\n32.0952\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:25 (running for 00:05:41.62)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:30 (running for 00:05:46.71)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:35 (running for 00:05:51.93)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:41 (running for 00:05:57.61)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:46 (running for 00:06:02.79)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:51 (running for 00:06:08.06)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:20:56 (running for 00:06:13.36)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:02 (running for 00:06:18.71)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:07 (running for 00:06:23.95)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:13 (running for 00:06:29.72)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:18 (running for 00:06:35.43)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:24 (running for 00:06:41.36)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:30 (running for 00:06:47.02)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:36 (running for 00:06:52.91)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:41 (running for 00:06:58.10)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n4\n298.268\n16000\n42.16\n97\n12\n42.16\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:46 (running for 00:07:03.31)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:51 (running for 00:07:08.39)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:21:57 (running for 00:07:13.54)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:02 (running for 00:07:18.90)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:07 (running for 00:07:24.11)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:13 (running for 00:07:29.46)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:18 (running for 00:07:34.60)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:23 (running for 00:07:39.79)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:28 (running for 00:07:44.86)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:33 (running for 00:07:50.17)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:38 (running for 00:07:55.29)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:44 (running for 00:08:00.65)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:49 (running for 00:08:05.79)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:22:54 (running for 00:08:11.30)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n5\n381.94\n20000\n33.3697\n103\n10\n33.3697\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:00 (running for 00:08:16.92)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:05 (running for 00:08:22.07)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:10 (running for 00:08:27.19)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:16 (running for 00:08:32.58)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:21 (running for 00:08:37.69)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:26 (running for 00:08:42.86)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:31 (running for 00:08:47.97)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:36 (running for 00:08:53.14)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:41 (running for 00:08:58.26)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:47 (running for 00:09:03.47)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:52 (running for 00:09:08.58)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:23:57 (running for 00:09:13.72)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n6\n452.454\n24000\n39.95\n100\n9\n39.95\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:02 (running for 00:09:19.13)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:07 (running for 00:09:24.41)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:13 (running for 00:09:29.49)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:18 (running for 00:09:34.71)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:23 (running for 00:09:39.84)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:28 (running for 00:09:45.06)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:33 (running for 00:09:50.14)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:38 (running for 00:09:55.34)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:43 (running for 00:10:00.44)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:49 (running for 00:10:05.64)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:24:54 (running for 00:10:10.74)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n7\n513.63\n28000\n34.3846\n93\n9\n34.3846\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:00 (running for 00:10:16.66)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:05 (running for 00:10:21.77)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:10 (running for 00:10:26.94)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:15 (running for 00:10:32.14)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:20 (running for 00:10:37.34)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:26 (running for 00:10:42.45)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:31 (running for 00:10:47.60)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:36 (running for 00:10:52.70)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:41 (running for 00:10:57.88)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:46 (running for 00:11:03.02)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:51 (running for 00:11:08.22)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:25:56 (running for 00:11:13.34)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n8\n573.102\n32000\n36.4495\n77\n11\n36.4495\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:02 (running for 00:11:19.16)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:07 (running for 00:11:24.27)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:13 (running for 00:11:29.46)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:18 (running for 00:11:34.59)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:23 (running for 00:11:39.78)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:28 (running for 00:11:44.93)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:33 (running for 00:11:50.05)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:38 (running for 00:11:55.15)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:43 (running for 00:12:00.31)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:48 (running for 00:12:05.38)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:54 (running for 00:12:10.58)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nRUNNING\n127.0.0.1:14344\n9\n633.545\n36000\n34.807\n93\n10\n34.807\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:26:57 (running for 00:12:13.80)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 TERMINATED)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_15f17_00000\nTERMINATED\n127.0.0.1:14344\n10\n693.094\n40000\n41.01\n91\n9\n41.01\n\n\n\n\n\n\n\nprint_reward(results3b)\n\nReward after 10 training iterations: 41.01\n\n\n\nplot_rewards(results3b)\n\nc:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\nplot_learning(results1, label=\"1: Full Observations\")\nplot_learning(results2, label=\"2: Partial Observations\")\nplot_learning(results3a, label=\"3a: Stacked, Partial Observations\")\nplot_learning(results3b, label=\"3b: LSTM\")\n\n\n\n\n\nLSTM with Stacked Observations\nUsing the StackedStatelessCartPole from above.\n\n#collapse-output\n\nconfig3b2 = ppo.DEFAULT_CONFIG.copy()\nconfig3b2[\"env\"] = \"StackedStatelessCartPole\"\nconfig3b2[\"model\"] = {\n    \"use_lstm\": True,\n}\n\nresults3b2 = ray.tune.run(\"PPO\", config=config3b2, stop=stop)\nprint(\"Option 3b2: Training finished successfully\")\n\n== Status ==Current time: 2021-12-01 23:29:25 (running for 00:00:00.15)Memory usage on this node: 9.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nPENDING\n\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:29:30 (running for 00:00:05.16)Memory usage on this node: 9.5/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nPENDING\n\n\n\n\n\n\n\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=10736) 2021-12-01 23:29:41,957 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n(pid=10736) 2021-12-01 23:29:41,957 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n(pid=10736) 2021-12-01 23:29:41,958 INFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=11688) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\gym\\spaces\\box.py:142: UserWarning: WARN: Casting input x to numpy array.\n(pid=11688)   logger.warn(\"Casting input x to numpy array.\")\n(pid=19560) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\gym\\spaces\\box.py:142: UserWarning: WARN: Casting input x to numpy array.\n(pid=19560)   logger.warn(\"Casting input x to numpy array.\")\n(pid=19560) 2021-12-01 23:29:54,372 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=10736) 2021-12-01 23:29:57,640 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n(pid=10736) 2021-12-01 23:29:59,755 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n(pid=10736) 2021-12-01 23:29:59,755 INFO trainable.py:110 -- Trainable.setup took 17.805 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n(pid=10736) 2021-12-01 23:29:59,755 WARNING util.py:57 -- Install gputil for GPU system monitoring.\n(pid=10736) 2021-12-01 23:30:05,789 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n(pid=10736) [2021-12-01 23:39:23,601 E 10736 16912] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=10736) Windows fatal exception: access violation\n(pid=10736) \n(pid=19560) [2021-12-01 23:39:23,630 C 19560 20340] core_worker.cc:796:  Check failed: _s.ok() Bad status: IOError: Unknown error\n(pid=19560) *** StackTrace Information ***\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyInit__raylet\n(pid=19560)     PyNumber_InPlaceLshift\n(pid=19560)     Py_CheckFunctionResult\n(pid=19560)     PyEval_EvalFrameDefault\n(pid=19560)     Py_CheckFunctionResult\n(pid=19560)     PyEval_EvalFrameDefault\n(pid=19560)     PyEval_EvalCodeWithName\n(pid=19560)     PyEval_EvalCodeEx\n(pid=19560)     PyEval_EvalCode\n(pid=19560)     PyArena_New\n(pid=19560)     PyArena_New\n(pid=19560)     PyRun_FileExFlags\n(pid=19560)     PyRun_SimpleFileExFlags\n(pid=19560)     PyRun_AnyFileExFlags\n(pid=19560)     Py_FatalError\n(pid=19560)     Py_RunMain\n(pid=19560)     Py_RunMain\n(pid=19560)     Py_Main\n(pid=19560)     BaseThreadInitThunk\n(pid=19560)     RtlUserThreadStart\n(pid=19560) \n(pid=11688) [2021-12-01 23:39:23,637 C 11688 12384] core_worker.cc:796:  Check failed: _s.ok() Bad status: IOError: Unknown error\n(pid=11688) *** StackTrace Information ***\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyInit__raylet\n(pid=11688)     PyNumber_InPlaceLshift\n(pid=11688)     Py_CheckFunctionResult\n(pid=11688)     PyEval_EvalFrameDefault\n(pid=11688)     Py_CheckFunctionResult\n(pid=11688)     PyEval_EvalFrameDefault\n(pid=11688)     PyEval_EvalCodeWithName\n(pid=11688)     PyEval_EvalCodeEx\n(pid=11688)     PyEval_EvalCode\n(pid=11688)     PyArena_New\n(pid=11688)     PyArena_New\n(pid=11688)     PyRun_FileExFlags\n(pid=11688)     PyRun_SimpleFileExFlags\n(pid=11688)     PyRun_AnyFileExFlags\n(pid=11688)     Py_FatalError\n(pid=11688)     Py_RunMain\n(pid=11688)     Py_RunMain\n(pid=11688)     Py_Main\n(pid=11688)     BaseThreadInitThunk\n(pid=11688)     RtlUserThreadStart\n(pid=11688) \n(pid=19560) Windows fatal exception: access violation\n(pid=19560) \n(pid=19560) Stack (most recent call first):\n(pid=19560)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\worker.py\", line 425 in main_loop\n(pid=19560)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\workers/default_worker.py\", line 218 in &lt;module&gt;\n(pid=11688) Windows fatal exception: access violation\n(pid=11688) \n(pid=11688) Stack (most recent call first):\n(pid=11688)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\worker.py\", line 425 in main_loop\n(pid=11688)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\workers/default_worker.py\", line 218 in &lt;module&gt;\n2021-12-01 23:39:23,731 INFO tune.py:630 -- Total run time: 598.23 seconds (597.55 seconds for the tuning loop).\n\n\n== Status ==Current time: 2021-12-01 23:29:59 (running for 00:00:34.23)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:00 (running for 00:00:35.26)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:05 (running for 00:00:40.36)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:11 (running for 00:00:45.58)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:16 (running for 00:00:50.71)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:21 (running for 00:00:55.84)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:26 (running for 00:01:00.96)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:31 (running for 00:01:06.11)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:36 (running for 00:01:11.19)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:41 (running for 00:01:16.30)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:46 (running for 00:01:21.41)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:30:52 (running for 00:01:26.58)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n\n\n\n\n\n\nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 4000\n  custom_metrics: {}\n  date: 2021-12-01_23-30-56\n  done: false\n  episode_len_mean: 23.75\n  episode_media: {}\n  episode_reward_max: 76.0\n  episode_reward_mean: 23.75\n  episode_reward_min: 8.0\n  episodes_this_iter: 168\n  episodes_total: 168\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.67047119140625\n          entropy_coeff: 0.0\n          kl: 0.01581866294145584\n          model: {}\n          policy_loss: -0.01895357482135296\n          total_loss: 154.9998321533203\n          vf_explained_var: -0.10604370385408401\n          vf_loss: 155.015625\n    num_agent_steps_sampled: 4000\n    num_agent_steps_trained: 4000\n    num_steps_sampled: 4000\n    num_steps_trained: 4000\n  iterations_since_restore: 1\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 92.95384615384614\n    ram_util_percent: 86.28846153846153\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.12315661179645192\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.14159590390983534\n    mean_inference_ms: 2.4392604373977185\n    mean_raw_obs_processing_ms: 0.21398437689550173\n  time_since_restore: 56.266863107681274\n  time_this_iter_s: 56.266863107681274\n  time_total_s: 56.266863107681274\n  timers:\n    learn_throughput: 79.628\n    learn_time_ms: 50233.338\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 662.919\n    sample_time_ms: 6033.918\n    update_time_ms: 0.0\n  timestamp: 1638397856\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 4000\n  training_iteration: 1\n  trial_id: 23a62_00000\n  \nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 8000\n  custom_metrics: {}\n  date: 2021-12-01_23-31-51\n  done: false\n  episode_len_mean: 26.83783783783784\n  episode_media: {}\n  episode_reward_max: 99.0\n  episode_reward_mean: 26.83783783783784\n  episode_reward_min: 9.0\n  episodes_this_iter: 148\n  episodes_total: 316\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6504567265510559\n          entropy_coeff: 0.0\n          kl: 0.008978299796581268\n          model: {}\n          policy_loss: -0.003679021494463086\n          total_loss: 127.99153137207031\n          vf_explained_var: 0.08064287155866623\n          vf_loss: 127.99342346191406\n    num_agent_steps_sampled: 8000\n    num_agent_steps_trained: 8000\n    num_steps_sampled: 8000\n    num_steps_trained: 8000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 2\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 93.6051948051948\n    ram_util_percent: 85.9285714285714\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11010479833490981\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1344388731949505\n    mean_inference_ms: 2.3811965957541155\n    mean_raw_obs_processing_ms: 0.23579910601161452\n  time_since_restore: 111.88376545906067\n  time_this_iter_s: 55.616902351379395\n  time_total_s: 111.88376545906067\n  timers:\n    learn_throughput: 79.946\n    learn_time_ms: 50033.46\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 128.91\n    sample_time_ms: 31029.425\n    update_time_ms: 7.819\n  timestamp: 1638397911\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 8000\n  training_iteration: 2\n  trial_id: 23a62_00000\n  \nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 12000\n  custom_metrics: {}\n  date: 2021-12-01_23-32-46\n  done: false\n  episode_len_mean: 31.140625\n  episode_media: {}\n  episode_reward_max: 84.0\n  episode_reward_mean: 31.140625\n  episode_reward_min: 9.0\n  episodes_this_iter: 128\n  episodes_total: 444\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6188111901283264\n          entropy_coeff: 0.0\n          kl: 0.01211484707891941\n          model: {}\n          policy_loss: -0.0035639703273773193\n          total_loss: 120.47854614257812\n          vf_explained_var: 0.1551436185836792\n          vf_loss: 120.47969055175781\n    num_agent_steps_sampled: 12000\n    num_agent_steps_trained: 12000\n    num_steps_sampled: 12000\n    num_steps_trained: 12000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 3\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 93.38815789473684\n    ram_util_percent: 85.94868421052632\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11705200286720863\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1417983632483218\n    mean_inference_ms: 2.3423456092602697\n    mean_raw_obs_processing_ms: 0.24562951880700387\n  time_since_restore: 167.125750541687\n  time_this_iter_s: 55.24198508262634\n  time_total_s: 167.125750541687\n  timers:\n    learn_throughput: 80.241\n    learn_time_ms: 49850.087\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 101.955\n    sample_time_ms: 39232.888\n    update_time_ms: 6.891\n  timestamp: 1638397966\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 12000\n  training_iteration: 3\n  trial_id: 23a62_00000\n  \nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 16000\n  custom_metrics: {}\n  date: 2021-12-01_23-33-42\n  done: false\n  episode_len_mean: 30.353383458646615\n  episode_media: {}\n  episode_reward_max: 90.0\n  episode_reward_mean: 30.353383458646615\n  episode_reward_min: 10.0\n  episodes_this_iter: 133\n  episodes_total: 577\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6017324328422546\n          entropy_coeff: 0.0\n          kl: 0.01713641546666622\n          model: {}\n          policy_loss: -0.01428857073187828\n          total_loss: 144.1490936279297\n          vf_explained_var: 0.12213249504566193\n          vf_loss: 144.15994262695312\n    num_agent_steps_sampled: 16000\n    num_agent_steps_trained: 16000\n    num_steps_sampled: 16000\n    num_steps_trained: 16000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 4\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 94.54285714285712\n    ram_util_percent: 85.35714285714288\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11847087716647563\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1291269029294712\n    mean_inference_ms: 2.3499539409316808\n    mean_raw_obs_processing_ms: 0.24600572504675797\n  time_since_restore: 222.71135187149048\n  time_this_iter_s: 55.58560132980347\n  time_total_s: 222.71135187149048\n  timers:\n    learn_throughput: 80.284\n    learn_time_ms: 49823.259\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 92.442\n    sample_time_ms: 43270.364\n    update_time_ms: 6.418\n  timestamp: 1638398022\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 16000\n  training_iteration: 4\n  trial_id: 23a62_00000\n  \nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 20000\n  custom_metrics: {}\n  date: 2021-12-01_23-34-39\n  done: false\n  episode_len_mean: 32.04032258064516\n  episode_media: {}\n  episode_reward_max: 95.0\n  episode_reward_mean: 32.04032258064516\n  episode_reward_min: 9.0\n  episodes_this_iter: 124\n  episodes_total: 701\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5604901909828186\n          entropy_coeff: 0.0\n          kl: 0.009820478968322277\n          model: {}\n          policy_loss: -0.0032165604643523693\n          total_loss: 100.9627914428711\n          vf_explained_var: 0.21908660233020782\n          vf_loss: 100.96404266357422\n    num_agent_steps_sampled: 20000\n    num_agent_steps_trained: 20000\n    num_steps_sampled: 20000\n    num_steps_trained: 20000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 5\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 95.19487179487179\n    ram_util_percent: 85.38333333333334\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11550202333653888\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12803896103049967\n    mean_inference_ms: 2.3503844656975446\n    mean_raw_obs_processing_ms: 0.24389184124832392\n  time_since_restore: 279.80817222595215\n  time_this_iter_s: 57.09682035446167\n  time_total_s: 279.80817222595215\n  timers:\n    learn_throughput: 79.803\n    learn_time_ms: 50123.572\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 87.481\n    sample_time_ms: 45724.201\n    update_time_ms: 6.735\n  timestamp: 1638398079\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 20000\n  training_iteration: 5\n  trial_id: 23a62_00000\n  \nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 24000\n  custom_metrics: {}\n  date: 2021-12-01_23-35-37\n  done: false\n  episode_len_mean: 34.64655172413793\n  episode_media: {}\n  episode_reward_max: 73.0\n  episode_reward_mean: 34.64655172413793\n  episode_reward_min: 9.0\n  episodes_this_iter: 116\n  episodes_total: 817\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5657119154930115\n          entropy_coeff: 0.0\n          kl: 0.013083796948194504\n          model: {}\n          policy_loss: -0.002956786658614874\n          total_loss: 113.45106506347656\n          vf_explained_var: 0.19632089138031006\n          vf_loss: 113.45140075683594\n    num_agent_steps_sampled: 24000\n    num_agent_steps_trained: 24000\n    num_steps_sampled: 24000\n    num_steps_trained: 24000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 6\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 96.2126582278481\n    ram_util_percent: 85.9253164556962\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11478686430048778\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.1277210771833874\n    mean_inference_ms: 2.3750658085342033\n    mean_raw_obs_processing_ms: 0.2452229849547805\n  time_since_restore: 337.58967638015747\n  time_this_iter_s: 57.78150415420532\n  time_total_s: 337.58967638015747\n  timers:\n    learn_throughput: 79.396\n    learn_time_ms: 50380.468\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 83.877\n    sample_time_ms: 47688.802\n    update_time_ms: 6.779\n  timestamp: 1638398137\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 24000\n  training_iteration: 6\n  trial_id: 23a62_00000\n  \nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 28000\n  custom_metrics: {}\n  date: 2021-12-01_23-36-34\n  done: false\n  episode_len_mean: 33.652542372881356\n  episode_media: {}\n  episode_reward_max: 80.0\n  episode_reward_mean: 33.652542372881356\n  episode_reward_min: 10.0\n  episodes_this_iter: 118\n  episodes_total: 935\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.55117267370224\n          entropy_coeff: 0.0\n          kl: 0.007611136883497238\n          model: {}\n          policy_loss: 0.007003166247159243\n          total_loss: 101.61392211914062\n          vf_explained_var: 0.2326377034187317\n          vf_loss: 101.60539245605469\n    num_agent_steps_sampled: 28000\n    num_agent_steps_trained: 28000\n    num_steps_sampled: 28000\n    num_steps_trained: 28000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 7\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 97.20759493670884\n    ram_util_percent: 83.11772151898732\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1171131524330598\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.13283579428360615\n    mean_inference_ms: 2.437068269845546\n    mean_raw_obs_processing_ms: 0.24942217294799132\n  time_since_restore: 394.7531487941742\n  time_this_iter_s: 57.163472414016724\n  time_total_s: 394.7531487941742\n  timers:\n    learn_throughput: 79.423\n    learn_time_ms: 50363.0\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 81.207\n    sample_time_ms: 49256.55\n    update_time_ms: 5.81\n  timestamp: 1638398194\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 28000\n  training_iteration: 7\n  trial_id: 23a62_00000\n  \nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 32000\n  custom_metrics: {}\n  date: 2021-12-01_23-37-31\n  done: false\n  episode_len_mean: 34.30769230769231\n  episode_media: {}\n  episode_reward_max: 83.0\n  episode_reward_mean: 34.30769230769231\n  episode_reward_min: 9.0\n  episodes_this_iter: 117\n  episodes_total: 1052\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5451725721359253\n          entropy_coeff: 0.0\n          kl: 0.009232791140675545\n          model: {}\n          policy_loss: -0.004543236922472715\n          total_loss: 98.80670928955078\n          vf_explained_var: 0.2929261326789856\n          vf_loss: 98.80941772460938\n    num_agent_steps_sampled: 32000\n    num_agent_steps_trained: 32000\n    num_steps_sampled: 32000\n    num_steps_trained: 32000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 8\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 95.07692307692308\n    ram_util_percent: 83.00512820512823\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11435559950687862\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12772642169718973\n    mean_inference_ms: 2.4294528410907237\n    mean_raw_obs_processing_ms: 0.2475755621430507\n  time_since_restore: 450.92096877098083\n  time_this_iter_s: 56.16781997680664\n  time_total_s: 450.92096877098083\n  timers:\n    learn_throughput: 79.407\n    learn_time_ms: 50373.586\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 79.838\n    sample_time_ms: 50101.286\n    update_time_ms: 5.584\n  timestamp: 1638398251\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 32000\n  training_iteration: 8\n  trial_id: 23a62_00000\n  \nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 36000\n  custom_metrics: {}\n  date: 2021-12-01_23-38-27\n  done: false\n  episode_len_mean: 39.45544554455446\n  episode_media: {}\n  episode_reward_max: 88.0\n  episode_reward_mean: 39.45544554455446\n  episode_reward_min: 10.0\n  episodes_this_iter: 101\n  episodes_total: 1153\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5534942746162415\n          entropy_coeff: 0.0\n          kl: 0.008644542656838894\n          model: {}\n          policy_loss: 0.000738372968044132\n          total_loss: 82.19181823730469\n          vf_explained_var: 0.3500000238418579\n          vf_loss: 82.18933868408203\n    num_agent_steps_sampled: 36000\n    num_agent_steps_trained: 36000\n    num_steps_sampled: 36000\n    num_steps_trained: 36000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 9\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 95.58076923076923\n    ram_util_percent: 82.91666666666666\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1135820346886012\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12853882753117699\n    mean_inference_ms: 2.4140773796778965\n    mean_raw_obs_processing_ms: 0.24722175705109586\n  time_since_restore: 507.0870122909546\n  time_this_iter_s: 56.166043519973755\n  time_total_s: 507.0870122909546\n  timers:\n    learn_throughput: 79.385\n    learn_time_ms: 50387.098\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 78.782\n    sample_time_ms: 50772.981\n    update_time_ms: 6.076\n  timestamp: 1638398307\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 36000\n  training_iteration: 9\n  trial_id: 23a62_00000\n  \nResult for PPO_StackedStatelessCartPole_23a62_00000:\n  agent_timesteps_total: 40000\n  custom_metrics: {}\n  date: 2021-12-01_23-39-23\n  done: true\n  episode_len_mean: 36.67272727272727\n  episode_media: {}\n  episode_reward_max: 80.0\n  episode_reward_mean: 36.67272727272727\n  episode_reward_min: 11.0\n  episodes_this_iter: 110\n  episodes_total: 1263\n  experiment_id: a83f6e57239f4aa1a70a247399bd5e70\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.5353969931602478\n          entropy_coeff: 0.0\n          kl: 0.007495984435081482\n          model: {}\n          policy_loss: -0.003161693923175335\n          total_loss: 78.63844299316406\n          vf_explained_var: 0.3598953187465668\n          vf_loss: 78.64009857177734\n    num_agent_steps_sampled: 40000\n    num_agent_steps_trained: 40000\n    num_steps_sampled: 40000\n    num_steps_trained: 40000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 10\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 94.4233766233766\n    ram_util_percent: 82.21298701298701\n  pid: 10736\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11409118320273612\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.13058247747347188\n    mean_inference_ms: 2.398278065616303\n    mean_raw_obs_processing_ms: 0.2469977443652814\n  time_since_restore: 562.8484704494476\n  time_this_iter_s: 55.76145815849304\n  time_total_s: 562.8484704494476\n  timers:\n    learn_throughput: 79.43\n    learn_time_ms: 50358.561\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 77.946\n    sample_time_ms: 51317.629\n    update_time_ms: 5.469\n  timestamp: 1638398363\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 40000\n  training_iteration: 10\n  trial_id: 23a62_00000\n  \nOption 3b2: Training finished successfully\n\n\n== Status ==Current time: 2021-12-01 23:30:59 (running for 00:01:33.56)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:04 (running for 00:01:38.73)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:09 (running for 00:01:43.86)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:14 (running for 00:01:49.03)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:19 (running for 00:01:54.11)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:24 (running for 00:01:59.28)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:29 (running for 00:02:04.40)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:35 (running for 00:02:09.57)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:40 (running for 00:02:14.67)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:45 (running for 00:02:19.82)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:50 (running for 00:02:24.91)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n1\n56.2669\n4000\n23.75\n76\n8\n23.75\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:31:55 (running for 00:02:30.26)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:00 (running for 00:02:35.31)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:06 (running for 00:02:40.49)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:11 (running for 00:02:45.57)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:16 (running for 00:02:50.74)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:23 (running for 00:02:57.84)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:28 (running for 00:03:03.01)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:33 (running for 00:03:08.13)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:38 (running for 00:03:13.31)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:43 (running for 00:03:18.38)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n2\n111.884\n8000\n26.8378\n99\n9\n26.8378\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:49 (running for 00:03:23.49)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:54 (running for 00:03:28.56)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:32:59 (running for 00:03:33.85)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:04 (running for 00:03:38.93)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:09 (running for 00:03:44.06)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:14 (running for 00:03:49.19)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:19 (running for 00:03:54.41)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:25 (running for 00:03:59.48)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:30 (running for 00:04:04.68)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:35 (running for 00:04:09.80)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:40 (running for 00:04:14.94)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n3\n167.126\n12000\n31.1406\n84\n9\n31.1406\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:45 (running for 00:04:20.12)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:50 (running for 00:04:25.23)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:33:55 (running for 00:04:30.30)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:00 (running for 00:04:35.44)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:06 (running for 00:04:40.56)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:11 (running for 00:04:45.73)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:16 (running for 00:04:50.88)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:21 (running for 00:04:56.00)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:26 (running for 00:05:01.12)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:31 (running for 00:05:06.30)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:36 (running for 00:05:11.38)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n4\n222.711\n16000\n30.3534\n90\n10\n30.3534\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:42 (running for 00:05:17.29)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:47 (running for 00:05:22.33)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:53 (running for 00:05:27.47)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:34:58 (running for 00:05:32.56)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:04 (running for 00:05:38.75)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:09 (running for 00:05:43.83)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:14 (running for 00:05:49.08)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:19 (running for 00:05:54.16)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:24 (running for 00:05:59.34)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:30 (running for 00:06:04.49)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:35 (running for 00:06:09.71)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n5\n279.808\n20000\n32.0403\n95\n9\n32.0403\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:40 (running for 00:06:15.17)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:45 (running for 00:06:20.23)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:50 (running for 00:06:25.36)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:35:56 (running for 00:06:30.51)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:01 (running for 00:06:35.67)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:06 (running for 00:06:40.82)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:11 (running for 00:06:45.91)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:16 (running for 00:06:51.04)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:21 (running for 00:06:56.15)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:26 (running for 00:07:01.36)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:33 (running for 00:07:07.50)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n6\n337.59\n24000\n34.6466\n73\n9\n34.6466\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:38 (running for 00:07:13.36)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:44 (running for 00:07:18.46)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:49 (running for 00:07:23.58)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:54 (running for 00:07:28.73)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:36:59 (running for 00:07:33.83)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:04 (running for 00:07:39.02)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:09 (running for 00:07:44.13)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:14 (running for 00:07:49.33)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:19 (running for 00:07:54.45)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:25 (running for 00:07:59.62)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:30 (running for 00:08:04.69)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n7\n394.753\n28000\n33.6525\n80\n10\n33.6525\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:36 (running for 00:08:10.57)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:41 (running for 00:08:15.68)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:46 (running for 00:08:20.81)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:51 (running for 00:08:25.95)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:37:56 (running for 00:08:31.11)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:01 (running for 00:08:36.23)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:06 (running for 00:08:41.36)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:13 (running for 00:08:47.46)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:18 (running for 00:08:52.61)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:23 (running for 00:08:57.71)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n8\n450.921\n32000\n34.3077\n83\n9\n34.3077\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:28 (running for 00:09:02.79)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:33 (running for 00:09:07.85)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:38 (running for 00:09:12.92)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:43 (running for 00:09:18.02)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:48 (running for 00:09:23.19)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:53 (running for 00:09:28.29)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:38:58 (running for 00:09:33.45)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:39:04 (running for 00:09:38.60)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:39:09 (running for 00:09:43.70)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:39:14 (running for 00:09:48.72)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:39:19 (running for 00:09:53.85)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nRUNNING\n127.0.0.1:10736\n9\n507.087\n36000\n39.4554\n88\n10\n39.4554\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:39:23 (running for 00:09:57.60)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 TERMINATED)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StackedStatelessCartPole_23a62_00000\nTERMINATED\n127.0.0.1:10736\n10\n562.848\n40000\n36.6727\n80\n11\n36.6727\n\n\n\n\n\n\n\nprint_reward(results3b2)\n\nReward after 10 training iterations: 36.67272727272727\n\n\n\nplot_rewards(results3b2)\n\nc:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\nplot_learning(results3a, label=\"3a: Stacked, Partial Observations\")\nplot_learning(results3b, label=\"3b: LSTM\")\nplot_learning(results3b2, label=\"3b2: LSTM + Stacking\")\n\n\n\n\n\n\n\nOption 3c: Use Attention for Processing the Sequence\nSelf-attention is a recent and popular alternative to RNNs for processing sequence data. Currently, the transformer architecture using self-attention is state of the art for natural language processing (NLP) tasks.\nA similar, yet slightly modified architecture using attention is also useful for RL (see related paper). Again, enabling attention in RLlib simply requires setting the corresponding flag in the model config:\n\n#collapse-output\n\nconfig3c = ppo.DEFAULT_CONFIG.copy()\nconfig3c[\"env\"] = \"StatelessCartPole\"\nconfig3c[\"model\"] = {\n    # Attention net wrapping (for tf) can already use the native keras\n    # model versions. For torch, this will have no effect.\n    \"_use_default_native_models\": True,\n    \"use_attention\": True,\n    # \"max_seq_len\": 10,\n    # \"attention_num_transformer_units\": 1,\n    # \"attention_dim\": 32,\n    # \"attention_memory_inference\": 10,\n    # \"attention_memory_training\": 10,\n    # \"attention_num_heads\": 1,\n    # \"attention_head_dim\": 32,\n    # \"attention_position_wise_mlp_dim\": 32,\n}\n\nresults3c = ray.tune.run(\"PPO\", config=config3c, stop=stop)\nprint(\"Option 3c: Training finished successfully\")\n\n== Status ==Current time: 2021-12-01 23:39:24 (running for 00:00:00.15)Memory usage on this node: 8.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nPENDING\n\n\n\n\n\n\n\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=12464) 2021-12-01 23:39:35,779 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n(pid=12464) 2021-12-01 23:39:35,780 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n(pid=12464) 2021-12-01 23:39:35,780 INFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=None) c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\redis\\connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n(pid=None)   warnings.warn(msg)\n(pid=12464) 2021-12-01 23:40:01,112 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n(pid=12464) 2021-12-01 23:40:01,115 INFO trainable.py:110 -- Trainable.setup took 25.341 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n(pid=12464) 2021-12-01 23:40:01,117 WARNING util.py:57 -- Install gputil for GPU system monitoring.\n(pid=12464) 2021-12-01 23:40:08,500 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n(pid=12464) [2021-12-01 23:43:32,273 E 12464 11260] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=12464) Windows fatal exception: access violation\n(pid=12464) \n(pid=15896) [2021-12-01 23:43:32,278 E 15896 15972] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet.\n(pid=15896) Windows fatal exception: access violation\n(pid=15896) \n(pid=17956) [2021-12-01 23:43:32,288 C 17956 2592] core_worker.cc:796:  Check failed: _s.ok() Bad status: IOError: Unknown error\n(pid=17956) *** StackTrace Information ***\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyInit__raylet\n(pid=17956)     PyNumber_InPlaceLshift\n(pid=17956)     Py_CheckFunctionResult\n(pid=17956)     PyEval_EvalFrameDefault\n(pid=17956)     Py_CheckFunctionResult\n(pid=17956)     PyEval_EvalFrameDefault\n(pid=17956)     PyEval_EvalCodeWithName\n(pid=17956)     PyEval_EvalCodeEx\n(pid=17956)     PyEval_EvalCode\n(pid=17956)     PyArena_New\n(pid=17956)     PyArena_New\n(pid=17956)     PyRun_FileExFlags\n(pid=17956)     PyRun_SimpleFileExFlags\n(pid=17956)     PyRun_AnyFileExFlags\n(pid=17956)     Py_FatalError\n(pid=17956)     Py_RunMain\n(pid=17956)     Py_RunMain\n(pid=17956)     Py_Main\n(pid=17956)     BaseThreadInitThunk\n(pid=17956)     RtlUserThreadStart\n(pid=17956) \n(pid=17956) Windows fatal exception: access violation\n(pid=17956) \n(pid=17956) Stack (most recent call first):\n(pid=17956)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\worker.py\", line 425 in main_loop\n(pid=17956)   File \"c:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\ray\\workers/default_worker.py\", line 218 in &lt;module&gt;\n2021-12-01 23:43:32,411 INFO tune.py:630 -- Total run time: 248.18 seconds (247.76 seconds for the tuning loop).\n\n\n== Status ==Current time: 2021-12-01 23:39:29 (running for 00:00:05.16)Memory usage on this node: 8.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 PENDING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nPENDING\n\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:01 (running for 00:00:36.89)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:02 (running for 00:00:37.93)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:07 (running for 00:00:43.01)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:12 (running for 00:00:48.07)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:17 (running for 00:00:53.12)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n\n\n\n\n\n\nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 4000\n  custom_metrics: {}\n  date: 2021-12-01_23-40-21\n  done: false\n  episode_len_mean: 23.939393939393938\n  episode_media: {}\n  episode_reward_max: 76.0\n  episode_reward_mean: 23.939393939393938\n  episode_reward_min: 10.0\n  episodes_this_iter: 165\n  episodes_total: 165\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6663342118263245\n          entropy_coeff: 0.0\n          kl: 0.01937255822122097\n          policy_loss: -0.013014732860028744\n          total_loss: 154.01339721679688\n          vf_explained_var: 0.006343733984977007\n          vf_loss: 154.0225372314453\n    num_agent_steps_sampled: 4000\n    num_agent_steps_trained: 4000\n    num_steps_sampled: 4000\n    num_steps_trained: 4000\n  iterations_since_restore: 1\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 70.58666666666666\n    ram_util_percent: 83.49666666666667\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10596248134570115\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12798089296365409\n    mean_inference_ms: 3.0966986549503033\n    mean_raw_obs_processing_ms: 0.1998490762828277\n  time_since_restore: 20.76484441757202\n  time_this_iter_s: 20.76484441757202\n  time_total_s: 20.76484441757202\n  timers:\n    learn_throughput: 299.129\n    learn_time_ms: 13372.16\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 541.375\n    sample_time_ms: 7388.589\n    update_time_ms: 4.002\n  timestamp: 1638398421\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 4000\n  training_iteration: 1\n  trial_id: '88823_00000'\n  \nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 8000\n  custom_metrics: {}\n  date: 2021-12-01_23-40-40\n  done: false\n  episode_len_mean: 27.791666666666668\n  episode_media: {}\n  episode_reward_max: 122.0\n  episode_reward_mean: 27.791666666666668\n  episode_reward_min: 9.0\n  episodes_this_iter: 144\n  episodes_total: 309\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.20000000298023224\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6823218464851379\n          entropy_coeff: 0.0\n          kl: 0.023256205022335052\n          policy_loss: 0.006493973080068827\n          total_loss: 161.61268615722656\n          vf_explained_var: 0.0007292712107300758\n          vf_loss: 161.60154724121094\n    num_agent_steps_sampled: 8000\n    num_agent_steps_trained: 8000\n    num_steps_sampled: 8000\n    num_steps_trained: 8000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 2\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 73.93076923076923\n    ram_util_percent: 83.53846153846153\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.11035390714468313\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11674551834165348\n    mean_inference_ms: 3.0091454225371876\n    mean_raw_obs_processing_ms: 0.1931061975500896\n  time_since_restore: 39.707205057144165\n  time_this_iter_s: 18.942360639572144\n  time_total_s: 39.707205057144165\n  timers:\n    learn_throughput: 313.757\n    learn_time_ms: 12748.702\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 289.977\n    sample_time_ms: 13794.207\n    update_time_ms: 4.002\n  timestamp: 1638398440\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 8000\n  training_iteration: 2\n  trial_id: '88823_00000'\n  \nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 12000\n  custom_metrics: {}\n  date: 2021-12-01_23-40-59\n  done: false\n  episode_len_mean: 23.017142857142858\n  episode_media: {}\n  episode_reward_max: 66.0\n  episode_reward_mean: 23.017142857142858\n  episode_reward_min: 8.0\n  episodes_this_iter: 175\n  episodes_total: 484\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.30000001192092896\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6649836897850037\n          entropy_coeff: 0.0\n          kl: 0.018709277734160423\n          policy_loss: -0.007455786690115929\n          total_loss: 69.26802062988281\n          vf_explained_var: -0.04406118765473366\n          vf_loss: 69.26985931396484\n    num_agent_steps_sampled: 12000\n    num_agent_steps_trained: 12000\n    num_steps_sampled: 12000\n    num_steps_trained: 12000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 3\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 70.76666666666667\n    ram_util_percent: 83.59629629629629\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10152428396558573\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11275608464037909\n    mean_inference_ms: 2.987558953459597\n    mean_raw_obs_processing_ms: 0.18824034213713292\n  time_since_restore: 58.64493227005005\n  time_this_iter_s: 18.937727212905884\n  time_total_s: 58.64493227005005\n  timers:\n    learn_throughput: 318.575\n    learn_time_ms: 12555.928\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 257.773\n    sample_time_ms: 15517.557\n    update_time_ms: 2.668\n  timestamp: 1638398459\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 12000\n  training_iteration: 3\n  trial_id: '88823_00000'\n  \nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 16000\n  custom_metrics: {}\n  date: 2021-12-01_23-41-20\n  done: false\n  episode_len_mean: 27.27891156462585\n  episode_media: {}\n  episode_reward_max: 65.0\n  episode_reward_mean: 27.27891156462585\n  episode_reward_min: 10.0\n  episodes_this_iter: 147\n  episodes_total: 631\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.30000001192092896\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.666556715965271\n          entropy_coeff: 0.0\n          kl: 0.021740607917308807\n          policy_loss: 0.0014071379555389285\n          total_loss: 86.13334655761719\n          vf_explained_var: -0.020127560943365097\n          vf_loss: 86.12541961669922\n    num_agent_steps_sampled: 16000\n    num_agent_steps_trained: 16000\n    num_steps_sampled: 16000\n    num_steps_trained: 16000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 4\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 77.01379310344828\n    ram_util_percent: 83.93103448275863\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.10143838749352374\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.10536623976424975\n    mean_inference_ms: 2.9572038509498433\n    mean_raw_obs_processing_ms: 0.2044611579055872\n  time_since_restore: 79.04436016082764\n  time_this_iter_s: 20.399427890777588\n  time_total_s: 79.04436016082764\n  timers:\n    learn_throughput: 311.522\n    learn_time_ms: 12840.182\n    load_throughput: 0.0\n    load_time_ms: 0.0\n    sample_throughput: 244.428\n    sample_time_ms: 16364.753\n    update_time_ms: 3.002\n  timestamp: 1638398480\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 16000\n  training_iteration: 4\n  trial_id: '88823_00000'\n  \nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 20000\n  custom_metrics: {}\n  date: 2021-12-01_23-41-45\n  done: false\n  episode_len_mean: 23.446428571428573\n  episode_media: {}\n  episode_reward_max: 102.0\n  episode_reward_mean: 23.446428571428573\n  episode_reward_min: 9.0\n  episodes_this_iter: 168\n  episodes_total: 799\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.44999998807907104\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6605092287063599\n          entropy_coeff: 0.0\n          kl: 0.019639955833554268\n          policy_loss: -0.012088990770280361\n          total_loss: 87.5208740234375\n          vf_explained_var: -0.08362725377082825\n          vf_loss: 87.52411651611328\n    num_agent_steps_sampled: 20000\n    num_agent_steps_trained: 20000\n    num_steps_sampled: 20000\n    num_steps_trained: 20000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 5\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 89.04\n    ram_util_percent: 84.47428571428571\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1082024611015279\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.11219990467946991\n    mean_inference_ms: 3.1819671927268045\n    mean_raw_obs_processing_ms: 0.2181941025366806\n  time_since_restore: 104.5215425491333\n  time_this_iter_s: 25.477182388305664\n  time_total_s: 104.5215425491333\n  timers:\n    learn_throughput: 297.013\n    learn_time_ms: 13467.402\n    load_throughput: 19987152.728\n    load_time_ms: 0.2\n    sample_throughput: 225.522\n    sample_time_ms: 17736.596\n    update_time_ms: 2.401\n  timestamp: 1638398505\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 20000\n  training_iteration: 5\n  trial_id: '88823_00000'\n  \nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 24000\n  custom_metrics: {}\n  date: 2021-12-01_23-42-12\n  done: false\n  episode_len_mean: 28.964285714285715\n  episode_media: {}\n  episode_reward_max: 94.0\n  episode_reward_mean: 28.964285714285715\n  episode_reward_min: 9.0\n  episodes_this_iter: 140\n  episodes_total: 939\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.44999998807907104\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6493598818778992\n          entropy_coeff: 0.0\n          kl: 0.008863288909196854\n          policy_loss: -0.009960012510418892\n          total_loss: 130.17213439941406\n          vf_explained_var: -0.04542897269129753\n          vf_loss: 130.1781005859375\n    num_agent_steps_sampled: 24000\n    num_agent_steps_trained: 24000\n    num_steps_sampled: 24000\n    num_steps_trained: 24000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 6\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 87.96944444444443\n    ram_util_percent: 84.64722222222221\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.12465831386460766\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12816464580125522\n    mean_inference_ms: 3.389497130627828\n    mean_raw_obs_processing_ms: 0.2190003983187485\n  time_since_restore: 130.94173955917358\n  time_this_iter_s: 26.420197010040283\n  time_total_s: 130.94173955917358\n  timers:\n    learn_throughput: 287.926\n    learn_time_ms: 13892.464\n    load_throughput: 11985152.518\n    load_time_ms: 0.334\n    sample_throughput: 208.532\n    sample_time_ms: 19181.747\n    update_time_ms: 4.777\n  timestamp: 1638398532\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 24000\n  training_iteration: 6\n  trial_id: '88823_00000'\n  \nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 28000\n  custom_metrics: {}\n  date: 2021-12-01_23-42-32\n  done: false\n  episode_len_mean: 35.74107142857143\n  episode_media: {}\n  episode_reward_max: 111.0\n  episode_reward_mean: 35.74107142857143\n  episode_reward_min: 10.0\n  episodes_this_iter: 112\n  episodes_total: 1051\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.44999998807907104\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6521294116973877\n          entropy_coeff: 0.0\n          kl: 0.008087929338216782\n          policy_loss: -0.001228039851412177\n          total_loss: 156.5521697998047\n          vf_explained_var: -0.018433474004268646\n          vf_loss: 156.5497589111328\n    num_agent_steps_sampled: 28000\n    num_agent_steps_trained: 28000\n    num_steps_sampled: 28000\n    num_steps_trained: 28000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 7\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 79.4392857142857\n    ram_util_percent: 84.67857142857142\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1278596423707056\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12899242071720216\n    mean_inference_ms: 3.353225638041823\n    mean_raw_obs_processing_ms: 0.2171522871719407\n  time_since_restore: 151.05801963806152\n  time_this_iter_s: 20.11628007888794\n  time_total_s: 151.05801963806152\n  timers:\n    learn_throughput: 291.389\n    learn_time_ms: 13727.331\n    load_throughput: 9323635.44\n    load_time_ms: 0.429\n    sample_throughput: 202.114\n    sample_time_ms: 19790.796\n    update_time_ms: 4.81\n  timestamp: 1638398552\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 28000\n  training_iteration: 7\n  trial_id: '88823_00000'\n  \nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 32000\n  custom_metrics: {}\n  date: 2021-12-01_23-42-52\n  done: false\n  episode_len_mean: 28.52857142857143\n  episode_media: {}\n  episode_reward_max: 117.0\n  episode_reward_mean: 28.52857142857143\n  episode_reward_min: 8.0\n  episodes_this_iter: 140\n  episodes_total: 1191\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.44999998807907104\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.6189269423484802\n          entropy_coeff: 0.0\n          kl: 0.010187552310526371\n          policy_loss: -0.01204092800617218\n          total_loss: 135.2486572265625\n          vf_explained_var: -0.08293487131595612\n          vf_loss: 135.25611877441406\n    num_agent_steps_sampled: 32000\n    num_agent_steps_trained: 32000\n    num_steps_sampled: 32000\n    num_steps_trained: 32000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 8\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 80.66551724137932\n    ram_util_percent: 84.77931034482759\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.12512978011718637\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12649312220671538\n    mean_inference_ms: 3.3173461928775416\n    mean_raw_obs_processing_ms: 0.21560295664752305\n  time_since_restore: 171.38141465187073\n  time_this_iter_s: 20.323395013809204\n  time_total_s: 171.38141465187073\n  timers:\n    learn_throughput: 292.724\n    learn_time_ms: 13664.765\n    load_throughput: 7989150.476\n    load_time_ms: 0.501\n    sample_throughput: 202.015\n    sample_time_ms: 19800.559\n    update_time_ms: 4.709\n  timestamp: 1638398572\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 32000\n  training_iteration: 8\n  trial_id: '88823_00000'\n  \nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 36000\n  custom_metrics: {}\n  date: 2021-12-01_23-43-12\n  done: false\n  episode_len_mean: 35.785714285714285\n  episode_media: {}\n  episode_reward_max: 91.0\n  episode_reward_mean: 35.785714285714285\n  episode_reward_min: 10.0\n  episodes_this_iter: 112\n  episodes_total: 1303\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.44999998807907104\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.616661548614502\n          entropy_coeff: 0.0\n          kl: 0.00952129065990448\n          policy_loss: -0.00046885418123565614\n          total_loss: 135.04832458496094\n          vf_explained_var: -0.05452437326312065\n          vf_loss: 135.0445098876953\n    num_agent_steps_sampled: 36000\n    num_agent_steps_trained: 36000\n    num_steps_sampled: 36000\n    num_steps_trained: 36000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 9\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 76.94814814814812\n    ram_util_percent: 84.89629629629631\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.1238827989880333\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12481765181822614\n    mean_inference_ms: 3.2899851537455973\n    mean_raw_obs_processing_ms: 0.21254970774456478\n  time_since_restore: 190.8527319431305\n  time_this_iter_s: 19.471317291259766\n  time_total_s: 190.8527319431305\n  timers:\n    learn_throughput: 295.82\n    learn_time_ms: 13521.73\n    load_throughput: 8987794.286\n    load_time_ms: 0.445\n    sample_throughput: 201.373\n    sample_time_ms: 19863.604\n    update_time_ms: 4.629\n  timestamp: 1638398592\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 36000\n  training_iteration: 9\n  trial_id: '88823_00000'\n  \nResult for PPO_StatelessCartPole_88823_00000:\n  agent_timesteps_total: 40000\n  custom_metrics: {}\n  date: 2021-12-01_23-43-31\n  done: true\n  episode_len_mean: 35.06140350877193\n  episode_media: {}\n  episode_reward_max: 147.0\n  episode_reward_mean: 35.06140350877193\n  episode_reward_min: 10.0\n  episodes_this_iter: 114\n  episodes_total: 1417\n  experiment_id: 7e1494afa3414dd998e7cde489d370fd\n  hostname: nb-stschn\n  info:\n    learner:\n      default_policy:\n        custom_metrics: {}\n        learner_stats:\n          cur_kl_coeff: 0.44999998807907104\n          cur_lr: 4.999999873689376e-05\n          entropy: 0.610243558883667\n          entropy_coeff: 0.0\n          kl: 0.0024006376042962074\n          policy_loss: -0.00499696284532547\n          total_loss: 148.4215850830078\n          vf_explained_var: -0.0558871254324913\n          vf_loss: 148.42550659179688\n    num_agent_steps_sampled: 40000\n    num_agent_steps_trained: 40000\n    num_steps_sampled: 40000\n    num_steps_trained: 40000\n    num_steps_trained_this_iter: 0\n  iterations_since_restore: 10\n  node_ip: 127.0.0.1\n  num_healthy_workers: 2\n  off_policy_estimator: {}\n  perf:\n    cpu_util_percent: 78.55714285714284\n    ram_util_percent: 84.91785714285713\n  pid: 12464\n  policy_reward_max: {}\n  policy_reward_mean: {}\n  policy_reward_min: {}\n  sampler_perf:\n    mean_action_processing_ms: 0.12214391459572835\n    mean_env_render_ms: 0.0\n    mean_env_wait_ms: 0.12507529134994974\n    mean_inference_ms: 3.262920007962691\n    mean_raw_obs_processing_ms: 0.21362642628938813\n  time_since_restore: 210.48571395874023\n  time_this_iter_s: 19.63298201560974\n  time_total_s: 210.48571395874023\n  timers:\n    learn_throughput: 298.029\n    learn_time_ms: 13421.519\n    load_throughput: 9986438.095\n    load_time_ms: 0.401\n    sample_throughput: 201.72\n    sample_time_ms: 19829.44\n    update_time_ms: 4.566\n  timestamp: 1638398611\n  timesteps_since_restore: 0\n  timesteps_this_iter: 0\n  timesteps_total: 40000\n  training_iteration: 10\n  trial_id: '88823_00000'\n  \nOption 3c: Training finished successfully\n\n\n== Status ==Current time: 2021-12-01 23:40:22 (running for 00:00:58.71)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n1\n20.7648\n4000\n23.9394\n76\n10\n23.9394\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:27 (running for 00:01:03.74)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n1\n20.7648\n4000\n23.9394\n76\n10\n23.9394\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:33 (running for 00:01:08.80)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n1\n20.7648\n4000\n23.9394\n76\n10\n23.9394\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:38 (running for 00:01:13.84)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n1\n20.7648\n4000\n23.9394\n76\n10\n23.9394\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:43 (running for 00:01:19.70)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n2\n39.7072\n8000\n27.7917\n122\n9\n27.7917\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:49 (running for 00:01:25.75)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n2\n39.7072\n8000\n27.7917\n122\n9\n27.7917\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:40:56 (running for 00:01:31.81)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n2\n39.7072\n8000\n27.7917\n122\n9\n27.7917\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:01 (running for 00:01:37.66)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n3\n58.6449\n12000\n23.0171\n66\n8\n23.0171\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:07 (running for 00:01:43.73)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n3\n58.6449\n12000\n23.0171\n66\n8\n23.0171\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:13 (running for 00:01:48.78)Memory usage on this node: 10.0/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n3\n58.6449\n12000\n23.0171\n66\n8\n23.0171\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:18 (running for 00:01:54.22)Memory usage on this node: 10.0/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n3\n58.6449\n12000\n23.0171\n66\n8\n23.0171\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:24 (running for 00:02:00.18)Memory usage on this node: 10.0/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n4\n79.0444\n16000\n27.2789\n65\n10\n27.2789\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:29 (running for 00:02:05.30)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n4\n79.0444\n16000\n27.2789\n65\n10\n27.2789\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:34 (running for 00:02:10.38)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n4\n79.0444\n16000\n27.2789\n65\n10\n27.2789\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:39 (running for 00:02:15.49)Memory usage on this node: 10.0/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n4\n79.0444\n16000\n27.2789\n65\n10\n27.2789\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:44 (running for 00:02:20.59)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n4\n79.0444\n16000\n27.2789\n65\n10\n27.2789\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:49 (running for 00:02:25.71)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n5\n104.522\n20000\n23.4464\n102\n9\n23.4464\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:41:55 (running for 00:02:30.94)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n5\n104.522\n20000\n23.4464\n102\n9\n23.4464\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:00 (running for 00:02:36.06)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n5\n104.522\n20000\n23.4464\n102\n9\n23.4464\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:05 (running for 00:02:41.16)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n5\n104.522\n20000\n23.4464\n102\n9\n23.4464\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:10 (running for 00:02:46.26)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n5\n104.522\n20000\n23.4464\n102\n9\n23.4464\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:16 (running for 00:02:52.11)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n6\n130.942\n24000\n28.9643\n94\n9\n28.9643\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:21 (running for 00:02:57.22)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n6\n130.942\n24000\n28.9643\n94\n9\n28.9643\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:26 (running for 00:03:02.28)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n6\n130.942\n24000\n28.9643\n94\n9\n28.9643\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:31 (running for 00:03:07.37)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n6\n130.942\n24000\n28.9643\n94\n9\n28.9643\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:37 (running for 00:03:13.23)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n7\n151.058\n28000\n35.7411\n111\n10\n35.7411\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:42 (running for 00:03:18.32)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n7\n151.058\n28000\n35.7411\n111\n10\n35.7411\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:47 (running for 00:03:23.38)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n7\n151.058\n28000\n35.7411\n111\n10\n35.7411\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:52 (running for 00:03:28.45)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n7\n151.058\n28000\n35.7411\n111\n10\n35.7411\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:42:57 (running for 00:03:33.61)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n8\n171.381\n32000\n28.5286\n117\n8\n28.5286\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:43:02 (running for 00:03:38.66)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n8\n171.381\n32000\n28.5286\n117\n8\n28.5286\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:43:07 (running for 00:03:43.70)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n8\n171.381\n32000\n28.5286\n117\n8\n28.5286\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:43:13 (running for 00:03:49.08)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n9\n190.853\n36000\n35.7857\n91\n10\n35.7857\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:43:18 (running for 00:03:54.13)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n9\n190.853\n36000\n35.7857\n91\n10\n35.7857\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:43:23 (running for 00:03:59.18)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n9\n190.853\n36000\n35.7857\n91\n10\n35.7857\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:43:28 (running for 00:04:04.24)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 RUNNING)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nRUNNING\n127.0.0.1:12464\n9\n190.853\n36000\n35.7857\n91\n10\n35.7857\n\n\n\n\n\n\n== Status ==Current time: 2021-12-01 23:43:32 (running for 00:04:07.82)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.65 GiB objectsResult logdir: C:\\Users\\Stefan\\ray_results\\PPONumber of trials: 1/1 (1 TERMINATED)\n\n\n\nTrial name\nstatus\nloc\niter\ntotal time (s)\nts\nreward\nepisode_reward_max\nepisode_reward_min\nepisode_len_mean\n\n\n\n\nPPO_StatelessCartPole_88823_00000\nTERMINATED\n127.0.0.1:12464\n10\n210.486\n40000\n35.0614\n147\n10\n35.0614\n\n\n\n\n\n\n\nprint_reward(results3c)\n\nReward after 10 training iterations: 35.06140350877193\n\n\n\nplot_rewards(results3c)\n\nc:\\users\\stefan\\git-repos\\private\\blog\\venv\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\nplot_learning(results1, label=\"1: Full Observations\")\nplot_learning(results2, label=\"2: Partial Observations\")\nplot_learning(results3a, label=\"3a: Stacked, Partial Observations\")\nplot_learning(results3b, label=\"3b: LSTM\")\nplot_learning(results3c, label=\"3c: Attention\")\n\n\n\n\n\nAttention with Stacked Observations\n\n\n\n\n\n\nImportant\n\n\n\nThis blog post is still work in progress. Currently, there seems to be an issue with attention in RLlib."
  },
  {
    "objectID": "posts/deepcomp/index.html",
    "href": "posts/deepcomp/index.html",
    "title": "Cell Selection with Deep Reinforcement Learning",
    "section": "",
    "text": "5G tower image by Christoph Scholz under CC BY-SA 2.0 license.\nIn the last year, I have worked a lot on mobility management and cell selection in mobile networks, which is very relevant for upcoming 5G networks and beyond. In this blog post, I try to give a brief, high-level overview of my research, describing what cell selection is, why it is important yet challenging. I also outline how I solved the cell selection problem with modern self-learning techniques and visualize the outcomes.\nThe whole blog post is meant for people who are interested but do not have a scientific background or extensive knowledge in the area. The corresponding research paper is accepted at 2023 IEEE Transactions on Network and Service Management (TNSM); a preprint is available here.\nEither way, if something is unclear, or you have open questions, feel free to reach out to me. My contact information is on my website."
  },
  {
    "objectID": "posts/deepcomp/index.html#centralized-cell-selection-with-deepcomp",
    "href": "posts/deepcomp/index.html#centralized-cell-selection-with-deepcomp",
    "title": "Cell Selection with Deep Reinforcement Learning",
    "section": "Centralized cell selection with DeepCoMP",
    "text": "Centralized cell selection with DeepCoMP\nAs a centralized approach, DeepCoMP assumes global knowledge and control of all users. This means, DeepCoMP needs to collect information from all users in a central location, e.g., about users’ current connections, signal strength, etc. Similarly, DeepCoMP represents a single, centralized entity that selects cells for all users simultaneously. This is illustrated in the figure below.\n\n\n\nCentralized approach: DeepCoMP"
  },
  {
    "objectID": "posts/deepcomp/index.html#distributed-cell-selection-with-dd-comp-and-d3-comp",
    "href": "posts/deepcomp/index.html#distributed-cell-selection-with-dd-comp-and-d3-comp",
    "title": "Cell Selection with Deep Reinforcement Learning",
    "section": "Distributed cell selection with DD-CoMP and D3-CoMP",
    "text": "Distributed cell selection with DD-CoMP and D3-CoMP\nIn addition to the centralized DeepCoMP approach, I also propose two distributed approaches, called DD-CoMP and D3-CoMP. These approaches use multi-agent reinforcement learning, where multiple “agents” select cells independently in parallel. Particularly, I assume one agent per user, which observes the users current connections, signal strength, etc. and selects suitable cells. Because each agent only cares about its own user, it is much simpler to implement and use than the centralized DeepCoMP approach. This is visualized in the figure below.\n\n\n\nDistributed approaches: DD-CoMP and D3-CoMP\n\n\nIn reinforcement learning, such agents are initially trained, where they test and learn from many different actions, and then apply their learned policy (in the “inference” phase). In DD-CoMP training experience from all users is combined to learn cell selection more efficiently. In D3-CoMP each agent trains independently from all other agents and only uses experience from its own user. This allows to reduce communication between agents."
  },
  {
    "objectID": "posts/test-markdown-post/index.html",
    "href": "posts/test-markdown-post/index.html",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Note\n\n\n\nThis post was generated by fastpages and kept for reference.\nNote that the blog was later moved from fastpages to Quarto and this page was adjusted. Still not all tips may be up to date anymore. More useful tips on authoring with Markdown are in the Quarto Guide."
  },
  {
    "objectID": "posts/test-markdown-post/index.html#basic-setup",
    "href": "posts/test-markdown-post/index.html#basic-setup",
    "title": "An Example Markdown Post",
    "section": "Basic setup",
    "text": "Basic setup\nJekyll requires blog post files to be named according to the following format:\nYEAR-MONTH-DAY-filename.md\nWhere YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files.\nThe first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above."
  },
  {
    "objectID": "posts/test-markdown-post/index.html#basic-formatting",
    "href": "posts/test-markdown-post/index.html#basic-formatting",
    "title": "An Example Markdown Post",
    "section": "Basic formatting",
    "text": "Basic formatting\nYou can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule:"
  },
  {
    "objectID": "posts/test-markdown-post/index.html#lists",
    "href": "posts/test-markdown-post/index.html#lists",
    "title": "An Example Markdown Post",
    "section": "Lists",
    "text": "Lists\nHere’s a list:\n\nitem 1\nitem 2\n\nAnd a numbered list:\n\nitem 1\nitem 2"
  },
  {
    "objectID": "posts/test-markdown-post/index.html#callout-boxes-and-stuff",
    "href": "posts/test-markdown-post/index.html#callout-boxes-and-stuff",
    "title": "An Example Markdown Post",
    "section": "Callout Boxes and stuff",
    "text": "Callout Boxes and stuff\n\nThis is a quotation\n\n\n\n\n\n\n\nTip\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\nand\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "posts/test-markdown-post/index.html#images",
    "href": "posts/test-markdown-post/index.html#images",
    "title": "An Example Markdown Post",
    "section": "Images",
    "text": "Images\n\n\n\n“fast.ai’s logo”"
  },
  {
    "objectID": "posts/test-markdown-post/index.html#code",
    "href": "posts/test-markdown-post/index.html#code",
    "title": "An Example Markdown Post",
    "section": "Code",
    "text": "Code\nYou can format text and code per usual\nGeneral preformatted text:\n# Do a thing\ndo_thing()\nPython code and output:\n# Prints '2'\nprint(1+1)\n2\nFormatting text as shell commands:\necho \"hello world\"\n./some_script.sh --option \"value\"\nwget https://example.com/cat_photo1.png\nFormatting text as YAML:\nkey: value\n- another_key: \"another value\""
  },
  {
    "objectID": "posts/test-markdown-post/index.html#tables",
    "href": "posts/test-markdown-post/index.html#tables",
    "title": "An Example Markdown Post",
    "section": "Tables",
    "text": "Tables\n\n\n\nColumn 1\nColumn 2\n\n\n\n\nA thing\nAnother thing"
  },
  {
    "objectID": "posts/test-markdown-post/index.html#equations",
    "href": "posts/test-markdown-post/index.html#equations",
    "title": "An Example Markdown Post",
    "section": "Equations",
    "text": "Equations\nInline: \\(E = mc^2\\) And display math:\n\\[\\alpha \\beta \\gamma\\]"
  },
  {
    "objectID": "posts/test-markdown-post/index.html#footnotes",
    "href": "posts/test-markdown-post/index.html#footnotes",
    "title": "An Example Markdown Post",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the footnote.↩︎"
  },
  {
    "objectID": "posts/test-notebook-post/index.html",
    "href": "posts/test-notebook-post/index.html",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "Note\n\n\n\nThis post was generated by fastpages and kept for reference.\nFuture posts should be written as .qmd files as described here."
  },
  {
    "objectID": "posts/test-notebook-post/index.html#front-matter",
    "href": "posts/test-notebook-post/index.html#front-matter",
    "title": "Fastpages Notebook Blog Post",
    "section": "Front Matter",
    "text": "Front Matter\nThe first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this:\n# \"My Title\"\n&gt; \"Awesome summary\"\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\n\nSetting toc: true will automatically generate a table of contents\nSetting badges: true will automatically include GitHub and Google Colab links to your notebook.\nSetting comments: true will enable commenting on your blog post, powered by utterances.\n\nThe title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README."
  },
  {
    "objectID": "posts/test-notebook-post/index.html#markdown-shortcuts",
    "href": "posts/test-notebook-post/index.html#markdown-shortcuts",
    "title": "Fastpages Notebook Blog Post",
    "section": "Markdown Shortcuts",
    "text": "Markdown Shortcuts\nA #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post.\nA #hide_input comment at the top of any code cell will only hide the input of that cell.\n\n\nThe comment #hide_input was used to hide the code that produced this.\n\n\nput a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it:\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n\nput a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:\n\n\nCode\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\n\nplace a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it:\n\n#collapse-output\nprint('The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.')\n\nThe comment #collapse-output was used to collapse the output of this cell by default but you can expand it."
  },
  {
    "objectID": "posts/test-notebook-post/index.html#interactive-charts-with-altair",
    "href": "posts/test-notebook-post/index.html#interactive-charts-with-altair",
    "title": "Fastpages Notebook Blog Post",
    "section": "Interactive Charts With Altair",
    "text": "Interactive Charts With Altair\nCharts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook.\n\nExample 1: DropDown\n\n# single-value selection over [Major_Genre, MPAA_Rating] pairs\n# use specific hard-wired values as the initial selected values\nselection = alt.selection_single(\n    name='Select',\n    fields=['Major_Genre', 'MPAA_Rating'],\n    init={'Major_Genre': 'Drama', 'MPAA_Rating': 'R'},\n    bind={'Major_Genre': alt.binding_select(options=genres), 'MPAA_Rating': alt.binding_radio(options=mpaa)}\n)\n  \n# scatter plot, modify opacity based on selection\nalt.Chart(df).mark_circle().add_selection(\n    selection\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\n\n\n\nExample 2: Tooltips\n\nalt.Chart(df).mark_circle().add_selection(\n    alt.selection_interval(bind='scales', encodings=['x'])\n).encode(\n    alt.X('Rotten_Tomatoes_Rating', type='quantitative'),\n    alt.Y('IMDB_Rating', type='quantitative', axis=alt.Axis(minExtent=30)),\n#     y=alt.Y('IMDB_Rating:Q', ), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\n\nExample 3: More Tooltips\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)"
  },
  {
    "objectID": "posts/test-notebook-post/index.html#data-tables",
    "href": "posts/test-notebook-post/index.html#data-tables",
    "title": "Fastpages Notebook Blog Post",
    "section": "Data Tables",
    "text": "Data Tables\nYou can display tables per the usual way in your blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'Distributor', 'MPAA_Rating', 'IMDB_Rating', 'Rotten_Tomatoes_Rating']].head()\n\n\n\n\n\n\n\n\nTitle\nWorldwide_Gross\nProduction_Budget\nDistributor\nMPAA_Rating\nIMDB_Rating\nRotten_Tomatoes_Rating\n\n\n\n\n0\nThe Land Girls\n146083.0\n8000000.0\nGramercy\nR\n6.1\nNaN\n\n\n1\nFirst Love, Last Rites\n10876.0\n300000.0\nStrand\nR\n6.9\nNaN\n\n\n2\nI Married a Strange Person\n203134.0\n250000.0\nLionsgate\nNone\n6.8\nNaN\n\n\n3\nLet's Talk About Sex\n373615.0\n300000.0\nFine Line\nNone\nNaN\n13.0\n\n\n4\nSlam\n1087521.0\n1000000.0\nTrimark\nR\n3.4\n62.0"
  },
  {
    "objectID": "posts/test-notebook-post/index.html#images",
    "href": "posts/test-notebook-post/index.html#images",
    "title": "Fastpages Notebook Blog Post",
    "section": "Images",
    "text": "Images\n\nLocal Images\nYou can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax:\n![](images/fastai.png)\n\n\n\nRemote Images\nRemote images can be included with the following markdown syntax:\n![](https://image.flaticon.com/icons/svg/36/36686.svg) (just and example; link broken)\n\n\nAnimated Gifs\nAnimated Gifs work, too!\n![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif)\n\n\n\nCaptions\nYou can include captions with markdown images like this:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")"
  },
  {
    "objectID": "posts/test-notebook-post/index.html#github-flavored-emojis",
    "href": "posts/test-notebook-post/index.html#github-flavored-emojis",
    "title": "Fastpages Notebook Blog Post",
    "section": "GitHub Flavored Emojis",
    "text": "GitHub Flavored Emojis\nTyping I give this post two :+1:! will render this:\nI give this post two :+1:!"
  },
  {
    "objectID": "posts/test-notebook-post/index.html#tweetcards",
    "href": "posts/test-notebook-post/index.html#tweetcards",
    "title": "Fastpages Notebook Blog Post",
    "section": "Tweetcards",
    "text": "Tweetcards\nTyping &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20"
  },
  {
    "objectID": "posts/test-notebook-post/index.html#youtube-videos",
    "href": "posts/test-notebook-post/index.html#youtube-videos",
    "title": "Fastpages Notebook Blog Post",
    "section": "Youtube Videos",
    "text": "Youtube Videos\nTyping &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this:"
  },
  {
    "objectID": "posts/test-notebook-post/index.html#boxes-callouts",
    "href": "posts/test-notebook-post/index.html#boxes-callouts",
    "title": "Fastpages Notebook Blog Post",
    "section": "Boxes / Callouts",
    "text": "Boxes / Callouts\nTyping &gt; Warning: There will be no second warning! will render this:\n\n\n\n\n\n\nWarning\n\n\n\nThere will be no second warning!\n\n\nTyping &gt; Important: Pay attention! It's important. will render this:\n\n\n\n\n\n\nImportant\n\n\n\nPay attention! It’s important.\n\n\nTyping &gt; Tip: This is my tip. will render this:\n\n\n\n\n\n\nTip\n\n\n\nThis is my tip.\n\n\nTyping &gt; Note: Take note of this. will render this:\n\n\n\n\n\n\nNote\n\n\n\nTake note of this.\n\n\nTyping &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs:\n\n\n\n\n\n\nNote\n\n\n\nA doc link to an example website: fast.ai should also work fine."
  },
  {
    "objectID": "posts/test-notebook-post/index.html#footnotes",
    "href": "posts/test-notebook-post/index.html#footnotes",
    "title": "Fastpages Notebook Blog Post",
    "section": "Footnotes",
    "text": "Footnotes\nYou can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this:\n{% raw %}For example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ 'This is the footnote.' | fndetail: 1 }}\n{{ 'This is the other footnote. You can even have a [link](https://stefanbschneider.github.io/)!' | fndetail: 2 }}{% endraw %}\nFor example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ ‘This is the footnote.’ | fndetail: 1 }} {{ ‘This is the other footnote. You can even have a link!’ | fndetail: 2 }}"
  },
  {
    "objectID": "posts/question-answering-huggingface/index.html",
    "href": "posts/question-answering-huggingface/index.html",
    "title": "Building a Simple Q&A App with HuggingFace and Gradio",
    "section": "",
    "text": "Large language models (LLMs) like GPT, BART, etc. have demonstrated incredible abilities in natural language.\nThis blog post describes how you can use LLMs to build and deploy your own app in just a few lines of Python code with the HuggingFace ecosystem. HuggingFace provides pre-trained models, datasets, and other tools that are handy when working with machine learning models without having to understand all the underlying theory. If you are interested in how LLMs work, see my other blog post on the underlying transformer architecture.\nAs an example, the goal of this post is to build an app that answers questions about a given PDF document. The focus is on showing a simple proof of concept rather than high-quality answers.\nFirst, let’s install the necessary dependencies:\n\n%%capture --no-display\npip install -U pypdf torch transformers gradio\n\n\nQuestion Answering with HuggingFace\nWe can read the text of PDF document with pypdf. As an example, I’m using the author version of a paper I wrote on mobile-env.\n\nfrom pathlib import Path\nfrom typing import Union\nfrom pypdf import PdfReader\n\n\ndef get_text_from_pdf(pdf_file: Union[str, Path]) -&gt; str:\n    \"\"\"Read the PDF from the given path and return a string with its entire content.\"\"\"\n    reader = PdfReader(pdf_file)\n\n    # Extract text from all pages\n    full_text = \"\"\n    for page in reader.pages:\n        full_text += page.extract_text()\n    return full_text\n\n# Read and print parts of the PDF\npdf_text = get_text_from_pdf(\"mobileenv_author_version.pdf\")\npdf_text[:1500]\n\n'mobile-env: An Open Platform for Reinforcement\\nLearning in Wireless Mobile Networks\\nStefan Schneider, Stefan Werner\\nPaderborn University, Germany\\n{stschn, stwerner}@mail.upb.de\\nRamin Khalili, Artur Hecker\\nHuawei Technologies, Germany\\n{ramin.khalili, artur.hecker}@huawei.com\\nHolger Karl\\nHasso Plattner Institute,\\nUniversity of Potsdam, Germany\\nholger.karl@hpi.de\\nAbstract—Recent reinforcement learning approaches for con-\\ntinuous control in wireless mobile networks have shown im-\\npressive results. But due to the lack of open and compatible\\nsimulators, authors typically create their own simulation en-\\nvironments for training and evaluation. This is cumbersome\\nand time-consuming for authors and limits reproducibility and\\ncomparability, ultimately impeding progress in the ﬁeld.\\nTo this end, we proposemobile-env, a simple and open platform\\nfor training, evaluating, and comparing reinforcement learning\\nand conventional approaches for continuous control in mobile\\nwireless networks. mobile-env is lightweight and implements\\nthe common OpenAI Gym interface and additional wrappers,\\nwhich allows connecting virtually any single-agent or multi-agent\\nreinforcement learning framework to the environment. While\\nmobile-env provides sensible default values and can be used out\\nof the box, it also has many conﬁguration options and is easy to\\nextend. We therefore believe mobile-env to be a valuable platform\\nfor driving meaningful progress in autonomous coordination of\\nwireless mobile networks.\\nIndex T'\n\n\nNow we can create a question answering pipeline using HuggingFace, loading a pre-trained model. Then we can ask some questions, providing the PDF text as context.\n\nfrom transformers import pipeline\n\nquestion_answerer = pipeline(task=\"question-answering\", model=\"deepset/tinyroberta-squad2\")\n\nDevice set to use mps:0\n\n\n\nquestion_answerer(\"What is mobile-env?\", pdf_text)\n\n/opt/homebrew/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n  warnings.warn(\n\n\n{'score': 0.9887111186981201,\n 'start': 16488,\n 'end': 16505,\n 'answer': 'GitHub repository'}\n\n\n\nquestion_answerer(\"What programming language is mobile-env written in?\", pdf_text)\n\n{'score': 0.9665615558624268, 'start': 3552, 'end': 3558, 'answer': 'Python'}\n\n\n\nquestion_answerer(\"What is the main difference between mobile-env and other simulators?\", pdf_text)\n\n{'score': 0.6506955027580261,\n 'start': 12539,\n 'end': 12570,\n 'answer': 'more ﬂexible, better documented'}\n\n\nThe pipeline returns a dict, where the answer is a quote from the given context, here the PDF document. This is called extractive question answering.\nIt also provides a score indicating the model’s confindence in the answer and the start/end index from where the answer is quoted.\nThat’s it! Let’s see how we can build a simple app on top of this.\n\n\nBuilding an App with Gradio\nGradio allows building simple apps tailored for machine learning use cases. You can define the inputs, a function to where to pass these inputs, and how to display the functions outputs.\nHere, our inputs are the PDF document and the question. The function loads the document and passes the question and text to the pre-trained model. It then outputs the models answer to the user.\n\nimport gradio as gr\n\ndef answer_doc_question(pdf_file, question):\n    pdf_text = get_text_from_pdf(pdf_file)\n    answer = question_answerer(question, pdf_text)\n    return answer[\"answer\"]\n\n# Add default a file and question, so it's easy to try out the app.\npdf_input = gr.File(\n    value=\"https://ris.uni-paderborn.de/download/30236/30237/author_version.pdf\",\n    file_types=[\".pdf\"],\n    label=\"Upload a PDF document and ask a question about it.\",\n)\nquestion = gr.Textbox(\n    value=\"What is mobile-env?\",\n    label=\"Type a question regarding the uploaded document here.\",\n)\ngr.Interface(\n    fn=answer_doc_question, inputs=[pdf_input, question], outputs=\"text\"\n).launch()\n\nRunning on local URL:  http://127.0.0.1:7862\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\n/opt/homebrew/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n  warnings.warn(\n\n\nIf you run this locally, you should see a rendered app based on the question answering pipeline we built above!\n\n\nDeploying the app in HuggingFace Spaces\nYou can easily host the app on HuggingFace Spaces, which provide free (and slow) hosting (or fast paid hosting).\nYou simply create a new space under your account and add an app.py, which contains all code above. The requirements go into a requirements.txt. That’s it!\nThis is the app we built here: https://huggingface.co/spaces/stefanbschneider/pdf-question-answering\n\n\n\n\nWhat’s Next?\n\nRead about the underlying transformer architecture powering most LLMs\nImprove the quality of the question answering app. Some ideas:\n\nFine-tune the pre-trained model on a domain dataset, eg, Arxiv Q&A\nDomain adaptation by fine-tuning a masked model directly on the document\nUsing the document-question-answering pipeline on HuggingFace\nTrying a model that supports generative question answering"
  },
  {
    "objectID": "posts/django-bootstrap/index.html",
    "href": "posts/django-bootstrap/index.html",
    "title": "Using Bootstrap to Style a Django App",
    "section": "",
    "text": "Django allows building simple (and complex) web apps quickly, using Django Templates for rendering. By default, forms, buttons, and other elements are not styled and look quite ugly:\nUsing Bootstrap and django-crispy-forms, the rendered templates can easily be improved to look much nicer, without having to adjust styling manually. For example:\nAs an example, I extend my Django “Hello World” App (described in a previous post) by adding a simple form and rendering it with Bootstrap and django-crispy-forms. All it does is asking for the user’s name and a date and then displaying &lt;username&gt; says \"Hello World!\" on &lt;date&gt; and a counter of how often the button has been clicked. Still, the small example illustrates how to use Bootstrap and django-crispy-forms. Especially with many or large forms, django-crispy-forms becomes useful to reduce repetitive boilerplate."
  },
  {
    "objectID": "posts/django-bootstrap/index.html#creating-a-django-form",
    "href": "posts/django-bootstrap/index.html#creating-a-django-form",
    "title": "Using Bootstrap to Style a Django App",
    "section": "Creating a Django Form",
    "text": "Creating a Django Form\nAs an example, I create a new form that allows users to specify their name and an arbitrary date that will be displayed in the “Hello World” app. For that, I create helloworld/forms.py with the following content:\nimport datetime\n\nfrom django import forms\n\n\nclass HelloWorldForm(forms.Form):\n    \"\"\"Form asking for the user's name and an arbitrary date, both used inside the displayed 'Hello World' text.\"\"\"\n    username = forms.CharField(label='Your Name', max_length=100)\n    date = forms.DateField(label='An arbitrary date', initial=datetime.date.today,\n                           widget=forms.widgets.DateInput(attrs={'type': 'date'}),\n                           help_text='The entered name and date will be displayed temporarily but publicly in the '\n                                     'generated \"Hello World\" message. It will not be stored.')\nMost field arguments are optional but provide additional information for django-crispy-forms to display in the Bootstrap form.\nI then use this form inside views.py for my index view (the only view of the “Hello World” app so far):\ndef index(request):\n    # retriever counter model instance from DB or create it if it doesn't exist yet\n    counter, created = Counter.objects.get_or_create(name='hello-world-button')\n\n    # increment counter when a POST request arrives (from the button click)\n    # see previous blog post: https://stefanbschneider.github.io/blog/django-db\n    if request.method == 'POST':\n        counter.value += 1\n        counter.save()\n\n        # and get the values filled in form\n        form = HelloWorldForm(request.POST)\n        if form.is_valid():\n            username = form.cleaned_data['username']\n            date = form.cleaned_data['date']\n\n    else:\n        form = HelloWorldForm()\n        username = 'Nobody'\n        date = datetime.date.today()\n\n    context = {\n        'clicks': counter.value,\n        'form': form,\n        'username': username,\n        'date': date,\n    }\n    return render(request, 'helloworld/index.html', context)\nFinally, show the form in the index.html template:\n&lt;div class=\"alert alert-success\" role=\"alert\"&gt;\n    {{ username }} says: \"Hello World!\" on {{ date }}. (Button clicked {{ clicks }}x in total.)\n&lt;/div&gt;\n\n&lt;form action=\"{% url 'helloworld:index' %}\" method=\"post\"&gt;\n    {% csrf_token %}\n    {{ form }}\n    &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;Greet the world!&lt;/button&gt;\n&lt;/form&gt;\nNow, when running the development server, the app shows the new form:\n\n\n\nDjango form without Bootstrap\n\n\nThe “Hello World” message should display the entered username and date as well as the total click count. However, the form does not yet use Bootstrap and is still quite ugly!"
  },
  {
    "objectID": "posts/django-bootstrap/index.html#making-the-form-pretty",
    "href": "posts/django-bootstrap/index.html#making-the-form-pretty",
    "title": "Using Bootstrap to Style a Django App",
    "section": "Making the Form Pretty",
    "text": "Making the Form Pretty\nTo make the form look nicer, I first include Bootstrap. For using Bootstrap, simply include the Bootstrap CCS and JavaScript inside the head of the Django app’s main/base template. The Bootstrap website has the latest instructions.\nFor my “Hello World” Django app, I simply add the following lines inside helloworld/templates/helloworld/index.html:\n&lt;head&gt;\n    &lt;title&gt;Hello World&lt;/title&gt;\n    \n    {#  Bootstrap  #}\n    &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1\" crossorigin=\"anonymous\"&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n\n&lt;!--    other includes --&gt;\n&lt;/head&gt;\n&lt;!-- body --&gt;\nThis loads the new Bootstrap 5 from the JSDeliver CDN network so it can be used within the Django app templates. Now, the alert and button should already look nicer, but the form fields will still look ugly.\nTo also render the form fields with Bootstrap, I use django-crispy-forms. All it takes, is loading crispy and passing the form to crispy inside index.html:\n{% load crispy_forms_tags %}\n\n&lt;div class=\"alert alert-success\" role=\"alert\"&gt;\n    {{ username }} says: \"Hello World!\" on {{ date }}. (Button clicked {{ clicks }}x in total.)\n&lt;/div&gt;\n\n&lt;form action=\"{% url 'helloworld:index' %}\" method=\"post\"&gt;\n    {% csrf_token %}\n    {{ form | crispy }}\n    &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;Greet the world!&lt;/button&gt;\n&lt;/form&gt;\nNow, the Django app should be rendered with Bootstrap and already look much nicer:\n\n\n\nDjango form styled with Bootstrap\n\n\nThe nice thing is that crispy will handle all the overhead of styling each form field with bootstrap, which is particularly useful when having many large forms inside a Django app."
  },
  {
    "objectID": "posts/django-heroku/index.html",
    "href": "posts/django-heroku/index.html",
    "title": "Building a Django App and Deploying It on Heroku",
    "section": "",
    "text": "In this guide, I show how to setup a simple Django web app that says “Hello World!” and how to deploy it on Heroku through GitHub. The process is simple enough, but I still kept running into errors when deploying new Django apps, so I thought to finally just write it down step-by-step."
  },
  {
    "objectID": "posts/django-heroku/index.html#requirements",
    "href": "posts/django-heroku/index.html#requirements",
    "title": "Building a Django App and Deploying It on Heroku",
    "section": "Requirements",
    "text": "Requirements\nFor that, Heroku and GitHub accounts are required. Both are free.\nAll code should be in a GitHub repository. For reference, this link my GitHub repository. The steps described here belong to release v1.0.0."
  },
  {
    "objectID": "posts/django-heroku/index.html#django-hello-world-app",
    "href": "posts/django-heroku/index.html#django-hello-world-app",
    "title": "Building a Django App and Deploying It on Heroku",
    "section": "Django “Hello World” App",
    "text": "Django “Hello World” App\n\nInitial Setup\nInstall Django:\npip install django\nCreate a new Django project (inside the GitHub repository):\ndjango-admin startproject myproject\nA project can consist of multiple apps. Create a new Django helloworld app inside the myproject project:\ncd myproject\npython manage.py startapp helloworld\nRun the dev server:\npython manage.py runserver\nGo to http://localhost:8000/ in the browser. This should confirm the successful initial setup:\n\n\n\nCreate the “Hello World” App\nLink the helloworld app to the project by adding\n'helloworld.apps.HelloworldConfig',\nto the INSTALLED_APPS inside myproject/settings.py.\nCreate a new HTML-template index.html inside helloworld/templates/helloworld (also create these folders):\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;Hello World&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\nHello World!\n&lt;/body&gt;\n&lt;/html&gt;\nCreate helloworld/urls.py to configure a URL path showing the new template:\nfrom django.urls import path\nfrom django.views.generic import TemplateView\n\n\nurlpatterns = [\n    path('', TemplateView.as_view(template_name='helloworld/index.html'), name='index'),\n]\nFinally, link to the helloworld URLs from the project’s myproject/urls.py:\nfrom django.contrib import admin\nfrom django.urls import path, include\n\nurlpatterns = [\n    path('', include('helloworld.urls')),\n    path('admin/', admin.site.urls),\n]\nNow, http://localhost:8000/ should show the dummy “hello world” message:"
  },
  {
    "objectID": "posts/django-heroku/index.html#deploying-the-app-on-heroku",
    "href": "posts/django-heroku/index.html#deploying-the-app-on-heroku",
    "title": "Building a Django App and Deploying It on Heroku",
    "section": "Deploying the App on Heroku",
    "text": "Deploying the App on Heroku\n\nCreating a New App on Heroku\nTo create a new app on Heroku, I log into the Heroku dashboard and select New &gt; Create new app. For the new app, any unique name is ok (I chose django-hello-world-app).\n\n\n\n\n\n\n\nNote\n\n\n\nThe name of your Heroku app will also determine its URL, which is &lt;app-name&gt;.herokuapp.com. It can still be changed in the settings later.\n\n\nNext, I select my GitHub repository as deployment method inside Heroku (this may require authorization using GitHub credentials):\n\nFinally, I have to set some configuration variables (= environmental variables) inside Heroku: Settings &gt; Config Vars &gt; Reveal Config Vars. * Set DJANGO_SETTINGS_MODULE to myproject.prod_settings, which is are the settings for production deployment, created later. * Set DJANGO_SECRET_KEY to a randomly generated secret key that is used for deployment. This key must not be commited to the GitHub repository.\n\n\n\nPreparing Deployment\nBefore the actual deployment on Heroku, a few additional steps are required.\nCurrently, the generated folder structure should look like this:\nmyproject/\n    helloworld/\n    myproject/\n    manage.py\nFor deployment on Heroku, move everything into the top-level folder such that manage.py is in the project root and there is no more top-level myproject directory:\nhelloworld/\nmyproject/\nmanage.py\nThe Procfile indicates how to deploy and serve the web app, here with gunicorn:\nrelease: python manage.py migrate --no-input\nweb: gunicorn myproject.wsgi\nAccordingly, the dependencies of this “hello world” app are (saved in requirements.txt):\ndjango\ndjango-heroku\ngunicorn\nHeroku needs django-heroku for proper deployment (see Heroku docs).\nSpecify the root path for static files by appending to the myproject/settings.py (also import os):\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/3.1/howto/static-files/\nSTATIC_URL = '/static/'\n# path to where static files are copied for deployment (eg, for heroku)\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n# location of static files in local development: https://learndjango.com/tutorials/django-favicon-tutorial\n# not needed yet, only once adding static files (eg, images)\n# STATICFILES_DIRS = [os.path.join(BASE_DIR, 'static'),]\nCreate myproject/prod_settings.py with additional settings for production deployment:\n\"\"\" Production Settings \"\"\"\n# default: use settings from main settings.py if not overwritten\nfrom .settings import *\n\nimport django_heroku\n\n\nDEBUG = False\nSECRET_KEY = os.getenv('DJANGO_SECRET_KEY', SECRET_KEY)\n# adjust to the URL of your Heroku app\nALLOWED_HOSTS = ['django-hello-world-app.herokuapp.com']\n\n# Activate Django-Heroku.\ndjango_heroku.settings(locals())\nCommit and push everything to the repository’s main branch.\n\n\nAutomated Deployment via GitHub\nTo test the deployment, go to the Heroku dashboard Deploy &gt; Manual deploy, select the main branch and deploy. After the build and deployment succeeds, click Open app to open &lt;app-name&gt;.herokuapp.com, which should show “Hello World”. Deployment successful!\nFinally, enable automatic deploys at Deploy &gt; Automatic deploys &gt; Enable such that the latest version of the Django app is build and deployed automatically with every push to the main branch.\n\n\n\n\n\n\nNote\n\n\n\nHeroku’s free dynos are free but power off if they are unused. So loading a deployed app that hasn’t been used in a while may take multiple seconds."
  },
  {
    "objectID": "posts/django-heroku/index.html#what-next",
    "href": "posts/django-heroku/index.html#what-next",
    "title": "Building a Django App and Deploying It on Heroku",
    "section": "What Next?",
    "text": "What Next?\nSmall example apps I built with Django and deployed on Heroku:\n\nQuotify: An example app showing inspirational quotes. [Code] [App]\nIdeally: Organize & Grow Your Ideas. [Code] [App]\nFeelYa: The app that gets you! [Code] [App]"
  },
  {
    "objectID": "posts/pytorch-django/index.html",
    "href": "posts/pytorch-django/index.html",
    "title": "Using PyTorch Inside a Django App",
    "section": "",
    "text": "In this blog post, I build a simple image classification app using a pre-trained DenseNet 121 model in PyTorch. I deploy this image classification model inside a Django web app on Heroku.\nThis is very much related to the PyTorch guide on deployment with Flask. Here, I show an alternative using Django, which is not as light-weight but contains more features built-in than Flask. For more information on differences between Django and Flask, see this website.\nNote that my deployed app may take several seconds to load because I use Heroku’s fee dynos, which automatically power off when they are unused."
  },
  {
    "objectID": "posts/pytorch-django/index.html#initial-setup-install-django-and-pytorch",
    "href": "posts/pytorch-django/index.html#initial-setup-install-django-and-pytorch",
    "title": "Using PyTorch Inside a Django App",
    "section": "Initial Setup: Install Django and PyTorch",
    "text": "Initial Setup: Install Django and PyTorch\nRequirements: Python 3, GitHub and Heroku account. Install Django and PyTorch:\npip install django trochvision\nCreate a Django project pytorch_django and an app image_classification:\ndjango-admin startproject pytorch_django\ncd pytorch_django\npython manage.py startapp image_classification\nInside settings.py, add 'image_classification.apps.ImageClassificationConfig' to the INSTALLED_APPS list.\nTo verify, that there are no errors yet, start the Django dev server:\npython manage.py runserver\nGo to localhost:8000:"
  },
  {
    "objectID": "posts/pytorch-django/index.html#pytorch-image-classification",
    "href": "posts/pytorch-django/index.html#pytorch-image-classification",
    "title": "Using PyTorch Inside a Django App",
    "section": "PyTorch Image Classification",
    "text": "PyTorch Image Classification\nTo classify uploaded images, I use a DenseNet neural network that is pretrained on the ImageNet dataset. Since the web app is very simple and does not have any other functionality, I simply implement the image classification inside the Django image_classification/views.py module.\n\n\n\n\n\n\nNote\n\n\n\nThis code is taken from a PyTorch tutorial and is under MIT license.\n\n\nFirst, I load the pretrained DenseNet, switch to evaluation/inference mode (since I do not need any further training), and load the mapping for predicted indices to human-readable labels for ImageNet. The JSON-file containing the mapping is available here and should be saved in the Django static directory under as defined in STATICFILES_DIRS (settings.py).\nimport io\nimport os\nimport json\n\nfrom torchvision import models\nfrom torchvision import transforms\nfrom PIL import Image\nfrom django.conf import settings\n\n\n# load pretrained DenseNet and go straight to evaluation mode for inference\n# load as global variable here, to avoid expensive reloads with each request\nmodel = models.densenet121(pretrained=True)\nmodel.eval()\n\n# load mapping of ImageNet index to human-readable label (from staticfiles directory)\n# run \"python manage.py collectstatic\" to ensure all static files are copied to the STATICFILES_DIRS\njson_path = os.path.join(settings.STATIC_ROOT, \"imagenet_class_index.json\")\nimagenet_mapping = json.load(open(json_path))\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to load the pretrained model once as global variable and not inside the view function, which would reload the model on each request (expensive and slow!).\n\n\n\n\n\n\n\n\nTip\n\n\n\nLoading the static JSON file via settings.STATIC_ROOT should work both in development and production deployment but requires running python manage.py collectstatic first.\n\n\nThen, I need a function to transform an uploaded image (passed in bytes) into the required format for DenseNet, which is a 224 x 224 image with 3 RGB channels. The following code does this transformation and also normalizes the image, returning the corresponding tensor:\ndef transform_image(image_bytes):\n    \"\"\"\n    Transform image into required DenseNet format: 224x224 with 3 RGB channels and normalized.\n    Return the corresponding tensor.\n    \"\"\"\n    my_transforms = transforms.Compose([transforms.Resize(255),\n                                        transforms.CenterCrop(224),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(\n                                            [0.485, 0.456, 0.406],\n                                            [0.229, 0.224, 0.225])])\n    image = Image.open(io.BytesIO(image_bytes))\n    return my_transforms(image).unsqueeze(0)\nFinally, this function can be used inside the prediction function, where the transformed tensor of the uploaded image is passed through the pretrained DenseNet model in a forward pass. Since I only do inference here not training, I do not need a backward pass for backpropagation. The model predicts the index of the corresponding ImageNet class, which is just an integer. To display a more useful label, I retrieve the corresponding human-readable label from the imagenet_mapping dict that I created at the beginning from the downloaded JSON file:\ndef get_prediction(image_bytes):\n    \"\"\"For given image bytes, predict the label using the pretrained DenseNet\"\"\"\n    tensor = transform_image(image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    predicted_idx = str(y_hat.item())\n    class_name, human_label = imagenet_mapping[predicted_idx]\n    return human_label"
  },
  {
    "objectID": "posts/pytorch-django/index.html#django-url-setup",
    "href": "posts/pytorch-django/index.html#django-url-setup",
    "title": "Using PyTorch Inside a Django App",
    "section": "Django URL setup",
    "text": "Django URL setup\nHaving the PyTorch classification logic implemented in image_classification/views.py, I now need to integrate it into the Django app and really use it in a Django view and template. For that, I first make some adjustments in the URLs by creating a separate image_classification/urls.py for the URLs of the image classification app:\nfrom django.urls import path, include\nfrom django.conf import settings\nfrom django.conf.urls.static import static\nfrom . import views\n\napp_name = 'image_classification'\nurlpatterns = [\n    # two paths: with or without given image\n    path('', views.index, name='index'),\n] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\nWhen visiting the main page of the web app, the requests are now directed to an index view, which I need to implement next and which will make use of the previous PyTorch classification logic. Before, I still need link these URLs to the project’s URLs in pytorch_django/urls.py such that they become effective:\nurlpatterns = [\n    path('', include('image_classification.urls')),\n    path('admin/', admin.site.urls),\n]"
  },
  {
    "objectID": "posts/pytorch-django/index.html#django-image-upload-classification-and-display",
    "href": "posts/pytorch-django/index.html#django-image-upload-classification-and-display",
    "title": "Using PyTorch Inside a Django App",
    "section": "Django Image Upload, Classification, and Display",
    "text": "Django Image Upload, Classification, and Display\nNow, I implement the index view, which accepts an uploaded image, processes it, and passes it to the PyTorch classification logic implemented above. I also need a simple Django template to render the web interface, where users can upload an image and submit it for classification. After classification, the template needs to show the predicted label.\n\nForm\nFor submitting uploaded images, I use a very simply Django form with an ImageField in image_classification/forms.py:\nfrom django import forms\n\nclass ImageUploadForm(forms.Form):\n    image = forms.ImageField()\n\n\nView\nI use this form inside my index view to accept uploaded images. (index is how I called it in my image_classification/urls.py but it could be any other name.) Here, I just want to display the uploaded image and pass it to the PyTorch model for classification. I do not want to (even temporarily) store it to the file system/disk. Hence, inside the view (image_classification/views.py), I get the image from the form, get its byte representation (for PyTorch) and create an image URI for displaying the image in the template later (see StackOverflow):\nimport base64\nfrom django.shortcuts import render\nfrom .forms import ImageUploadForm\n\ndef index(request):\n    image_uri = None\n    predicted_label = None\n\n    if request.method == 'POST':\n        # in case of POST: get the uploaded image from the form and process it\n        form = ImageUploadForm(request.POST, request.FILES)\n        if form.is_valid():\n            # retrieve the uploaded image and convert it to bytes (for PyTorch)\n            image = form.cleaned_data['image']\n            image_bytes = image.file.read()\n            # convert and pass the image as base64 string to avoid storing it to DB or filesystem\n            encoded_img = base64.b64encode(image_bytes).decode('ascii')\n            image_uri = 'data:%s;base64,%s' % ('image/jpeg', encoded_img)\n\n            # get predicted label with previously implemented PyTorch function\n            try:\n                predicted_label = get_prediction(image_bytes)\n            except RuntimeError as re:\n                print(re)\n\n    else:\n        # in case of GET: simply show the empty form for uploading images\n        form = ImageUploadForm()\n\n    # pass the form, image URI, and predicted label to the template to be rendered\n    context = {\n        'form': form,\n        'image_uri': image_uri,\n        'predicted_label': predicted_label,\n    }\n    return render(request, 'image_classification/index.html', context)\n\n\nTemplate\nThe index view above calls Django’s render function on a template image_classification/index.html, which I need to create now (inside the image_classification/templates directory). The template needs to show the form for uploading images and, after submitting and image, the uploaded image and its predicted label.\n&lt;h1&gt;Image Classification App&lt;/h1&gt;\n&lt;p&gt;A simple Django web app with a pretrained PyTorch DenseNet model will try to classify the selected image according to ImageNet labels. Uploaded images are not saved.&lt;/p&gt;\n&lt;p&gt;&lt;small&gt;Further information:\n    &lt;a href=\"\" target=\"_blank\"&gt;Blog Post&lt;/a&gt;,\n    &lt;a href=\"https://github.com/stefanbschneider/pytorch-django\" target=\"_blank\"&gt;GitHub&lt;/a&gt;&lt;/small&gt;\n&lt;/p&gt;\n\n&lt;form method=\"post\" enctype=\"multipart/form-data\" style=\"margin-top: 50px; margin-bottom: 30px;\"&gt;\n    {% csrf_token %}\n    {{ form }}\n    &lt;button type=\"submit\" id=\"btnUpload\" class=\"btn btn-primary\"&gt;Upload&lt;/button&gt;\n&lt;/form&gt;\n\n{% if image_uri is not None %}\n    {% if predicted_label is not None %}\n        &lt;div class=\"alert alert-primary\" role=\"alert\"&gt;\n            Predicted label: &lt;b&gt;{{ predicted_label }}&lt;/b&gt;\n        &lt;/div&gt;\n    {% else %}\n        &lt;div class=\"alert alert-danger\" role=\"alert\"&gt;\n            Prediction error. No label predicted.\n        &lt;/div&gt;\n    {% endif %}\n\n    &lt;img src=\"{{ image_uri }}\" class=\"img-fluid\" alt=\"Uploaded image\"\n         style=\"max-width: min(500px, 100%); height: auto; margin-top: 30px;\"&gt;\n{% endif %}\nThe uploaded image uses the saved and passed image URI from before and does not save or load any image from disk, which is important for privacy.\nThis template relies on some Bootstrap styling (see my corresponding blog post), but it is of course possible to omit that.\n\n\nTesting the App Locally\nRunning the app locally should now work without errors and show a simple page with the image upload form:\n\nAfter uploading an image, the app shows the image and its classification below:\n\nHere, it correctly classifies the image as a (tiger) cat."
  },
  {
    "objectID": "posts/pytorch-django/index.html#deployment-on-heroku",
    "href": "posts/pytorch-django/index.html#deployment-on-heroku",
    "title": "Using PyTorch Inside a Django App",
    "section": "Deployment on Heroku",
    "text": "Deployment on Heroku\nFor (production) deployment of this simple web app on Heroku, a few extra steps are necessary. Also refer to my dedicated blog post on this topic for details.\n\nFile Structure\nFor some reason, the default directory structure always breaks my Heroku deployment. It works, when removing the parent pytorch_django directory like this:\n# original structure when generating the project and app\npytorch_django\n    image_classification\n        ...\n    pytorch_django\n        ...\n    manage.py\nREADME.md\n\n# after removing the parent directory\nimage_classification\n    ...\npytorch_django\n    ...\nmanage.py\nREADME.md\n\n\nSetup and Production Settings\nAfter creating the app on Heroku and enabling automatic deploys from the corresponding GitHub repo, set the following config variables (in Heroku: Settings &gt; Config Vars):\nDJANGO_SETTINGS_MODULE: pytorch_django.prod_settings\nDJANGO_SECRET_KEY: &lt;randomly-generated-secret-key&gt;\nThis indicates that Heroku should use a separate prod_settings.py rather than the settings.py used for development. This prod_settings.py simply overwrites and disables debug mode, sets the production secret key, and allowed hosts. It also makes use of the django_heroku package for further settings.\nimport django_heroku\n# default: use settings from main settings.py if not overwritten\nfrom .settings import *\n\nDEBUG = False\nSECRET_KEY = os.getenv('DJANGO_SECRET_KEY', SECRET_KEY)\n# adjust this to the URL of your Heroku app\nALLOWED_HOSTS = ['pytorch-django.herokuapp.com']\n# Activate Django-Heroku.\ndjango_heroku.settings(locals())\n\n\nProcfile and Requirements\nAlso, add a Procfile in the project root that indicates how to prepare the release and deployment on Heroku using gunicorn:\nrelease: python manage.py migrate --no-input\nweb: gunicorn pytorch_django.wsgi\nThe paths depend on the project name and directory structure.\nAlso specify the requirements that need to be installed for the app in requirements.txt:\n-f https://download.pytorch.org/whl/torch_stable.html\ndjango==3.2\nwhitenoise==5.2.0\ngunicorn==20.0.4\ndjango-heroku==0.3.1\n# cpu version of torch and torchvision for heroku to reduce slug size\ntorch==1.8.1+cpu\ntorchvision==0.9.1+cpu\nFor deployment on Heroku, it’s important to use the CPU version of PyTorch since the slug size is otherwise too large (above 500 MB), which leads to a build error (see StackOverflow). The free Herku dynos only support CPU anyways.\n\n\nStatic Files\nFor serving static files (here, the JSON containing the ImageNet label mapping), configure STATIC_ROOT, STATIC_URL, and STATICFILES_DIR in settings.py:\nSTATIC_URL = '/static/'\n# path to where static files are copied for deployment (eg, for heroku)\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n# location of static files in local development: https://learndjango.com/tutorials/django-favicon-tutorial\nSTATICFILES_DIRS = [os.path.join(BASE_DIR, 'static'),]\nFor production, use whitenoise as described here. Make sure to add the staticfiles directory to GitHub as it will not be created automatically by Django.\nThe STATIC_ROOT is used inside views.py (see above) to load the JSON file for mapping. To copy all static files from their STATICFILES_DIRS to STATIC_ROOT, run\npython manage.py collectstatic\nThis is only required once locally. Heroku executes it on each deploy automatically.\n\n\nTesting the Deployed App\nCheck the Heroku activity/logs to see if the build and deployment are successful. After successful deployment, access the app at its URL. Mine is at https://pytorch-django.herokuapp.com/.\n\n\n\n\n\n\nDemo unavailable\n\n\n\nUnfortunately, Heroku shut down their free instances, which I used to deploy the demo. Hence, it is unavailable now.\nYou can still browse the GitHub repository with the full source code."
  },
  {
    "objectID": "posts/pytorch-django/index.html#what-next",
    "href": "posts/pytorch-django/index.html#what-next",
    "title": "Using PyTorch Inside a Django App",
    "section": "What Next?",
    "text": "What Next?\nOutcomes of this blog post:\n\nDeployed app (Now offline because Heroku shut down free instances)\nGitHub code\n\nExternal links:\n\nPyTorch tutorial on image classification with DenseNet and Flask\nPyTorch DenseNet information\nImageNet dataset"
  },
  {
    "objectID": "posts/descript/index.html",
    "href": "posts/descript/index.html",
    "title": "Easily editing videos with Descript",
    "section": "",
    "text": "After the Covid outbreak, many conferences and similar events were moved online or into a hybrid format. For me as researcher, this means that I now only rarely present my research outcomes live in person. Instead, I record more and more videos of my conferences presentations to share online as part of online conferences. Some of them are also on YouTube.\nI noticed that I tend to use filler words like “uhm”, “äh”, “like”, etc, which are even more annoying in recorded and publishded videos than in live, in-person presentations. Of course, I practice my talks before recording, but it is difficult to get rid of bloopers and filler words completely without editing.\nSince I am not used to video editing, manually searching for and removing filler words in long presentations would have meant hours of work. Hence, I searched for some tool to support this editing process and found Descript, which does exactly what I needed: It can record presentations, converts speech to text, automatically detects and removes filler words, improves audio, and more. To edit and remove parts of the videos, simply delete the corresponding auto-generated text like you would in Word."
  },
  {
    "objectID": "posts/descript/index.html#demonstration",
    "href": "posts/descript/index.html#demonstration",
    "title": "Easily editing videos with Descript",
    "section": "Demonstration",
    "text": "Demonstration\nHere is a short demonstration of how it works:\n\nOf course, it is not perfect yet and sometimes words are recognized incorrectly. But overall, I was very impressed by how well it usually works, plus there are frequent updates and improvements. It has certainly saved me hours of editing and significantly improved my videos. Since it took me a while to find the tool, I wanted to spread the word in this blog post, hopefully saving others some time."
  },
  {
    "objectID": "posts/descript/index.html#product-video",
    "href": "posts/descript/index.html#product-video",
    "title": "Easily editing videos with Descript",
    "section": "Product Video",
    "text": "Product Video\nFor more available features, check out the product video/advertisement, which I find very entertaining:"
  },
  {
    "objectID": "posts/descript/index.html#try-it-yourself",
    "href": "posts/descript/index.html#try-it-yourself",
    "title": "Easily editing videos with Descript",
    "section": "Try It Yourself",
    "text": "Try It Yourself\nYou can try Descript for free for some hours (but with watermark, I think). After that, there is a discounted plan for students/non-profit, which might be useful, or more powerful plans for creators, professionals, or enterprises."
  },
  {
    "objectID": "posts/google-analytics-track-links/index.html",
    "href": "posts/google-analytics-track-links/index.html",
    "title": "Tracking Link Clicks With Google Analytics",
    "section": "",
    "text": "Google Analytics has been very helpful to monitor and analyze web traffic on my blog. It provides useful insights into how frequently which of my posts and pages are visited, where visitors are from, etc. As such it already keeps track of link clicks to and within pages of my blog.\nHowever, it is often also interesting to track and monitor clicks to outgoing links, i.e., to external websites. This is useful, for example, to keep track of clicks to affiliate links on a website. Such outgoing link clicks are not tracked and shown by default in Google Analytics (the new/current GA4 version) but have to be configured manually.\nIt took me surprisingly long to figure out how to track clicks to specific (outgoing) links in Google Analytics, even though it is very simple. To spare others some time, here how it works (details and screenshots below):"
  },
  {
    "objectID": "posts/google-analytics-track-links/index.html#tracking-a-single-specific-link",
    "href": "posts/google-analytics-track-links/index.html#tracking-a-single-specific-link",
    "title": "Tracking Link Clicks With Google Analytics",
    "section": "Tracking a Single, Specific Link",
    "text": "Tracking a Single, Specific Link\nIf you want to track a single, specific link, e.g., https://www.example.com/abc, set a single matching condition with parameter link_url and operator “equals (ignore case)”. Then set the value to the specific URL of the link, e.g., https://www.example.com/abc."
  },
  {
    "objectID": "posts/google-analytics-track-links/index.html#tracking-groups-of-links",
    "href": "posts/google-analytics-track-links/index.html#tracking-groups-of-links",
    "title": "Tracking Link Clicks With Google Analytics",
    "section": "Tracking Groups of Links",
    "text": "Tracking Groups of Links\nIf you are interested in clicks to all links going to a certain website, but you do not want to distinguish clicks to different pages on that website, you can create and track a single event for all these link clicks. For example, you want to track clicks to https://www.example.com/, https://www.example.com/abc, https://www.example.com/def, etc. all together in a single event.\nFor that, you also just need a single matching condition, again using the link_url parameter. Here, simply use the operator “contains (ignore case)” instead of “equals (ignore case)” and use as value the domain name (e.g., example.com), not the full URL (e.g., https://www.example.com/abc).\nI use this approach, creating one event per external page that I am interested in, grouping all links to that page in a single event."
  },
  {
    "objectID": "posts/google-analytics-track-links/index.html#tracking-affiliate-links",
    "href": "posts/google-analytics-track-links/index.html#tracking-affiliate-links",
    "title": "Tracking Link Clicks With Google Analytics",
    "section": "Tracking Affiliate Links",
    "text": "Tracking Affiliate Links\nIt is also simple to add additional matching conditions for more fine-grained control. For example, this allows tracking clicks to affiliate links, which typically refer to one or multiple pages on a partner website and always contain a certain suffix holding the affiliate ID, e.g., ?in=569. Of course, this affiliate ID is specific to you and the website.\nTo track clicks to all affiliate links, simply extend the approach from above:\n\nUse one matching rule to capture all link clicks to pages of a certain domain with parameter link_url, operator “contains (ignore case)”, and value corresponding to the target domain (e.g., example.com).\nAdd a second matching rule (click “Add condition”) that filters out only affiliate links to the above domain. For that, use the parameter link_url, operator “contains (ignore)”, and use your affiliate ID as value (e.g., ?in=569).\n\n\n\n\nConfiguration of a new Google Analytics event for tracking link clicks to example.com with an example affiliate ID.\n\n\nFinally, click “Create” to save the newly create event - whether it is for tracking a single link, multiple links, or affiliate links. Note that the new event is often not directly included in the list of events. For me, it took several days until Google Analytics showed the new event - both in the reports and in the list of events under “Configure”."
  },
  {
    "objectID": "posts/django-google-analytics/index.html",
    "href": "posts/django-google-analytics/index.html",
    "title": "Adding Google Analytics to a Django App",
    "section": "",
    "text": "Google Analytics is the most popular solution to monitor and analyze web traffic. It provides detailed information about users visiting your website or web app - both historically and in real-time.\nHere, use the new Google Analytics GA4 to create a “Google Analytics property” for monitoring analyzing the traffic on a Django web app. As simple example app, I build on the previously described Django “Hello World” app. I use WebsitePolicies to create a free cookie banner.\nThe final Django app is deployed on Heroku and available here. The final code is in this this GitHub repository."
  },
  {
    "objectID": "posts/django-google-analytics/index.html#requirements",
    "href": "posts/django-google-analytics/index.html#requirements",
    "title": "Adding Google Analytics to a Django App",
    "section": "Requirements",
    "text": "Requirements\n\nA Django web app. This post builds on the Django “Hello World” app described in this post.\n\nRelease v1.1.0 belongs to this blog post.\n\nA Google account you can use for Google Analytics. It’s free."
  },
  {
    "objectID": "posts/django-google-analytics/index.html#creating-the-google-analytics-ga4-property",
    "href": "posts/django-google-analytics/index.html#creating-the-google-analytics-ga4-property",
    "title": "Adding Google Analytics to a Django App",
    "section": "Creating the Google Analytics GA4 Property",
    "text": "Creating the Google Analytics GA4 Property\nGo to the Google Analytics dashboard and log in if requested. To monitor a new Django app, create a new Google Analytics property by navigating to Admin &gt; + Create Property.\n\nFill in the fields, selecting a property name, region, currency, and information regarding the monitored Django app.\n\nOnce the new property is created, select Data Streams &gt; Web to set up a new data stream for the Django web app. Paste the URL of the Django app:"
  },
  {
    "objectID": "posts/django-google-analytics/index.html#adding-google-analytics-to-the-django-app",
    "href": "posts/django-google-analytics/index.html#adding-google-analytics-to-the-django-app",
    "title": "Adding Google Analytics to a Django App",
    "section": "Adding Google Analytics to the Django App",
    "text": "Adding Google Analytics to the Django App\nGet the code snippet from the created Google Analytics data stream from the created data stream site under Tagging Instructions &gt; Add new on-page tag &gt; Global Site Tag (gtag.js). The snippet should look like this (of course with the real tag ID instead of YOURTAGID):\n&lt;!-- Global site tag (gtag.js) - Google Analytics --&gt;\n&lt;script async src=\"https://www.googletagmanager.com/gtag/js?id=YOURTAGID\"&gt;&lt;/script&gt;\n&lt;script&gt;\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'YOURTAGID');\n&lt;/script&gt;\nCopy the code snippet and paste it inside the Django app’s main/base template. In case of my “Hello World” app, there is just a single template, which looks like this:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;Hello World&lt;/title&gt;\n\n    &lt;!-- Global site tag (gtag.js) - Google Analytics --&gt;\n    &lt;script async src=\"https://www.googletagmanager.com/gtag/js?id=YOURTAGID\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n      window.dataLayer = window.dataLayer || [];\n      function gtag(){dataLayer.push(arguments);}\n      gtag('js', new Date());\n\n      gtag('config', 'YOURTAGID');\n    &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\nHello World!\n&lt;/body&gt;\n&lt;/html&gt;\nSave, commit, push, and deploy. Google Analytics will likely only work on the deployed app in production.\nTo validate that it works, open the Django app in Chrome and open developer tools. Opening Application &gt; Cookies should show that the Django app now has cookies used for Google Analytics:\n\nOpening the Google Analytics realtime dashboard should show that there is a visitor (this may take a few seconds):\n\n\n\n\n\n\n\nNote\n\n\n\nFirefox may block Google Analytics or send “Do-Not-Track” Signals such that visiting the Django app is not recognized or shown in the realtime dashboard. For testing, turn off tracking protection or use Chrome."
  },
  {
    "objectID": "posts/django-google-analytics/index.html#adding-a-cookie-banner",
    "href": "posts/django-google-analytics/index.html#adding-a-cookie-banner",
    "title": "Adding Google Analytics to a Django App",
    "section": "Adding a Cookie Banner",
    "text": "Adding a Cookie Banner\nUsing cookies for Google Analytics legally requires informing users of the Django app of these cookies. If the app does not yet have a cookie banner, it’s now time to create one.\nI use WebsitePolicies for generating suitable cookie banners. The service is simple and free for non-commercial apps. Simply fill in the questions, and the service generates an HTML snippet to copy and paste into the Django app’s base template (similar to the GA4 tag).\nThe updated web page should show a cookie banner similar to this:"
  },
  {
    "objectID": "posts/django-google-analytics/index.html#what-next",
    "href": "posts/django-google-analytics/index.html#what-next",
    "title": "Adding Google Analytics to a Django App",
    "section": "What Next?",
    "text": "What Next?\n\nCreate a cookie banner and privacy policy on WebsitePolicies\nMigrating an existing Google Analytics universal property to the new GA4 (Guide by Google)\nDjango Hello World GitHub Repository\n\nOther blog posts:\n\nBuilding a Django App and Deploying It on Heroku\nAdding a Database to a Django App\nUsing Bootstrap to Style a Django App"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Stefan. I hold a computer science Ph.D. and am passionate about learning new things. I use this blog to take notes for myself - and possibly help others.\nContact details and more information about myself are on my website.\nI created this blog using Quarto."
  }
]